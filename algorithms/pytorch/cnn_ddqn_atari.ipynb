{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-09T12:41:01.201097Z",
     "end_time": "2023-04-09T12:41:01.243117Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "from typing import Dict, Iterable, List\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStack, RecordEpisodeStatistics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from stable_baselines3.common.save_util import load_from_pkl, save_to_pkl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from typing import Dict\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-09T11:39:06.707922Z",
     "end_time": "2023-04-09T11:39:06.750795Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_env(env_name=\"ALE/Pong-v5\", seed=42):\n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\", full_action_space=False, frameskip=1)\n",
    "    env = AtariPreprocessing(env)\n",
    "    env = FrameStack(env, 4)\n",
    "    env = RecordEpisodeStatistics(env)\n",
    "    env.observation_space.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-09T11:39:07.363282Z",
     "end_time": "2023-04-09T11:39:07.372113Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_channels, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_channels, 32, stride=4, kernel_size=8)\n",
    "        self.conv2 = nn.Conv2d(32, 64, stride=2, kernel_size=4)\n",
    "        self.conv3 = nn.Conv2d(64, 64, stride=1, kernel_size=3)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # 64 * 7 * 7 is the output of last conv layer because of the formula above, for an input of 84*84\n",
    "        self.linear = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.head = nn.Linear(512, num_actions)  # Head layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float() / 255  # Rescale input from [0, 255] to [0, 1]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.linear(x))\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **policy network**: $Q^{A}_{\\theta}$\n",
    "- **target network**: $Q^{B}_{\\theta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-09T11:39:08.518131Z",
     "end_time": "2023-04-09T11:39:08.521906Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def linear_schedule(start_epsilon: float, end_epsilon: float, duration: int, timestep: int):\n",
    "    slope = (end_epsilon - start_epsilon) / duration\n",
    "    return max(slope * timestep + start_epsilon, end_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-09T19:27:37.745045Z",
     "end_time": "2023-04-09T19:27:37.756803Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_latest_checkpoint_file\u001B[39m(files: \u001B[43mList\u001B[49m[\u001B[38;5;28mstr\u001B[39m]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28many\u001B[39m:\n\u001B[1;32m      2\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m    Return the most recent checkpoint file from the passed list of files.\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124;03m    :return: the file with the most recent date time or ``None`` if no files were found (because of the lack of correctly formatted date in the file name)\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m     10\u001B[0m     datetime_regex \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124md\u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124md\u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124md\u001B[39m\u001B[38;5;132;01m{4}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124md\u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124md\u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124md\u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "def get_latest_checkpoint_file(files: List[str]) -> any:\n",
    "    \"\"\"\n",
    "    Return the most recent checkpoint file from the passed list of files.\n",
    "\n",
    "    If multiple files with same datetime are passed, only the first is returned\n",
    "\n",
    "    :param files: list of file names containing a formatted datetime (=> %d-%m-%Y_%H:%M:%S)\n",
    "    :return: the file with the most recent date time or ``None`` if no files were found (because of the lack of correctly formatted date in the file name)\n",
    "    \"\"\"\n",
    "    datetime_regex = r\"\\d{2}-\\d{2}-\\d{4}_\\d{2}:\\d{2}:\\d{2}\"\n",
    "\n",
    "    latest_file = None\n",
    "    latest_datetime = datetime.min\n",
    "    for file in files:\n",
    "        match = re.search(datetime_regex, file)\n",
    "        if not match: continue # Go to next element in list if no match is found\n",
    "\n",
    "        file_datetime = datetime.strptime(match.group(), \"%d-%m-%Y_%H:%M:%S\")\n",
    "        if file_datetime > latest_datetime:\n",
    "            latest_datetime = file_datetime\n",
    "            latest_file = file\n",
    "\n",
    "    return latest_file\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, name=\"cnn_ddqn_ALE-Pong-v5\", env_name=\"ALE/Pong-v5\"):\n",
    "        self.name = name\n",
    "\n",
    "        self.env = make_env(env_name)\n",
    "\n",
    "        self.start_datetime = None\n",
    "        self.start_time = None\n",
    "\n",
    "        # I use the same hyperparameters as this model: https://huggingface.co/sb3/dqn-PongNoFrameskip-v4\n",
    "\n",
    "        self.MAX_TIMESTEPS = 10_000_000  # Maximum number of total steps\n",
    "        self.TARGET_UPDATE_INTERVAL = 1000  # Number of steps between the synchronisation of q and target network\n",
    "        self.LEARNING_STARTS = 100_000  # The number of steps to wait before we start the training, so the agent can explore and store its experience in the replay buffer\n",
    "\n",
    "        self.TRAIN_FREQUENCY = 4 # Training is done each 4 steps\n",
    "\n",
    "        self.CHECKPOINT_INTERVAL_EPISODE = 1000 # Checkpoint saving interval per episode (a checkpoint will be saved each X episodes)\n",
    "\n",
    "        self.REPLAY_SIZE = 100_000\n",
    "        self.BATCH_SIZE = 32\n",
    "\n",
    "        self.GAMMA = 0.99  # Discount rate\n",
    "\n",
    "        self.EXPLORATION_FRACTION = 0.1  # The fraction of 'TOTAL_TIMESTEPS' it takes from 'EPSILON_START' to 'EPSILON_END'.\n",
    "        self.EPSILON_INITIAL = 1.0\n",
    "        self.EPSILON_FINAL = 0.01\n",
    "\n",
    "        self.epsilon = self.EPSILON_INITIAL  # Exploration probability\n",
    "\n",
    "        self.memory = ReplayBuffer(\n",
    "            buffer_size=self.REPLAY_SIZE,\n",
    "            observation_space=self.env.observation_space,\n",
    "            action_space=self.env.action_space,\n",
    "            device=device,\n",
    "            optimize_memory_usage=True,\n",
    "            handle_timeout_termination=False\n",
    "        )\n",
    "\n",
    "        self.timesteps = 0\n",
    "\n",
    "        self.policy_network = DQN(4, self.env.action_space.n).to(device)\n",
    "        self.target_network = DQN(4, self.env.action_space.n).to(device)\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.policy_network.parameters(), lr=0.0001)\n",
    "        self.loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "        # Metrics/Logs\n",
    "        self.PATH = \"runs\"\n",
    "        if not os.path.exists(self.PATH):\n",
    "            os.makedirs(self.PATH)\n",
    "\n",
    "        self.CHECKPOINTS_PATH = f\"{self.PATH}/checkpoints\"\n",
    "        self.LOGS_PATH = f\"{self.PATH}/logs\"\n",
    "\n",
    "        self.is_loaded_from_checkpoint = False\n",
    "        self.writer = None\n",
    "\n",
    "    def remember(self, observation, next_observation, action, reward, done, infos):\n",
    "        self.memory.add(observation, next_observation, action, reward, done, infos)\n",
    "\n",
    "    def act(self, state):\n",
    "        if self.timesteps >= self.LEARNING_STARTS:\n",
    "            self.epsilon = linear_schedule(self.EPSILON_INITIAL, self.EPSILON_FINAL,\n",
    "                                           int(self.EXPLORATION_FRACTION * self.MAX_TIMESTEPS), self.timesteps - self.LEARNING_STARTS)\n",
    "\n",
    "        if self.timesteps < self.LEARNING_STARTS or np.random.rand() < self.epsilon:\n",
    "            # Random action\n",
    "            return np.array(self.env.action_space.sample())\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor(np.array(state), device=device).unsqueeze(0)\n",
    "                q_values = self.policy_network(state_tensor)\n",
    "                return q_values.argmax(dim=1)[0].cpu().numpy()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "\n",
    "    def optimize_model(self):\n",
    "        minibatch = self.memory.sample(self.BATCH_SIZE)\n",
    "\n",
    "        # Calculate Q values for current states\n",
    "        # For each q_values, get the action according to the minibatch\n",
    "        q_values = self.policy_network(minibatch.observations).gather(1, minibatch.actions)\n",
    "\n",
    "        # Then, calculate the best actions for the next states, and return its indices\n",
    "        with torch.no_grad():\n",
    "            best_next_actions = self.policy_network(minibatch.next_observations).argmax(1).unsqueeze(1)\n",
    "\n",
    "        # Calculate the Q values for the next states using the target network, and return the action according to the best next action returned by the q network\n",
    "        target_next_q_values = self.target_network(minibatch.next_observations).gather(1, best_next_actions)\n",
    "\n",
    "        # Calculate the target Q values using Double DQN\n",
    "        target_q_values = minibatch.rewards + (1 - minibatch.dones) * self.GAMMA * target_next_q_values\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = self.loss_fn(q_values, target_q_values)\n",
    "\n",
    "        # Compute metrics for loss\n",
    "        if self.timesteps % 100 == 0:\n",
    "            self.writer.add_scalar(\"Loss/td_loss\", loss, self.timesteps)\n",
    "            self.writer.add_scalar(\"Loss/q_values\", q_values.squeeze().mean().item(), self.timesteps)\n",
    "            steps_per_second = int(self.timesteps / (time.time() - self.start_time))\n",
    "            #print(\"Steps per second: \", steps_per_second)\n",
    "            self.writer.add_scalar(\"Charts/steps_per_second\", steps_per_second, self.timesteps)\n",
    "\n",
    "\n",
    "        # Optimise Q network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        if self.start_datetime is None:\n",
    "            print(\"SAVE_CHECKPOINT_ERROR: Training need to have started to save a checkpoint.\")\n",
    "            return\n",
    "\n",
    "        print(\"Saving checkpoint...\")\n",
    "        current_datetime_str = datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
    "        start_datetime_str = self.start_datetime.strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
    "\n",
    "        save_parent_directory = f\"{self.CHECKPOINTS_PATH}/{self.name}_{start_datetime_str}\"\n",
    "        save_path = save_parent_directory + \"/chkpt_\" + current_datetime_str + \".tar\"\n",
    "        replay_buffer_path = save_parent_directory + \"/replay_buffer_\" + current_datetime_str\n",
    "\n",
    "        if not os.path.exists(save_parent_directory):\n",
    "            os.makedirs(save_parent_directory)\n",
    "\n",
    "        checkpoint = {\n",
    "            \"env\": self.env,\n",
    "            \"timesteps\": self.timesteps,\n",
    "            \"start_datetime\": self.start_datetime,\n",
    "            \"epsilon\": self.epsilon,\n",
    "            \"policy_network\": self.policy_network.state_dict(),\n",
    "            \"target_network\": self.target_network.state_dict(),\n",
    "            \"optimizer\": self.optimizer.state_dict(),\n",
    "        }\n",
    "\n",
    "        torch.save(checkpoint, save_path)\n",
    "        # Saving the replay buffer will takes time! But it is needed to properly resume training\n",
    "        save_to_pkl(replay_buffer_path, self.memory, verbose=1)\n",
    "\n",
    "        print(f\"Checkpoint saved into {save_parent_directory}\")\n",
    "\n",
    "    def load_last_checkpoint(self, path):\n",
    "        \"\"\"\n",
    "        Load the last saved checkpoint found in the given ``path``\n",
    "\n",
    "        :param path: the path to the directory containing the checkpoint(s)\n",
    "        \"\"\"\n",
    "        print(f\"Loading most recent checkpoint from {path}\")\n",
    "        self.is_loaded_from_checkpoint = True\n",
    "\n",
    "        # Using list comprehension to filter directories and only get the files\n",
    "        files = [file for file in os.listdir(path) if os.path.isfile(os.path.join(path, file))]\n",
    "\n",
    "        checkpoint_files = [chkpt_file for chkpt_file in files if \"chkpt\" in chkpt_file]\n",
    "        replay_buffer_files = [chkpt_file for chkpt_file in files if \"replay_buffer\" in chkpt_file]\n",
    "\n",
    "        checkpoint_file = get_latest_checkpoint_file(checkpoint_files)\n",
    "        replay_buffer_file = get_latest_checkpoint_file(replay_buffer_files)\n",
    "\n",
    "        checkpoint: Dict[str, any] = torch.load(path + \"/\" + checkpoint_file)\n",
    "\n",
    "        self.env = checkpoint[\"env\"]\n",
    "        self.timesteps = checkpoint[\"timesteps\"]\n",
    "        self.start_datetime: datetime = checkpoint[\"start_datetime\"]\n",
    "        self.start_time = self.start_datetime.timestamp()\n",
    "\n",
    "        self.epsilon = checkpoint[\"epsilon\"]\n",
    "\n",
    "        self.policy_network.load_state_dict(checkpoint[\"policy_network\"])\n",
    "        self.target_network.load_state_dict(checkpoint[\"target_network\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "        self.memory: ReplayBuffer = load_from_pkl(path + \"/\" + replay_buffer_file)\n",
    "        print(\"Checkpoint successfully loaded, you can resume the training now.\")\n",
    "\n",
    "    def run(self):\n",
    "        # Either create a new SummaryWriter or resume from previous one\n",
    "        if not self.is_loaded_from_checkpoint:\n",
    "            current_datetime = datetime.now()\n",
    "            self.start_datetime = current_datetime\n",
    "            self.start_time = current_datetime.timestamp()\n",
    "\n",
    "        start_datetime_str = self.start_datetime.strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
    "        self.writer = SummaryWriter(f\"{self.LOGS_PATH}/{self.name}_{start_datetime_str}\")\n",
    "\n",
    "        while self.timesteps < self.MAX_TIMESTEPS:\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                self.timesteps += 1\n",
    "\n",
    "                action = self.act(state)\n",
    "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                self.remember(state, next_state, action, reward, terminated, info)\n",
    "\n",
    "                if self.timesteps >= self.LEARNING_STARTS and self.timesteps % self.TRAIN_FREQUENCY == 0:\n",
    "                    self.optimize_model()\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    mean_reward = np.mean(self.env.return_queue)\n",
    "\n",
    "                    # Get episode statistics from info (\"episode\" key only exist when episode is done)\n",
    "                    episode_reward = info[\"episode\"][\"r\"]\n",
    "                    self.writer.add_scalar(\"Charts/episodic_return\", episode_reward, self.timesteps)\n",
    "                    self.writer.add_scalar(\"Charts/episodic_length\", info[\"episode\"][\"l\"], self.timesteps)\n",
    "\n",
    "                    self.writer.add_scalar(\"Charts/Mean/last_100_mean_reward_on_timesteps\", mean_reward, self.timesteps)\n",
    "                    self.writer.add_scalar(\"Charts/Mean/last_100_mean_reward_on_episodes\", mean_reward, self.env.episode_count)\n",
    "\n",
    "                    self.writer.add_scalar(\"Charts/epsilon\", self.epsilon, self.timesteps)\n",
    "\n",
    "\n",
    "                    print(f\"Episode {self.env.episode_count} finished (timesteps: {self.timesteps}/{self.MAX_TIMESTEPS})\\n\"\n",
    "                          f\"Epsilon: {self.epsilon:.2f}, Episode reward: {episode_reward.item()}, Mean reward: {mean_reward:.2f}\")\n",
    "\n",
    "                    if self.env.episode_count % self.CHECKPOINT_INTERVAL_EPISODE == 0:\n",
    "                        self.save_checkpoint()\n",
    "                    print(\"***************************\")\n",
    "\n",
    "                if self.timesteps >= self.LEARNING_STARTS and self.timesteps % self.TARGET_UPDATE_INTERVAL == 0:\n",
    "                    self.update_target_network()\n",
    "                    #print(\"Target model updated.\")\n",
    "\n",
    "        self.save_checkpoint() # Save last checkpoint at the end of training\n",
    "\n",
    "        self.writer.flush()\n",
    "        self.writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-09T11:59:42.720987Z",
     "end_time": "2023-04-09T11:59:42.748900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deewens/miniconda3/envs/stable-baselines3/lib/python3.8/site-packages/stable_baselines3/common/buffers.py:229: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.82GB > 1.62GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading most recent checkpoint from runs/checkpoints/cnn_ddqn_ALE-Pong-v5_08-04-2023_20:00:00\n",
      "Checkpoint successfully loaded, you can resume the training now.\n",
      "Episode 2001 finished (timesteps: 4671335/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 14.62\n",
      "***************************\n",
      "Episode 2002 finished (timesteps: 4673556/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 14.65\n",
      "***************************\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[179], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#agent.run()\u001B[39;00m\n\u001B[1;32m      3\u001B[0m agent\u001B[38;5;241m.\u001B[39mload_last_checkpoint(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mruns/checkpoints/cnn_ddqn_ALE-Pong-v5_08-04-2023_20:00:00\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[177], line 221\u001B[0m, in \u001B[0;36mDQNAgent.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    218\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimesteps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 221\u001B[0m     action \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mact\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    222\u001B[0m     next_state, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[1;32m    223\u001B[0m     done \u001B[38;5;241m=\u001B[39m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated\n",
      "Cell \u001B[0;32mIn[177], line 98\u001B[0m, in \u001B[0;36mDQNAgent.act\u001B[0;34m(self, state)\u001B[0m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     97\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 98\u001B[0m         state_tensor \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     99\u001B[0m         q_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy_network(state_tensor)\n\u001B[1;32m    100\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m q_values\u001B[38;5;241m.\u001B[39margmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "agent = DQNAgent()\n",
    "#agent.run()\n",
    "agent.load_last_checkpoint(\"runs/checkpoints/cnn_ddqn_ALE-Pong-v5_08-04-2023_20:00:00\")\n",
    "agent.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
