{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T14:13:51.588410Z",
     "end_time": "2023-04-11T14:13:51.636821Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStack, RecordEpisodeStatistics, RecordVideo\n",
    "\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from stable_baselines3.common.save_util import load_from_pkl, save_to_pkl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from typing import Dict\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T14:15:57.810009Z",
     "end_time": "2023-04-11T14:15:57.852213Z"
    }
   },
   "outputs": [],
   "source": [
    "def step_trigger(step: int):\n",
    "    return step % 400_000 == 0\n",
    "\n",
    "def make_env(env_name=\"ALE/Pong-v5\", seed=42):\n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\", full_action_space=False, frameskip=1)\n",
    "    env = AtariPreprocessing(env)\n",
    "    env = FrameStack(env, 4)\n",
    "    env = RecordEpisodeStatistics(env)\n",
    "    # A video will be recorded every 400,000 steps.\n",
    "    env = RecordVideo(env, \"runs/videos/\", step_trigger=step_trigger, video_length=1000)\n",
    "    env.observation_space.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T14:15:58.120658Z",
     "end_time": "2023-04-11T14:15:58.152013Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_channels, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_channels, 32, stride=4, kernel_size=8)\n",
    "        self.conv2 = nn.Conv2d(32, 64, stride=2, kernel_size=4)\n",
    "        self.conv3 = nn.Conv2d(64, 64, stride=1, kernel_size=3)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # 64 * 7 * 7 is the output of last conv layer because of the formula above, for an input of 84*84\n",
    "        self.linear = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.head = nn.Linear(512, num_actions)  # Head layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float() / 255  # Rescale input from [0, 255] to [0, 1]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.linear(x))\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **policy network**: $Q^{A}_{\\theta}$\n",
    "- **target network**: $Q^{B}_{\\theta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T14:15:58.834706Z",
     "end_time": "2023-04-11T14:15:58.841192Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def linear_schedule(start_epsilon: float, end_epsilon: float, duration: int, timestep: int):\n",
    "    slope = (end_epsilon - start_epsilon) / duration\n",
    "    return max(slope * timestep + start_epsilon, end_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T14:15:59.252903Z",
     "end_time": "2023-04-11T14:15:59.301668Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_latest_checkpoint_file(files: List[str]) -> any:\n",
    "    \"\"\"\n",
    "    Return the most recent checkpoint file from the passed list of files.\n",
    "\n",
    "    If multiple files with same datetime are passed, only the first is returned\n",
    "\n",
    "    :param files: list of file names containing a formatted datetime (=> %d-%m-%Y_%H:%M:%S)\n",
    "    :return: the file with the most recent date time or ``None`` if no files were found (because of the lack of correctly formatted date in the file name)\n",
    "    \"\"\"\n",
    "    datetime_regex = r\"\\d{2}-\\d{2}-\\d{4}_\\d{2}:\\d{2}:\\d{2}\"\n",
    "\n",
    "    latest_file = None\n",
    "    latest_datetime = datetime.min\n",
    "    for file in files:\n",
    "        match = re.search(datetime_regex, file)\n",
    "        if not match: continue # Go to next element in list if no match is found\n",
    "\n",
    "        file_datetime = datetime.strptime(match.group(), \"%d-%m-%Y_%H:%M:%S\")\n",
    "        if file_datetime > latest_datetime:\n",
    "            latest_datetime = file_datetime\n",
    "            latest_file = file\n",
    "\n",
    "    return latest_file\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, name=\"CNN_DDQN_Pong-v5\", env_name=\"ALE/Pong-v5\"):\n",
    "        self.name = name\n",
    "\n",
    "        self.env = make_env(env_name)\n",
    "\n",
    "        self.start_datetime = None\n",
    "        self.start_time = None\n",
    "\n",
    "        # I use the same hyperparameters as this model: https://huggingface.co/sb3/dqn-PongNoFrameskip-v4\n",
    "\n",
    "        self.MAX_TIMESTEPS = 10_000_000  # Maximum number of total steps\n",
    "        self.TARGET_UPDATE_INTERVAL = 1000  # Number of steps between the synchronisation of q and target network\n",
    "        self.LEARNING_STARTS = 100_000  # The number of steps to wait before we start the training, so the agent can explore and store its experience in the replay buffer\n",
    "\n",
    "        self.TRAIN_FREQUENCY = 4 # Training is done each 4 steps\n",
    "\n",
    "        self.CHECKPOINT_INTERVAL_EPISODE = 500 # Checkpoint saving interval per episode (a checkpoint will be saved each X episodes)\n",
    "\n",
    "        self.REPLAY_SIZE = 100_000\n",
    "        self.BATCH_SIZE = 32\n",
    "\n",
    "        self.GAMMA = 0.99  # Discount rate\n",
    "\n",
    "        self.EXPLORATION_FRACTION = 0.1  # The fraction of 'TOTAL_TIMESTEPS' it takes from 'EPSILON_START' to 'EPSILON_END'.\n",
    "        self.EPSILON_INITIAL = 1.0\n",
    "        self.EPSILON_FINAL = 0.01\n",
    "\n",
    "        self.epsilon = self.EPSILON_INITIAL  # Exploration probability\n",
    "\n",
    "        self.memory = ReplayBuffer(\n",
    "            buffer_size=self.REPLAY_SIZE,\n",
    "            observation_space=self.env.observation_space,\n",
    "            action_space=self.env.action_space,\n",
    "            device=device,\n",
    "            optimize_memory_usage=True,\n",
    "            handle_timeout_termination=False\n",
    "        )\n",
    "\n",
    "        self.timesteps = 0\n",
    "\n",
    "        self.policy_network = DQN(4, self.env.action_space.n).to(device)\n",
    "        self.target_network = DQN(4, self.env.action_space.n).to(device)\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.policy_network.parameters(), lr=0.0001)\n",
    "        self.loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "        # Metrics/Logs\n",
    "        self.PATH = \"runs\"\n",
    "        if not os.path.exists(self.PATH):\n",
    "            os.makedirs(self.PATH)\n",
    "\n",
    "        self.CHECKPOINTS_PATH = f\"{self.PATH}/checkpoints\"\n",
    "        self.LOGS_PATH = f\"{self.PATH}/logs\"\n",
    "        self.VIDEO_PATH = f\"{self.PATH}/videos\"\n",
    "\n",
    "        self.is_loaded_from_checkpoint = False\n",
    "        self.writer = None\n",
    "\n",
    "    def remember(self, observation, next_observation, action, reward, done, infos):\n",
    "        self.memory.add(observation, next_observation, action, reward, done, infos)\n",
    "\n",
    "    def act(self, state):\n",
    "        # Reduce epsilon when learning started\n",
    "        if self.timesteps >= self.LEARNING_STARTS:\n",
    "            # Minus LEARNING_STARTS to takes into account that learning only started after LEARNING_STARTS,\n",
    "            # and so we want to start reducing epsilon only when learning start\n",
    "            self.epsilon = linear_schedule(\n",
    "                self.EPSILON_INITIAL,\n",
    "                self.EPSILON_FINAL,\n",
    "                int(self.EXPLORATION_FRACTION * self.MAX_TIMESTEPS),\n",
    "                self.timesteps - self.LEARNING_STARTS\n",
    "            )\n",
    "\n",
    "        if self.timesteps < self.LEARNING_STARTS or np.random.rand() < self.epsilon:\n",
    "            # Random action\n",
    "            return np.array(self.env.action_space.sample())\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor(np.array(state), device=device).unsqueeze(0)\n",
    "                q_values = self.policy_network(state_tensor)\n",
    "                return q_values.argmax(dim=1)[0].cpu().numpy()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "\n",
    "    def optimize_model(self):\n",
    "        minibatch = self.memory.sample(self.BATCH_SIZE)\n",
    "\n",
    "        # Calculate Q values for current states\n",
    "        # For each q_values, get the action according to the minibatch\n",
    "        q_values = self.policy_network(minibatch.observations).gather(1, minibatch.actions)\n",
    "\n",
    "        # Then, calculate the best actions for the next states, and return its indices\n",
    "        with torch.no_grad():\n",
    "            best_next_actions = self.policy_network(minibatch.next_observations).argmax(1).unsqueeze(1)\n",
    "\n",
    "        # Calculate the Q values for the next states using the target network, and return the action according to the best next action returned by the q network\n",
    "        target_next_q_values = self.target_network(minibatch.next_observations).gather(1, best_next_actions)\n",
    "\n",
    "        # Calculate the target Q values using Double DQN\n",
    "        target_q_values = minibatch.rewards + (1 - minibatch.dones) * self.GAMMA * target_next_q_values\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = self.loss_fn(q_values, target_q_values)\n",
    "\n",
    "        # Compute metrics for loss\n",
    "        if self.timesteps % 100 == 0:\n",
    "            self.writer.add_scalar(\"Loss/td_loss\", loss, self.timesteps)\n",
    "            self.writer.add_scalar(\"Loss/q_values\", q_values.squeeze().mean().item(), self.timesteps)\n",
    "            steps_per_second = int(self.timesteps / (time.time() - self.start_time))\n",
    "            #print(\"Steps per second: \", steps_per_second)\n",
    "            self.writer.add_scalar(\"Charts/steps_per_second\", steps_per_second, self.timesteps)\n",
    "\n",
    "\n",
    "        # Optimise Q network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        if self.start_datetime is None:\n",
    "            print(\"SAVE_CHECKPOINT_ERROR: Training need to have started to save a checkpoint.\")\n",
    "            return\n",
    "\n",
    "        print(\"Saving checkpoint...\")\n",
    "        current_datetime_str = datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
    "        start_datetime_str = self.start_datetime.strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
    "\n",
    "        save_parent_directory = f\"{self.CHECKPOINTS_PATH}/{self.name}_{start_datetime_str}\"\n",
    "        save_path = save_parent_directory + \"/chkpt_\" + current_datetime_str + \".tar\"\n",
    "        replay_buffer_path = save_parent_directory + \"/replay_buffer_\" + current_datetime_str\n",
    "\n",
    "        if not os.path.exists(save_parent_directory):\n",
    "            os.makedirs(save_parent_directory)\n",
    "\n",
    "        checkpoint = {\n",
    "            \"env\": self.env,\n",
    "            \"timesteps\": self.timesteps,\n",
    "            \"start_datetime\": self.start_datetime,\n",
    "            \"epsilon\": self.epsilon,\n",
    "            \"policy_network\": self.policy_network.state_dict(),\n",
    "            \"target_network\": self.target_network.state_dict(),\n",
    "            \"optimizer\": self.optimizer.state_dict(),\n",
    "        }\n",
    "\n",
    "        torch.save(checkpoint, save_path)\n",
    "        # Saving the replay buffer will takes time! But it is needed to properly resume training\n",
    "        save_to_pkl(replay_buffer_path, self.memory, verbose=1)\n",
    "\n",
    "        print(f\"Checkpoint saved into {save_parent_directory}\")\n",
    "\n",
    "    def load_last_checkpoint(self, path):\n",
    "        \"\"\"\n",
    "        Load the last saved checkpoint found in the given ``path``\n",
    "\n",
    "        :param path: the path to the directory containing the checkpoint(s)\n",
    "        \"\"\"\n",
    "        print(f\"Loading most recent checkpoint from {path}\")\n",
    "        self.is_loaded_from_checkpoint = True\n",
    "\n",
    "        # Using list comprehension to filter directories and only get the files\n",
    "        files = [file for file in os.listdir(path) if os.path.isfile(os.path.join(path, file))]\n",
    "\n",
    "        checkpoint_files = [chkpt_file for chkpt_file in files if \"chkpt\" in chkpt_file]\n",
    "        replay_buffer_files = [chkpt_file for chkpt_file in files if \"replay_buffer\" in chkpt_file]\n",
    "\n",
    "        checkpoint_file = get_latest_checkpoint_file(checkpoint_files)\n",
    "        replay_buffer_file = get_latest_checkpoint_file(replay_buffer_files)\n",
    "\n",
    "        checkpoint: Dict[str, any] = torch.load(path + \"/\" + checkpoint_file)\n",
    "\n",
    "        self.env = checkpoint[\"env\"]\n",
    "        self.timesteps = checkpoint[\"timesteps\"]\n",
    "        self.start_datetime: datetime = checkpoint[\"start_datetime\"]\n",
    "        self.start_time = self.start_datetime.timestamp()\n",
    "\n",
    "        self.epsilon = checkpoint[\"epsilon\"]\n",
    "\n",
    "        self.policy_network.load_state_dict(checkpoint[\"policy_network\"])\n",
    "        self.target_network.load_state_dict(checkpoint[\"target_network\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "        self.memory: ReplayBuffer = load_from_pkl(path + \"/\" + replay_buffer_file)\n",
    "        print(\"Checkpoint successfully loaded, you can resume the training now.\")\n",
    "\n",
    "    def run(self):\n",
    "        # Either create a new SummaryWriter or resume from previous one\n",
    "        if not self.is_loaded_from_checkpoint:\n",
    "            current_datetime = datetime.now()\n",
    "            self.start_datetime = current_datetime\n",
    "            self.start_time = current_datetime.timestamp()\n",
    "\n",
    "        start_datetime_str = self.start_datetime.strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
    "        self.writer = SummaryWriter(f\"{self.LOGS_PATH}/{self.name}_{start_datetime_str}\")\n",
    "\n",
    "        video_folder_path = f\"{self.VIDEO_PATH}/{self.name}_{start_datetime_str}\"\n",
    "        if not os.path.exists(video_folder_path):\n",
    "            os.makedirs(video_folder_path)\n",
    "        self.env.video_folder = video_folder_path\n",
    "\n",
    "        while self.timesteps < self.MAX_TIMESTEPS:\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                self.timesteps += 1\n",
    "\n",
    "                action = self.act(state)\n",
    "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                self.remember(state, next_state, action, reward, terminated, info)\n",
    "\n",
    "                if self.timesteps >= self.LEARNING_STARTS and self.timesteps % self.TRAIN_FREQUENCY == 0:\n",
    "                    self.optimize_model()\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    mean_reward = np.mean(self.env.return_queue)\n",
    "\n",
    "                    # Get episode statistics from info (\"episode\" key only exist when episode is done)\n",
    "                    episode_reward = info[\"episode\"][\"r\"]\n",
    "                    self.writer.add_scalar(\"Charts/episodic_return\", episode_reward, self.timesteps)\n",
    "                    self.writer.add_scalar(\"Charts/episodic_length\", info[\"episode\"][\"l\"], self.timesteps)\n",
    "\n",
    "                    self.writer.add_scalar(\"Charts/Mean/last_100_mean_reward_on_timesteps\", mean_reward, self.timesteps)\n",
    "                    self.writer.add_scalar(\"Charts/Mean/last_100_mean_reward_on_episodes\", mean_reward, self.env.episode_count)\n",
    "\n",
    "                    self.writer.add_scalar(\"Charts/epsilon\", self.epsilon, self.timesteps)\n",
    "\n",
    "\n",
    "                    print(f\"Episode {self.env.episode_count} finished (timesteps: {self.timesteps}/{self.MAX_TIMESTEPS})\\n\"\n",
    "                          f\"Epsilon: {self.epsilon:.2f}, Episode reward: {episode_reward.item()}, Mean reward: {mean_reward:.2f}\")\n",
    "\n",
    "                    if self.env.episode_count % self.CHECKPOINT_INTERVAL_EPISODE == 0:\n",
    "                        self.save_checkpoint()\n",
    "                    print(\"***************************\")\n",
    "\n",
    "                if self.timesteps >= self.LEARNING_STARTS and self.timesteps % self.TARGET_UPDATE_INTERVAL == 0:\n",
    "                    self.update_target_network()\n",
    "                    #print(\"Target model updated.\")\n",
    "\n",
    "        self.save_checkpoint() # Save last checkpoint at the end of training\n",
    "\n",
    "        self.writer.flush()\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-09T11:59:42.720987Z",
     "end_time": "2023-04-09T11:59:42.748900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deewens/miniconda3/envs/stable-baselines3/lib/python3.8/site-packages/gymnasium/wrappers/record_video.py:87: UserWarning: \u001B[33mWARN: Overwriting existing videos at /home/deewens/Documents/Studies/Fourth Year Project/Sources/RL-and-NN-Experiments/algorithms/pytorch/runs/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001B[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished (timesteps: 944/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -19.00\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_14:16:00/rl-video-step-0.mp4.\n",
      "Moviepy - Writing video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_14:16:00/rl-video-step-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/CNN_DDQN_Pong-v5_11-04-2023_14:16:00/rl-video-step-0.mp4\n",
      "Episode 2 finished (timesteps: 1852/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.00\n",
      "***************************\n",
      "Episode 3 finished (timesteps: 2614/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 4 finished (timesteps: 3521/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.50\n",
      "***************************\n",
      "Episode 5 finished (timesteps: 4422/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.40\n",
      "***************************\n",
      "Episode 6 finished (timesteps: 5609/10000000)\n",
      "Epsilon: 1.00, Episode reward: -18.0, Mean reward: -20.00\n",
      "***************************\n",
      "Episode 7 finished (timesteps: 6424/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.14\n",
      "***************************\n",
      "Episode 8 finished (timesteps: 7336/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.12\n",
      "***************************\n",
      "Episode 9 finished (timesteps: 8158/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 10 finished (timesteps: 9132/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.20\n",
      "***************************\n",
      "Episode 11 finished (timesteps: 9950/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 12 finished (timesteps: 10842/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 13 finished (timesteps: 11811/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 14 finished (timesteps: 12658/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.36\n",
      "***************************\n",
      "Episode 15 finished (timesteps: 13540/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.40\n",
      "***************************\n",
      "Episode 16 finished (timesteps: 14397/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.44\n",
      "***************************\n",
      "Episode 17 finished (timesteps: 15268/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.47\n",
      "***************************\n",
      "Episode 18 finished (timesteps: 16255/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.44\n",
      "***************************\n",
      "Episode 19 finished (timesteps: 17200/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.47\n",
      "***************************\n",
      "Episode 20 finished (timesteps: 18232/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.45\n",
      "***************************\n",
      "Episode 21 finished (timesteps: 19110/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.48\n",
      "***************************\n",
      "Episode 22 finished (timesteps: 20016/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.50\n",
      "***************************\n",
      "Episode 23 finished (timesteps: 21049/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.43\n",
      "***************************\n",
      "Episode 24 finished (timesteps: 22159/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.38\n",
      "***************************\n",
      "Episode 25 finished (timesteps: 23024/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.36\n",
      "***************************\n",
      "Episode 26 finished (timesteps: 24006/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 27 finished (timesteps: 24795/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 28 finished (timesteps: 25613/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.36\n",
      "***************************\n",
      "Episode 29 finished (timesteps: 26401/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.38\n",
      "***************************\n",
      "Episode 30 finished (timesteps: 27193/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.40\n",
      "***************************\n",
      "Episode 31 finished (timesteps: 28159/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.42\n",
      "***************************\n",
      "Episode 32 finished (timesteps: 29233/10000000)\n",
      "Epsilon: 1.00, Episode reward: -18.0, Mean reward: -20.34\n",
      "***************************\n",
      "Episode 33 finished (timesteps: 30108/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.36\n",
      "***************************\n",
      "Episode 34 finished (timesteps: 31031/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.38\n",
      "***************************\n",
      "Episode 35 finished (timesteps: 31992/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.37\n",
      "***************************\n",
      "Episode 36 finished (timesteps: 33177/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.36\n",
      "***************************\n",
      "Episode 37 finished (timesteps: 34255/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 38 finished (timesteps: 35168/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 39 finished (timesteps: 36124/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 40 finished (timesteps: 37201/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 41 finished (timesteps: 38022/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.29\n",
      "***************************\n",
      "Episode 42 finished (timesteps: 39130/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 43 finished (timesteps: 40112/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 44 finished (timesteps: 40935/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 45 finished (timesteps: 41794/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 46 finished (timesteps: 42923/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 47 finished (timesteps: 43894/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 48 finished (timesteps: 44731/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 49 finished (timesteps: 45583/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 50 finished (timesteps: 46406/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 51 finished (timesteps: 47215/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 52 finished (timesteps: 48160/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 53 finished (timesteps: 49065/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 54 finished (timesteps: 49829/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.30\n",
      "***************************\n",
      "Episode 55 finished (timesteps: 50707/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 56 finished (timesteps: 51672/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.30\n",
      "***************************\n",
      "Episode 57 finished (timesteps: 52598/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 58 finished (timesteps: 53445/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 59 finished (timesteps: 54361/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 60 finished (timesteps: 55254/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 61 finished (timesteps: 56150/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 62 finished (timesteps: 57156/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 63 finished (timesteps: 58143/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 64 finished (timesteps: 58965/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.34\n",
      "***************************\n",
      "Episode 65 finished (timesteps: 59832/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.34\n",
      "***************************\n",
      "Episode 66 finished (timesteps: 60740/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.35\n",
      "***************************\n",
      "Episode 67 finished (timesteps: 61687/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.34\n",
      "***************************\n",
      "Episode 68 finished (timesteps: 62675/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.34\n",
      "***************************\n",
      "Episode 69 finished (timesteps: 63709/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 70 finished (timesteps: 64618/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.34\n",
      "***************************\n",
      "Episode 71 finished (timesteps: 65456/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.34\n",
      "***************************\n",
      "Episode 72 finished (timesteps: 66365/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.35\n",
      "***************************\n",
      "Episode 73 finished (timesteps: 67125/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.36\n",
      "***************************\n",
      "Episode 74 finished (timesteps: 68007/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.36\n",
      "***************************\n",
      "Episode 75 finished (timesteps: 68986/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.37\n",
      "***************************\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m agent \u001B[38;5;241m=\u001B[39m DQNAgent()\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#agent.load_last_checkpoint(\"runs/checkpoints/cnn_ddqn_ALE-Pong-v5_08-04-2023_20:00:00\")\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[31], line 236\u001B[0m, in \u001B[0;36mDQNAgent.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    233\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimesteps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    235\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mact(state)\n\u001B[0;32m--> 236\u001B[0m next_state, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    237\u001B[0m done \u001B[38;5;241m=\u001B[39m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated\n\u001B[1;32m    239\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mremember(state, next_state, action, reward, terminated, info)\n",
      "File \u001B[0;32m~/miniconda3/envs/stable-baselines3/lib/python3.8/site-packages/gymnasium/wrappers/record_video.py:155\u001B[0m, in \u001B[0;36mRecordVideo.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[1;32m    148\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Steps through the environment using action, recording observations if :attr:`self.recording`.\"\"\"\u001B[39;00m\n\u001B[1;32m    149\u001B[0m     (\n\u001B[1;32m    150\u001B[0m         observations,\n\u001B[1;32m    151\u001B[0m         rewards,\n\u001B[1;32m    152\u001B[0m         terminateds,\n\u001B[1;32m    153\u001B[0m         truncateds,\n\u001B[1;32m    154\u001B[0m         infos,\n\u001B[0;32m--> 155\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    157\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mterminated \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtruncated):\n\u001B[1;32m    158\u001B[0m         \u001B[38;5;66;03m# increment steps and episodes\u001B[39;00m\n\u001B[1;32m    159\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep_id \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/stable-baselines3/lib/python3.8/site-packages/gymnasium/wrappers/record_episode_statistics.py:89\u001B[0m, in \u001B[0;36mRecordEpisodeStatistics.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[1;32m     82\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Steps through the environment, recording the episode statistics.\"\"\"\u001B[39;00m\n\u001B[1;32m     83\u001B[0m     (\n\u001B[1;32m     84\u001B[0m         observations,\n\u001B[1;32m     85\u001B[0m         rewards,\n\u001B[1;32m     86\u001B[0m         terminations,\n\u001B[1;32m     87\u001B[0m         truncations,\n\u001B[1;32m     88\u001B[0m         infos,\n\u001B[0;32m---> 89\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     90\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[1;32m     91\u001B[0m         infos, \u001B[38;5;28mdict\u001B[39m\n\u001B[1;32m     92\u001B[0m     ), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`info` dtype is \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(infos)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m while supported dtype is `dict`. This may be due to usage of other wrappers in the wrong order.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepisode_returns \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m rewards\n",
      "File \u001B[0;32m~/miniconda3/envs/stable-baselines3/lib/python3.8/site-packages/gymnasium/wrappers/frame_stack.py:179\u001B[0m, in \u001B[0;36mFrameStack.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[1;32m    171\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Steps through the environment, appending the observation to the frame buffer.\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \n\u001B[1;32m    173\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;124;03m        Stacked observations, reward, terminated, truncated, and information from the environment\u001B[39;00m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 179\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframes\u001B[38;5;241m.\u001B[39mappend(observation)\n\u001B[1;32m    181\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation(\u001B[38;5;28;01mNone\u001B[39;00m), reward, terminated, truncated, info\n",
      "File \u001B[0;32m~/miniconda3/envs/stable-baselines3/lib/python3.8/site-packages/gymnasium/wrappers/atari_preprocessing.py:138\u001B[0m, in \u001B[0;36mAtariPreprocessing.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    135\u001B[0m total_reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28;01mFalse\u001B[39;00m, {}\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframe_skip):\n\u001B[0;32m--> 138\u001B[0m     _, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    139\u001B[0m     total_reward \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m reward\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgame_over \u001B[38;5;241m=\u001B[39m terminated\n",
      "File \u001B[0;32m~/miniconda3/envs/stable-baselines3/lib/python3.8/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling env.reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 56\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/stable-baselines3/lib/python3.8/site-packages/gymnasium/wrappers/env_checker.py:49\u001B[0m, in \u001B[0;36mPassiveEnvChecker.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_step_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, action)\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 49\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/stable-baselines3/lib/python3.8/site-packages/shimmy/atari_env.py:294\u001B[0m, in \u001B[0;36mAtariEnv.step\u001B[0;34m(self, action_ind)\u001B[0m\n\u001B[1;32m    292\u001B[0m reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m    293\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(frameskip):\n\u001B[0;32m--> 294\u001B[0m     reward \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43male\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mact\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    295\u001B[0m is_terminal \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39male\u001B[38;5;241m.\u001B[39mgame_over(with_truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    296\u001B[0m is_truncated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39male\u001B[38;5;241m.\u001B[39mgame_truncated()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "agent = DQNAgent()\n",
    "#agent.load_last_checkpoint(\"runs/checkpoints/cnn_ddqn_ALE-Pong-v5_08-04-2023_20:00:00\")\n",
    "agent.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
