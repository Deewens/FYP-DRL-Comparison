{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-12T15:08:38.245964Z",
     "end_time": "2023-04-12T15:08:38.288296Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStack, RecordEpisodeStatistics, RecordVideo\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from stable_baselines3.common.save_util import load_from_pkl, save_to_pkl\n",
    "\n",
    "from algorithms.common.typing import HParams\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-12T15:27:13.164188Z",
     "end_time": "2023-04-12T15:27:13.212210Z"
    }
   },
   "outputs": [],
   "source": [
    "def step_trigger(step: int):\n",
    "    return step % 400_000 == 0\n",
    "\n",
    "def make_env(env_name=\"ALE/Pong-v5\", seed=42):\n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\", full_action_space=False, frameskip=1)\n",
    "    env = AtariPreprocessing(env)\n",
    "    env = FrameStack(env, 4)\n",
    "    env = RecordEpisodeStatistics(env)\n",
    "    # A video will be recorded every 400,000 steps.\n",
    "    # (Can't use lambda expression because it is not supported by cloud pickle when saving checkpoint model)\n",
    "    env = RecordVideo(env, \"runs/videos/\", step_trigger=step_trigger, video_length=1000)\n",
    "\n",
    "    env.observation_space.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T19:37:01.135301Z",
     "end_time": "2023-04-11T19:37:01.137849Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_channels, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_channels, 32, stride=4, kernel_size=8)\n",
    "        self.conv2 = nn.Conv2d(32, 64, stride=2, kernel_size=4)\n",
    "        self.conv3 = nn.Conv2d(64, 64, stride=1, kernel_size=3)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # 64 * 7 * 7 is the output of last conv layer, for an input of 84*84\n",
    "        self.linear = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.head = nn.Linear(512, num_actions)  # Head layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float() / 255  # Rescale input from [0, 255] to [0, 1]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.linear(x))\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **policy network**: $Q^{A}_{\\theta}$\n",
    "- **target network**: $Q^{B}_{\\theta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T19:37:01.138321Z",
     "end_time": "2023-04-11T19:37:01.204815Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def linear_schedule(start_epsilon: float, end_epsilon: float, duration: int, timestep: int):\n",
    "    slope = (end_epsilon - start_epsilon) / duration\n",
    "    return max(slope * timestep + start_epsilon, end_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T19:37:01.206563Z",
     "end_time": "2023-04-11T19:37:01.291434Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_latest_checkpoint_file(files: List[str]) -> any:\n",
    "    \"\"\"\n",
    "    Return the most recent checkpoint file from the passed list of files.\n",
    "\n",
    "    If multiple files with same datetime are passed, only the first is returned\n",
    "\n",
    "    :param files: list of file names containing a formatted datetime (=> %d-%m-%Y_%H:%M:%S)\n",
    "    :return: the file with the most recent date time or ``None`` if no files were found (because of the lack of correctly formatted date in the file name)\n",
    "    \"\"\"\n",
    "    datetime_regex = r\"\\d{2}-\\d{2}-\\d{4}_\\d{2}:\\d{2}:\\d{2}\"\n",
    "\n",
    "    latest_file = None\n",
    "    latest_datetime = datetime.min\n",
    "    for file in files:\n",
    "        match = re.search(datetime_regex, file)\n",
    "        if not match: continue # Go to next element in list if no match is found\n",
    "\n",
    "        file_datetime = datetime.strptime(match.group(), \"%d-%m-%Y_%H:%M:%S\")\n",
    "        if file_datetime > latest_datetime:\n",
    "            latest_datetime = file_datetime\n",
    "            latest_file = file\n",
    "\n",
    "    return latest_file\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, name, env_id, hparams: HParams):\n",
    "        self.name = name\n",
    "        self.env = make_env(env_id, seed=hparams[\"env_seed\"])\n",
    "\n",
    "        self.start_datetime = None\n",
    "        self.start_time = None\n",
    "\n",
    "        self.MAX_TIMESTEPS = hparams[\"max_timesteps\"]  # Maximum number of total steps\n",
    "        self.TARGET_UPDATE_INTERVAL = hparams[\"target_update_interval\"]  # Number of steps between the synchronisation of q and target network\n",
    "        self.LEARNING_STARTS = hparams[\"learning_starts\"]  # The number of steps to wait before we start the training, so the agent can explore and store its experience in the replay buffer\n",
    "\n",
    "        self.TRAIN_FREQUENCY = hparams[\"train_frequency\"] # Training is done each 4 steps\n",
    "\n",
    "        self.CHECKPOINT_INTERVAL_EPISODE = 1000 # Checkpoint saving interval per episode (a checkpoint will be saved each X episodes)\n",
    "\n",
    "        self.REPLAY_SIZE = hparams[\"replay_buffer_size\"]\n",
    "        self.BATCH_SIZE = hparams[\"batch_size\"]\n",
    "\n",
    "        self.GAMMA = 0.99  # Discount rate\n",
    "\n",
    "        self.EXPLORATION_FRACTION = hparams[\"exploration_fraction\"]  # The fraction of 'TOTAL_TIMESTEPS' it takes from 'EPSILON_START' to 'EPSILON_END'.\n",
    "        self.EPSILON_INITIAL = 1.0\n",
    "        self.EPSILON_FINAL = hparams[\"exploration_final\"]\n",
    "\n",
    "        self.epsilon = self.EPSILON_INITIAL  # Exploration probability\n",
    "\n",
    "        self.memory = ReplayBuffer(\n",
    "            buffer_size=self.REPLAY_SIZE,\n",
    "            observation_space=self.env.observation_space,\n",
    "            action_space=self.env.action_space,\n",
    "            device=device,\n",
    "            optimize_memory_usage=True,\n",
    "            handle_timeout_termination=False\n",
    "        )\n",
    "\n",
    "        self.timesteps = 0\n",
    "\n",
    "        self.policy_network = DQN(4, self.env.action_space.n).to(device)\n",
    "        self.target_network = DQN(4, self.env.action_space.n).to(device)\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.policy_network.parameters(), lr=hparams[\"learning_rate\"])\n",
    "        self.loss_fn = hparams[\"loss\"]\n",
    "\n",
    "        # Metrics/Logs\n",
    "        self.PATH = \"runs\"\n",
    "        if not os.path.exists(self.PATH):\n",
    "            os.makedirs(self.PATH)\n",
    "\n",
    "        self.CHECKPOINTS_PATH = f\"{self.PATH}/checkpoints\"\n",
    "        self.LOGS_PATH = f\"{self.PATH}/logs\"\n",
    "        self.VIDEO_PATH = f\"{self.PATH}/videos\"\n",
    "\n",
    "        self.is_loaded_from_checkpoint = False\n",
    "        self.writer = None\n",
    "\n",
    "    def remember(self, observation, next_observation, action, reward, done, infos):\n",
    "        self.memory.add(observation, next_observation, action, reward, done, infos)\n",
    "\n",
    "    def act(self, state):\n",
    "        # Reduce epsilon when learning started\n",
    "        if self.timesteps >= self.LEARNING_STARTS:\n",
    "            # Minus LEARNING_STARTS to takes into account that learning only started after LEARNING_STARTS,\n",
    "            # and so we want to start reducing epsilon only when learning start\n",
    "            self.epsilon = linear_schedule(\n",
    "                self.EPSILON_INITIAL,\n",
    "                self.EPSILON_FINAL,\n",
    "                int(self.EXPLORATION_FRACTION * self.MAX_TIMESTEPS),\n",
    "                self.timesteps - self.LEARNING_STARTS\n",
    "            )\n",
    "\n",
    "        if self.timesteps < self.LEARNING_STARTS or np.random.rand() < self.epsilon:\n",
    "            # Random action\n",
    "            return np.array(self.env.action_space.sample())\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor(np.array(state), device=device).unsqueeze(0)\n",
    "                q_values = self.policy_network(state_tensor)\n",
    "                return q_values.argmax(dim=1)[0].cpu().numpy()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "\n",
    "    def optimize_model(self):\n",
    "        minibatch = self.memory.sample(self.BATCH_SIZE)\n",
    "\n",
    "        # Calculate Q values for current states\n",
    "        # For each q_values, get the action according to the minibatch\n",
    "        q_values = self.policy_network(minibatch.observations).gather(1, minibatch.actions)\n",
    "\n",
    "        # Then, calculate the best actions for the next states, and return its indices\n",
    "        with torch.no_grad():\n",
    "            best_next_actions = self.policy_network(minibatch.next_observations).argmax(1).unsqueeze(1)\n",
    "\n",
    "        # Calculate the Q values for the next states using the target network, and return the action according to the best next action returned by the q network\n",
    "        target_next_q_values = self.target_network(minibatch.next_observations).gather(1, best_next_actions)\n",
    "\n",
    "        # Calculate the target Q values using Double DQN\n",
    "        target_q_values = minibatch.rewards + (1 - minibatch.dones) * self.GAMMA * target_next_q_values\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = self.loss_fn(q_values, target_q_values)\n",
    "\n",
    "        # Compute metrics for loss\n",
    "        if self.timesteps % 100 == 0:\n",
    "            self.writer.add_scalar(\"train/loss\", loss, self.timesteps)\n",
    "            self.writer.add_scalar(\"train/q_values\", q_values.squeeze().mean().item(), self.timesteps)\n",
    "            steps_per_second = int(self.timesteps / (time.time() - self.start_time))\n",
    "            #print(\"Steps per second: \", steps_per_second)\n",
    "            self.writer.add_scalar(\"train/steps_per_second\", steps_per_second, self.timesteps)\n",
    "\n",
    "\n",
    "        # Optimise Q network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        if self.start_datetime is None:\n",
    "            print(\"SAVE_CHECKPOINT_ERROR: Training need to have started to save a checkpoint.\")\n",
    "            return\n",
    "\n",
    "        print(\"Saving checkpoint...\")\n",
    "        current_datetime_str = datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
    "        start_datetime_str = self.start_datetime.strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
    "\n",
    "        save_parent_directory = f\"{self.CHECKPOINTS_PATH}/{self.name}_{start_datetime_str}\"\n",
    "        save_path = save_parent_directory + \"/chkpt_\" + current_datetime_str + \".tar\"\n",
    "        replay_buffer_path = save_parent_directory + \"/replay_buffer_\" + current_datetime_str\n",
    "\n",
    "        if not os.path.exists(save_parent_directory):\n",
    "            os.makedirs(save_parent_directory)\n",
    "\n",
    "        checkpoint = {\n",
    "            \"env\": self.env,\n",
    "            \"timesteps\": self.timesteps,\n",
    "            \"start_datetime\": self.start_datetime,\n",
    "            \"epsilon\": self.epsilon,\n",
    "            \"policy_network\": self.policy_network.state_dict(),\n",
    "            \"target_network\": self.target_network.state_dict(),\n",
    "            \"optimizer\": self.optimizer.state_dict(),\n",
    "        }\n",
    "\n",
    "        torch.save(checkpoint, save_path)\n",
    "        # Saving the replay buffer will takes time! But it is needed to properly resume training\n",
    "        save_to_pkl(replay_buffer_path, self.memory, verbose=1)\n",
    "\n",
    "        print(f\"Checkpoint saved into {save_parent_directory}\")\n",
    "\n",
    "    def load_last_checkpoint(self, path):\n",
    "        \"\"\"\n",
    "        Load the last saved checkpoint found in the given ``path``\n",
    "\n",
    "        :param path: the path to the directory containing the checkpoint(s)\n",
    "        \"\"\"\n",
    "        print(f\"Loading most recent checkpoint from {path}\")\n",
    "        self.is_loaded_from_checkpoint = True\n",
    "\n",
    "        # Using list comprehension to filter directories and only get the files\n",
    "        files = [file for file in os.listdir(path) if os.path.isfile(os.path.join(path, file))]\n",
    "\n",
    "        checkpoint_files = [chkpt_file for chkpt_file in files if \"chkpt\" in chkpt_file]\n",
    "        replay_buffer_files = [chkpt_file for chkpt_file in files if \"replay_buffer\" in chkpt_file]\n",
    "\n",
    "        checkpoint_file = get_latest_checkpoint_file(checkpoint_files)\n",
    "        replay_buffer_file = get_latest_checkpoint_file(replay_buffer_files)\n",
    "\n",
    "        checkpoint: Dict[str, any] = torch.load(path + \"/\" + checkpoint_file)\n",
    "\n",
    "        self.env = checkpoint[\"env\"]\n",
    "        self.timesteps = checkpoint[\"timesteps\"]\n",
    "        self.start_datetime: datetime = checkpoint[\"start_datetime\"]\n",
    "        self.start_time = self.start_datetime.timestamp()\n",
    "\n",
    "        self.epsilon = checkpoint[\"epsilon\"]\n",
    "\n",
    "        self.policy_network.load_state_dict(checkpoint[\"policy_network\"])\n",
    "        self.target_network.load_state_dict(checkpoint[\"target_network\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "        self.memory: ReplayBuffer = load_from_pkl(path + \"/\" + replay_buffer_file)\n",
    "        print(\"Checkpoint successfully loaded, you can resume the training now.\")\n",
    "\n",
    "    def run(self):\n",
    "        # Either create a new SummaryWriter or resume from previous one\n",
    "        if not self.is_loaded_from_checkpoint:\n",
    "            current_datetime = datetime.now()\n",
    "            self.start_datetime = current_datetime\n",
    "            self.start_time = current_datetime.timestamp()\n",
    "\n",
    "        start_datetime_str = self.start_datetime.strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
    "        self.writer = SummaryWriter(f\"{self.LOGS_PATH}/{self.name}_{start_datetime_str}\")\n",
    "\n",
    "        video_folder_path = f\"{self.VIDEO_PATH}/{self.name}_{start_datetime_str}\"\n",
    "        if not os.path.exists(video_folder_path):\n",
    "            os.makedirs(video_folder_path)\n",
    "        self.env.video_folder = video_folder_path\n",
    "\n",
    "        while self.timesteps < self.MAX_TIMESTEPS:\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                self.timesteps += 1\n",
    "\n",
    "                action = self.act(state)\n",
    "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                self.remember(state, next_state, action, reward, terminated, info)\n",
    "\n",
    "                if self.timesteps >= self.LEARNING_STARTS and self.timesteps % self.TRAIN_FREQUENCY == 0:\n",
    "                    self.optimize_model()\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    mean_reward = np.mean(self.env.return_queue)\n",
    "                    mean_length = np.mean(self.env.length_queue)\n",
    "\n",
    "                    # Get episode statistics from info (\"episode\" key only exist when episode is done)\n",
    "                    episode_reward = info[\"episode\"][\"r\"]\n",
    "                    self.writer.add_scalar(\"rollout/episodic_return\", episode_reward, self.timesteps)\n",
    "                    self.writer.add_scalar(\"rollout/episodic_length\", info[\"episode\"][\"l\"], self.timesteps)\n",
    "\n",
    "                    self.writer.add_scalar(\"rollout/ep_rew_mean\", mean_reward, self.timesteps)\n",
    "                    self.writer.add_scalar(\"rollout/ep_len_mean\", mean_length, self.timesteps)\n",
    "\n",
    "                    self.writer.add_scalar(\"rollout/exploration_rate\", self.epsilon, self.timesteps)\n",
    "\n",
    "\n",
    "                    print(f\"Episode {self.env.episode_count} finished (timesteps: {self.timesteps}/{self.MAX_TIMESTEPS})\\n\"\n",
    "                          f\"Epsilon: {self.epsilon:.2f}, Episode reward: {episode_reward.item()}, Mean reward: {mean_reward:.2f}\")\n",
    "\n",
    "                    if self.env.episode_count % self.CHECKPOINT_INTERVAL_EPISODE == 0:\n",
    "                        self.save_checkpoint()\n",
    "                    print(\"***************************\")\n",
    "\n",
    "                if self.timesteps >= self.LEARNING_STARTS and self.timesteps % self.TARGET_UPDATE_INTERVAL == 0:\n",
    "                    self.update_target_network()\n",
    "                    #print(\"Target model updated.\")\n",
    "\n",
    "        self.save_checkpoint() # Save last checkpoint at the end of training\n",
    "\n",
    "        self.writer.flush()\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T19:37:01.291338Z",
     "end_time": "2023-04-12T03:13:14.524312Z"
    }
   },
   "outputs": [],
   "source": [
    "# I use the same hyperparameters as this model: https://huggingface.co/sb3/dqn-PongNoFrameskip-v4\n",
    "hparams_config: HParams = {\n",
    "    \"env_seed\": 42,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"loss\": torch.nn.SmoothL1Loss,\n",
    "    \"max_timesteps\": 10_000_000,\n",
    "    \"target_update_interval\": 1000,\n",
    "    \"learning_starts\": 100_000,\n",
    "    \"train_frequency\": 4,\n",
    "    \"replay_buffer_size\": 10_000,\n",
    "    \"batch_size\": 32,\n",
    "    \"discount_rate\": 0.99,\n",
    "    \"exploration_fraction\": 0.1,\n",
    "    \"exploration_final\": 0.01,\n",
    "}\n",
    "\n",
    "agent = DQNAgent(name=\"CNN_Double_DQN_Pong-v5\", env_id=\"ALE/Pong-v5\", hparams=hparams_config)\n",
    "#agent.load_last_checkpoint(\"runs/checkpoints/cnn_ddqn_ALE-Pong-v5_08-04-2023_20:00:00\")\n",
    "agent.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
