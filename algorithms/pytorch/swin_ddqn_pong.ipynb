{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T14:25:18.725981Z",
     "start_time": "2023-04-11T14:25:15.591977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Fused window process have not been installed. Please refer to get_started.md for installation.\n",
      "Tutel has not been installed. To use Swin-MoE, please install Tutel; otherwise, just ignore this.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStack, RecordEpisodeStatistics, RecordVideo\n",
    "\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from stable_baselines3.common.save_util import load_from_pkl, save_to_pkl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from common.swin_transformer.models.swin_transformer import SwinTransformer\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from typing import Dict\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T15:50:06.983300Z",
     "start_time": "2023-04-09T15:50:06.982344Z"
    }
   },
   "outputs": [],
   "source": [
    "def step_trigger(step: int):\n",
    "    return step % 400_000 == 0\n",
    "\n",
    "def make_env(env_name=\"ALE/Pong-v5\", seed=42):\n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\", full_action_space=False, frameskip=1)\n",
    "    env = AtariPreprocessing(env)\n",
    "    env = FrameStack(env, 4)\n",
    "    env = RecordEpisodeStatistics(env)\n",
    "    # A video will be recorded every 400,000 steps.\n",
    "    env = RecordVideo(env, \"runs/videos/\", step_trigger=step_trigger, video_length=1000)\n",
    "    env.observation_space.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T14:25:27.770507Z",
     "start_time": "2023-04-11T14:25:27.363141Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1678402421473/work/aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SwinDQN(\n",
       "  (swin): SwinTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(4, 96, kernel_size=(3, 3), stride=(3, 3))\n",
       "      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        dim=96, input_resolution=(28, 28), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=96, input_resolution=(28, 28), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=96, window_size=(7, 7), num_heads=3\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=96, input_resolution=(28, 28), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=96, window_size=(7, 7), num_heads=3\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          input_resolution=(28, 28), dim=96\n",
       "          (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicLayer(\n",
       "        dim=192, input_resolution=(14, 14), depth=3\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=192, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=192, window_size=(7, 7), num_heads=3\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=192, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=192, window_size=(7, 7), num_heads=3\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SwinTransformerBlock(\n",
       "            dim=192, input_resolution=(14, 14), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=192, window_size=(7, 7), num_heads=3\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          input_resolution=(14, 14), dim=192\n",
       "          (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): BasicLayer(\n",
       "        dim=384, input_resolution=(7, 7), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            dim=384, input_resolution=(7, 7), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=384, window_size=(7, 7), num_heads=6\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "    (head): Linear(in_features=384, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SwinDQN(nn.Module):\n",
    "    def __init__(self, num_channels, num_actions):\n",
    "        super(SwinDQN, self).__init__()\n",
    "\n",
    "        self.swin = SwinTransformer(\n",
    "            img_size=84,\n",
    "            in_chans=num_channels,\n",
    "            num_classes=num_actions,\n",
    "            depths=[2, 3, 2],\n",
    "            num_heads=[3, 3, 6],\n",
    "            patch_size=3,\n",
    "            window_size=7,\n",
    "            embed_dim=96,\n",
    "            mlp_ratio=4,\n",
    "            drop_path_rate=0.1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float() / 255  # Rescale input from [0, 255] to [0, 1]\n",
    "        return self.swin(x)\n",
    "\n",
    "swin = SwinDQN(4, 6)\n",
    "swin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **policy network**: $Q^{A}_{\\theta}$\n",
    "- **target network**: $Q^{B}_{\\theta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T14:26:40.816894Z",
     "start_time": "2023-04-11T14:26:40.802611Z"
    }
   },
   "outputs": [],
   "source": [
    "def linear_schedule(start_epsilon: float, end_epsilon: float, duration: int, timestep: int):\n",
    "    slope = (end_epsilon - start_epsilon) / duration\n",
    "    return max(slope * timestep + start_epsilon, end_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T14:26:45.973074Z",
     "start_time": "2023-04-11T14:26:45.924016Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_latest_checkpoint_file(files: List[str]) -> any:\n",
    "    \"\"\"\n",
    "    Return the most recent checkpoint file from the passed list of files.\n",
    "\n",
    "    If multiple files with same datetime are passed, only the first is returned\n",
    "\n",
    "    :param files: list of file names containing a formatted datetime (=> %d-%m-%Y_%H:%M:%S)\n",
    "    :return: the file with the most recent date time or ``None`` if no files were found (because of the lack of correctly formatted date in the file name)\n",
    "    \"\"\"\n",
    "    datetime_regex = r\"\\d{2}-\\d{2}-\\d{4}_\\d{2}:\\d{2}:\\d{2}\"\n",
    "\n",
    "    latest_file = None\n",
    "    latest_datetime = datetime.min\n",
    "    for file in files:\n",
    "        match = re.search(datetime_regex, file)\n",
    "        if not match: continue # Go to next element in list if no match is found\n",
    "\n",
    "        file_datetime = datetime.strptime(match.group(), \"%d-%m-%Y_%H:%M:%S\")\n",
    "        if file_datetime > latest_datetime:\n",
    "            latest_datetime = file_datetime\n",
    "            latest_file = file\n",
    "\n",
    "    return latest_file\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, name=\"CNN_DDQN_Pong-v5\", env_name=\"ALE/Pong-v5\"):\n",
    "        self.name = name\n",
    "\n",
    "        self.env = make_env(env_name)\n",
    "\n",
    "        self.start_datetime = None\n",
    "        self.start_time = None\n",
    "\n",
    "        # I use the same hyperparameters as this model: https://huggingface.co/sb3/dqn-PongNoFrameskip-v4\n",
    "\n",
    "        self.MAX_TIMESTEPS = 10_000_000  # Maximum number of total steps\n",
    "        self.TARGET_UPDATE_INTERVAL = 1000  # Number of steps between the synchronisation of q and target network\n",
    "        self.LEARNING_STARTS = 100_000  # The number of steps to wait before we start the training, so the agent can explore and store its experience in the replay buffer\n",
    "\n",
    "        self.TRAIN_FREQUENCY = 4 # Training is done each 4 steps\n",
    "\n",
    "        self.CHECKPOINT_INTERVAL_EPISODE = 500 # Checkpoint saving interval per episode (a checkpoint will be saved each X episodes)\n",
    "        \n",
    "        self.REPLAY_SIZE = 10_000\n",
    "        self.BATCH_SIZE = 32\n",
    "\n",
    "        self.GAMMA = 0.99  # Discount rate\n",
    "\n",
    "        self.EXPLORATION_FRACTION = 0.1  # The fraction of 'TOTAL_TIMESTEPS' it takes from 'EPSILON_START' to 'EPSILON_END'.\n",
    "        self.EPSILON_INITIAL = 1.0\n",
    "        self.EPSILON_FINAL = 0.01\n",
    "\n",
    "        self.epsilon = self.EPSILON_INITIAL  # Exploration probability\n",
    "\n",
    "        self.memory = ReplayBuffer(\n",
    "            buffer_size=self.REPLAY_SIZE,\n",
    "            observation_space=self.env.observation_space,\n",
    "            action_space=self.env.action_space,\n",
    "            device=device,\n",
    "            optimize_memory_usage=True,\n",
    "            handle_timeout_termination=False\n",
    "        )\n",
    "\n",
    "        self.timesteps = 0\n",
    "\n",
    "        self.policy_network = SwinDQN(4, self.env.action_space.n).to(device)\n",
    "        self.target_network = SwinDQN(4, self.env.action_space.n).to(device)\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.policy_network.parameters(), lr=0.0001)\n",
    "        self.loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "        # Metrics/Logs\n",
    "        self.PATH = \"runs\"\n",
    "        if not os.path.exists(self.PATH):\n",
    "            os.makedirs(self.PATH)\n",
    "\n",
    "        self.CHECKPOINTS_PATH = f\"{self.PATH}/checkpoints\"\n",
    "        self.LOGS_PATH = f\"{self.PATH}/logs\"\n",
    "        self.VIDEO_PATH = f\"{self.PATH}/videos\"\n",
    "\n",
    "        self.is_loaded_from_checkpoint = False\n",
    "        self.writer = None\n",
    "\n",
    "    def remember(self, observation, next_observation, action, reward, done, infos):\n",
    "        self.memory.add(observation, next_observation, action, reward, done, infos)\n",
    "\n",
    "    def act(self, state):\n",
    "        # Reduce epsilon when learning started\n",
    "        if self.timesteps >= self.LEARNING_STARTS:\n",
    "             # Minus LEARNING_STARTS to takes into account that learning only started after LEARNING_STARTS,\n",
    "             # and so we want to start reducing epsilon only when learning start\n",
    "            self.epsilon = linear_schedule(\n",
    "                self.EPSILON_INITIAL,\n",
    "                self.EPSILON_FINAL,\n",
    "                int(self.EXPLORATION_FRACTION * self.MAX_TIMESTEPS),\n",
    "                self.timesteps - self.LEARNING_STARTS\n",
    "            )\n",
    "\n",
    "        if self.timesteps < self.LEARNING_STARTS or np.random.rand() < self.epsilon:\n",
    "            # Random action\n",
    "            return np.array(self.env.action_space.sample())\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor(np.array(state), device=device).unsqueeze(0)\n",
    "                q_values = self.policy_network(state_tensor)\n",
    "                return q_values.argmax(dim=1)[0].cpu().numpy()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "\n",
    "    def optimize_model(self):\n",
    "        minibatch = self.memory.sample(self.BATCH_SIZE)\n",
    "\n",
    "        # Calculate Q values for current states\n",
    "        # For each q_values, get the action according to the minibatch\n",
    "        q_values = self.policy_network(minibatch.observations).gather(1, minibatch.actions)\n",
    "\n",
    "        # Then, calculate the best actions for the next states, and return its indices\n",
    "        with torch.no_grad():\n",
    "            best_next_actions = self.policy_network(minibatch.next_observations).argmax(1).unsqueeze(1)\n",
    "\n",
    "        # Calculate the Q values for the next states using the target network, and return the action according to the best next action returned by the q network\n",
    "        target_next_q_values = self.target_network(minibatch.next_observations).gather(1, best_next_actions)\n",
    "\n",
    "        # Calculate the target Q values using Double DQN\n",
    "        target_q_values = minibatch.rewards + (1 - minibatch.dones) * self.GAMMA * target_next_q_values\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = self.loss_fn(q_values, target_q_values)\n",
    "\n",
    "        # Compute metrics for loss\n",
    "        if self.timesteps % 100 == 0:\n",
    "            self.writer.add_scalar(\"train/loss\", loss, self.timesteps)\n",
    "            self.writer.add_scalar(\"train/q_values\", q_values.squeeze().mean().item(), self.timesteps)\n",
    "            steps_per_second = int(self.timesteps / (time.time() - self.start_time))\n",
    "            #print(\"Steps per second: \", steps_per_second)\n",
    "            self.writer.add_scalar(\"train/steps_per_second\", steps_per_second, self.timesteps)\n",
    "\n",
    "\n",
    "        # Optimise Q network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        if self.start_datetime is None:\n",
    "            print(\"SAVE_CHECKPOINT_ERROR: Training need to have started to save a checkpoint.\")\n",
    "            return\n",
    "\n",
    "        print(\"Saving checkpoint...\")\n",
    "        current_datetime_str = datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
    "        start_datetime_str = self.start_datetime.strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
    "\n",
    "        save_parent_directory = f\"{self.CHECKPOINTS_PATH}/{self.name}_{start_datetime_str}\"\n",
    "        save_path = save_parent_directory + \"/chkpt_\" + current_datetime_str + \".tar\"\n",
    "        replay_buffer_path = save_parent_directory + \"/replay_buffer_\" + current_datetime_str\n",
    "\n",
    "        if not os.path.exists(save_parent_directory):\n",
    "            os.makedirs(save_parent_directory)\n",
    "\n",
    "        checkpoint = {\n",
    "            \"env\": self.env,\n",
    "            \"timesteps\": self.timesteps,\n",
    "            \"start_datetime\": self.start_datetime,\n",
    "            \"epsilon\": self.epsilon,\n",
    "            \"policy_network\": self.policy_network.state_dict(),\n",
    "            \"target_network\": self.target_network.state_dict(),\n",
    "            \"optimizer\": self.optimizer.state_dict(),\n",
    "        }\n",
    "\n",
    "        torch.save(checkpoint, save_path)\n",
    "        # Saving the replay buffer will takes time! But it is needed to properly resume training\n",
    "        save_to_pkl(replay_buffer_path, self.memory, verbose=1)\n",
    "\n",
    "        print(f\"Checkpoint saved into {save_parent_directory}\")\n",
    "\n",
    "    def load_last_checkpoint(self, path):\n",
    "        \"\"\"\n",
    "        Load the last saved checkpoint found in the given ``path``\n",
    "\n",
    "        :param path: the path to the directory containing the checkpoint(s)\n",
    "        \"\"\"\n",
    "        print(f\"Loading most recent checkpoint from {path}\")\n",
    "        self.is_loaded_from_checkpoint = True\n",
    "\n",
    "        # Using list comprehension to filter directories and only get the files\n",
    "        files = [file for file in os.listdir(path) if os.path.isfile(os.path.join(path, file))]\n",
    "\n",
    "        checkpoint_files = [chkpt_file for chkpt_file in files if \"chkpt\" in chkpt_file]\n",
    "        replay_buffer_files = [chkpt_file for chkpt_file in files if \"replay_buffer\" in chkpt_file]\n",
    "\n",
    "        checkpoint_file = get_latest_checkpoint_file(checkpoint_files)\n",
    "        replay_buffer_file = get_latest_checkpoint_file(replay_buffer_files)\n",
    "\n",
    "        checkpoint: Dict[str, any] = torch.load(path + \"/\" + checkpoint_file)\n",
    "\n",
    "        self.env = checkpoint[\"env\"]\n",
    "        self.timesteps = checkpoint[\"timesteps\"]\n",
    "        self.start_datetime: datetime = checkpoint[\"start_datetime\"]\n",
    "        self.start_time = self.start_datetime.timestamp()\n",
    "\n",
    "        self.epsilon = checkpoint[\"epsilon\"]\n",
    "\n",
    "        self.policy_network.load_state_dict(checkpoint[\"policy_network\"])\n",
    "        self.target_network.load_state_dict(checkpoint[\"target_network\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "        self.memory: ReplayBuffer = load_from_pkl(path + \"/\" + replay_buffer_file)\n",
    "        print(\"Checkpoint successfully loaded, you can resume the training now.\")\n",
    "\n",
    "    def run(self):\n",
    "        # Either create a new SummaryWriter or resume from previous one\n",
    "        if not self.is_loaded_from_checkpoint:\n",
    "            current_datetime = datetime.now()\n",
    "            self.start_datetime = current_datetime\n",
    "            self.start_time = current_datetime.timestamp()\n",
    "\n",
    "        start_datetime_str = self.start_datetime.strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
    "        self.writer = SummaryWriter(f\"{self.LOGS_PATH}/{self.name}_{start_datetime_str}\")\n",
    "        \n",
    "        video_folder_path = f\"{self.VIDEO_PATH}/{self.name}_{start_datetime_str}\"\n",
    "        if not os.path.exists(video_folder_path):\n",
    "            os.makedirs(video_folder_path)\n",
    "        self.env.video_folder = video_folder_path\n",
    "\n",
    "        while self.timesteps < self.MAX_TIMESTEPS:\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                self.timesteps += 1\n",
    "\n",
    "                action = self.act(state)\n",
    "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                self.remember(state, next_state, action, reward, terminated, info)\n",
    "\n",
    "                if self.timesteps >= self.LEARNING_STARTS and self.timesteps % self.TRAIN_FREQUENCY == 0:\n",
    "                    self.optimize_model()\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    mean_reward = np.mean(self.env.return_queue)\n",
    "                    length_reward = np.mean(self.env.length_queue)\n",
    "                    \n",
    "                    # Get episode statistics from info (\"episode\" key only exist when episode is done)\n",
    "                    episode_reward = info[\"episode\"][\"r\"]\n",
    "                    self.writer.add_scalar(\"rollout/episodic_return\", episode_reward, self.timesteps)\n",
    "                    self.writer.add_scalar(\"rollout/episodic_length\", info[\"episode\"][\"l\"], self.timesteps)\n",
    "\n",
    "                    self.writer.add_scalar(\"rollout/ep_len_mean\", length_reward, self.timesteps)\n",
    "                    self.writer.add_scalar(\"rollout/ep_rew_mean\", mean_reward, self.timesteps)\n",
    "\n",
    "                    self.writer.add_scalar(\"rollout/exploration_rate\", self.epsilon, self.timesteps)\n",
    "\n",
    "                    print(f\"Episode {self.env.episode_count} finished (timesteps: {self.timesteps}/{self.MAX_TIMESTEPS})\\n\"\n",
    "                          f\"Epsilon: {self.epsilon:.2f}, Episode reward: {episode_reward.item()}, Mean reward: {mean_reward:.2f}\")\n",
    "\n",
    "                    if self.env.episode_count % self.CHECKPOINT_INTERVAL_EPISODE == 0:\n",
    "                        self.save_checkpoint()\n",
    "                    print(\"***************************\")\n",
    "\n",
    "                if self.timesteps >= self.LEARNING_STARTS and self.timesteps % self.TARGET_UPDATE_INTERVAL == 0:\n",
    "                    self.update_target_network()\n",
    "                    #print(\"Target model updated.\")\n",
    "\n",
    "        self.save_checkpoint() # Save last checkpoint at the end of training\n",
    "\n",
    "        self.writer.flush()\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T11:59:42.748900Z",
     "start_time": "2023-04-09T11:59:42.720987Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/home/ubuntu/miniconda3/envs/pytorch/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:364: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished (timesteps: 822/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -21.00\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-0.mp4.\n",
      "Moviepy - Writing video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-0.mp4\n",
      "Episode 2 finished (timesteps: 1808/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.50\n",
      "***************************\n",
      "Episode 3 finished (timesteps: 2885/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.67\n",
      "***************************\n",
      "Episode 4 finished (timesteps: 3707/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.75\n",
      "***************************\n",
      "Episode 5 finished (timesteps: 4674/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.80\n",
      "***************************\n",
      "Episode 6 finished (timesteps: 5790/10000000)\n",
      "Epsilon: 1.00, Episode reward: -18.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 7 finished (timesteps: 6662/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.43\n",
      "***************************\n",
      "Episode 8 finished (timesteps: 7643/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.38\n",
      "***************************\n",
      "Episode 9 finished (timesteps: 8481/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 10 finished (timesteps: 9321/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.30\n",
      "***************************\n",
      "Episode 11 finished (timesteps: 10358/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.18\n",
      "***************************\n",
      "Episode 12 finished (timesteps: 11370/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 13 finished (timesteps: 12241/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 14 finished (timesteps: 13246/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.36\n",
      "***************************\n",
      "Episode 15 finished (timesteps: 14321/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 16 finished (timesteps: 15174/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 17 finished (timesteps: 15991/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.35\n",
      "***************************\n",
      "Episode 18 finished (timesteps: 17024/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.39\n",
      "***************************\n",
      "Episode 19 finished (timesteps: 17993/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.42\n",
      "***************************\n",
      "Episode 20 finished (timesteps: 18984/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.35\n",
      "***************************\n",
      "Episode 21 finished (timesteps: 20179/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.29\n",
      "***************************\n",
      "Episode 22 finished (timesteps: 20939/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 23 finished (timesteps: 21775/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.30\n",
      "***************************\n",
      "Episode 24 finished (timesteps: 23043/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 25 finished (timesteps: 23920/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 26 finished (timesteps: 24857/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 27 finished (timesteps: 25805/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 28 finished (timesteps: 26648/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.29\n",
      "***************************\n",
      "Episode 29 finished (timesteps: 27464/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 30 finished (timesteps: 28454/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.30\n",
      "***************************\n",
      "Episode 31 finished (timesteps: 29246/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 32 finished (timesteps: 30099/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.34\n",
      "***************************\n",
      "Episode 33 finished (timesteps: 30995/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 34 finished (timesteps: 31852/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 35 finished (timesteps: 32718/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 36 finished (timesteps: 33671/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 37 finished (timesteps: 34670/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.30\n",
      "***************************\n",
      "Episode 38 finished (timesteps: 35458/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 39 finished (timesteps: 36558/10000000)\n",
      "Epsilon: 1.00, Episode reward: -18.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 40 finished (timesteps: 37405/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 41 finished (timesteps: 38291/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.29\n",
      "***************************\n",
      "Episode 42 finished (timesteps: 39137/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 43 finished (timesteps: 40083/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.30\n",
      "***************************\n",
      "Episode 44 finished (timesteps: 40904/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 45 finished (timesteps: 42137/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.29\n",
      "***************************\n",
      "Episode 46 finished (timesteps: 42956/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.30\n",
      "***************************\n",
      "Episode 47 finished (timesteps: 43943/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.30\n",
      "***************************\n",
      "Episode 48 finished (timesteps: 44916/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.29\n",
      "***************************\n",
      "Episode 49 finished (timesteps: 45902/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.29\n",
      "***************************\n",
      "Episode 50 finished (timesteps: 46866/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 51 finished (timesteps: 47672/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.29\n",
      "***************************\n",
      "Episode 52 finished (timesteps: 48510/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.29\n",
      "***************************\n",
      "Episode 53 finished (timesteps: 49332/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.30\n",
      "***************************\n",
      "Episode 54 finished (timesteps: 50221/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 55 finished (timesteps: 51027/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 56 finished (timesteps: 51869/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 57 finished (timesteps: 52748/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 58 finished (timesteps: 53630/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.34\n",
      "***************************\n",
      "Episode 59 finished (timesteps: 54478/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.36\n",
      "***************************\n",
      "Episode 60 finished (timesteps: 55540/10000000)\n",
      "Epsilon: 1.00, Episode reward: -18.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 61 finished (timesteps: 56602/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.30\n",
      "***************************\n",
      "Episode 62 finished (timesteps: 57411/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.31\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 63 finished (timesteps: 58414/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 64 finished (timesteps: 59365/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 65 finished (timesteps: 60276/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 66 finished (timesteps: 61288/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.30\n",
      "***************************\n",
      "Episode 67 finished (timesteps: 62138/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 68 finished (timesteps: 62960/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 69 finished (timesteps: 63861/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 70 finished (timesteps: 64679/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.34\n",
      "***************************\n",
      "Episode 71 finished (timesteps: 65704/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 72 finished (timesteps: 66464/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 73 finished (timesteps: 67254/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.34\n",
      "***************************\n",
      "Episode 74 finished (timesteps: 68117/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.34\n",
      "***************************\n",
      "Episode 75 finished (timesteps: 69131/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 76 finished (timesteps: 69967/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 77 finished (timesteps: 70803/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 78 finished (timesteps: 71808/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 79 finished (timesteps: 72607/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 80 finished (timesteps: 73449/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 81 finished (timesteps: 74440/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 82 finished (timesteps: 75287/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 83 finished (timesteps: 76188/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 84 finished (timesteps: 77158/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.30\n",
      "***************************\n",
      "Episode 85 finished (timesteps: 78185/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 86 finished (timesteps: 79070/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 87 finished (timesteps: 79977/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 88 finished (timesteps: 80913/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 89 finished (timesteps: 81765/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 90 finished (timesteps: 82585/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 91 finished (timesteps: 83418/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.34\n",
      "***************************\n",
      "Episode 92 finished (timesteps: 84267/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.35\n",
      "***************************\n",
      "Episode 93 finished (timesteps: 85202/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.35\n",
      "***************************\n",
      "Episode 94 finished (timesteps: 86194/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.36\n",
      "***************************\n",
      "Episode 95 finished (timesteps: 87236/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.35\n",
      "***************************\n",
      "Episode 96 finished (timesteps: 88165/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.35\n",
      "***************************\n",
      "Episode 97 finished (timesteps: 89003/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.35\n",
      "***************************\n",
      "Episode 98 finished (timesteps: 89921/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.35\n",
      "***************************\n",
      "Episode 99 finished (timesteps: 90909/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.34\n",
      "***************************\n",
      "Episode 100 finished (timesteps: 91754/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.35\n",
      "***************************\n",
      "Episode 101 finished (timesteps: 92747/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.34\n",
      "***************************\n",
      "Episode 102 finished (timesteps: 93556/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.35\n",
      "***************************\n",
      "Episode 103 finished (timesteps: 94554/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 104 finished (timesteps: 95433/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 105 finished (timesteps: 96269/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 106 finished (timesteps: 97212/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.36\n",
      "***************************\n",
      "Episode 107 finished (timesteps: 98113/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.35\n",
      "***************************\n",
      "Episode 108 finished (timesteps: 99111/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.36\n",
      "***************************\n",
      "Episode 109 finished (timesteps: 100017/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.37\n",
      "***************************\n",
      "Episode 110 finished (timesteps: 100914/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.37\n",
      "***************************\n",
      "Episode 111 finished (timesteps: 101835/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.38\n",
      "***************************\n",
      "Episode 112 finished (timesteps: 102686/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.38\n",
      "***************************\n",
      "Episode 113 finished (timesteps: 103507/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.38\n",
      "***************************\n",
      "Episode 114 finished (timesteps: 104467/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.37\n",
      "***************************\n",
      "Episode 115 finished (timesteps: 105441/10000000)\n",
      "Epsilon: 0.99, Episode reward: -20.0, Mean reward: -20.37\n",
      "***************************\n",
      "Episode 116 finished (timesteps: 106361/10000000)\n",
      "Epsilon: 0.99, Episode reward: -20.0, Mean reward: -20.37\n",
      "***************************\n",
      "Episode 117 finished (timesteps: 107323/10000000)\n",
      "Epsilon: 0.99, Episode reward: -21.0, Mean reward: -20.37\n",
      "***************************\n",
      "Episode 118 finished (timesteps: 108189/10000000)\n",
      "Epsilon: 0.99, Episode reward: -21.0, Mean reward: -20.37\n",
      "***************************\n",
      "Episode 119 finished (timesteps: 109308/10000000)\n",
      "Epsilon: 0.99, Episode reward: -20.0, Mean reward: -20.36\n",
      "***************************\n",
      "Episode 120 finished (timesteps: 110134/10000000)\n",
      "Epsilon: 0.99, Episode reward: -21.0, Mean reward: -20.38\n",
      "***************************\n",
      "Episode 121 finished (timesteps: 110952/10000000)\n",
      "Epsilon: 0.99, Episode reward: -21.0, Mean reward: -20.40\n",
      "***************************\n",
      "Episode 122 finished (timesteps: 111798/10000000)\n",
      "Epsilon: 0.99, Episode reward: -21.0, Mean reward: -20.40\n",
      "***************************\n",
      "Episode 123 finished (timesteps: 112649/10000000)\n",
      "Epsilon: 0.99, Episode reward: -21.0, Mean reward: -20.41\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 124 finished (timesteps: 113731/10000000)\n",
      "Epsilon: 0.99, Episode reward: -18.0, Mean reward: -20.40\n",
      "***************************\n",
      "Episode 125 finished (timesteps: 114613/10000000)\n",
      "Epsilon: 0.99, Episode reward: -21.0, Mean reward: -20.40\n",
      "***************************\n",
      "Episode 126 finished (timesteps: 115495/10000000)\n",
      "Epsilon: 0.98, Episode reward: -21.0, Mean reward: -20.40\n",
      "***************************\n",
      "Episode 127 finished (timesteps: 116452/10000000)\n",
      "Epsilon: 0.98, Episode reward: -20.0, Mean reward: -20.41\n",
      "***************************\n",
      "Episode 128 finished (timesteps: 117602/10000000)\n",
      "Epsilon: 0.98, Episode reward: -20.0, Mean reward: -20.40\n",
      "***************************\n",
      "Episode 129 finished (timesteps: 118629/10000000)\n",
      "Epsilon: 0.98, Episode reward: -18.0, Mean reward: -20.37\n",
      "***************************\n",
      "Episode 130 finished (timesteps: 119466/10000000)\n",
      "Epsilon: 0.98, Episode reward: -21.0, Mean reward: -20.38\n",
      "***************************\n",
      "Episode 131 finished (timesteps: 120253/10000000)\n",
      "Epsilon: 0.98, Episode reward: -21.0, Mean reward: -20.38\n",
      "***************************\n",
      "Episode 132 finished (timesteps: 121168/10000000)\n",
      "Epsilon: 0.98, Episode reward: -21.0, Mean reward: -20.38\n",
      "***************************\n",
      "Episode 133 finished (timesteps: 122047/10000000)\n",
      "Epsilon: 0.98, Episode reward: -21.0, Mean reward: -20.39\n",
      "***************************\n",
      "Episode 134 finished (timesteps: 122981/10000000)\n",
      "Epsilon: 0.98, Episode reward: -20.0, Mean reward: -20.39\n",
      "***************************\n",
      "Episode 135 finished (timesteps: 123884/10000000)\n",
      "Epsilon: 0.98, Episode reward: -21.0, Mean reward: -20.40\n",
      "***************************\n",
      "Episode 136 finished (timesteps: 124914/10000000)\n",
      "Epsilon: 0.98, Episode reward: -19.0, Mean reward: -20.39\n",
      "***************************\n",
      "Episode 137 finished (timesteps: 125970/10000000)\n",
      "Epsilon: 0.97, Episode reward: -21.0, Mean reward: -20.40\n",
      "***************************\n",
      "Episode 138 finished (timesteps: 126984/10000000)\n",
      "Epsilon: 0.97, Episode reward: -19.0, Mean reward: -20.38\n",
      "***************************\n",
      "Episode 139 finished (timesteps: 127962/10000000)\n",
      "Epsilon: 0.97, Episode reward: -20.0, Mean reward: -20.40\n",
      "***************************\n",
      "Episode 140 finished (timesteps: 128892/10000000)\n",
      "Epsilon: 0.97, Episode reward: -19.0, Mean reward: -20.38\n",
      "***************************\n",
      "Episode 141 finished (timesteps: 129711/10000000)\n",
      "Epsilon: 0.97, Episode reward: -21.0, Mean reward: -20.38\n",
      "***************************\n",
      "Episode 142 finished (timesteps: 130623/10000000)\n",
      "Epsilon: 0.97, Episode reward: -21.0, Mean reward: -20.38\n",
      "***************************\n",
      "Episode 143 finished (timesteps: 131770/10000000)\n",
      "Epsilon: 0.97, Episode reward: -19.0, Mean reward: -20.37\n",
      "***************************\n",
      "Episode 144 finished (timesteps: 132809/10000000)\n",
      "Epsilon: 0.97, Episode reward: -19.0, Mean reward: -20.35\n",
      "***************************\n",
      "Episode 145 finished (timesteps: 133841/10000000)\n",
      "Epsilon: 0.97, Episode reward: -19.0, Mean reward: -20.35\n",
      "***************************\n",
      "Episode 146 finished (timesteps: 134687/10000000)\n",
      "Epsilon: 0.97, Episode reward: -21.0, Mean reward: -20.35\n",
      "***************************\n",
      "Episode 147 finished (timesteps: 135616/10000000)\n",
      "Epsilon: 0.96, Episode reward: -20.0, Mean reward: -20.35\n",
      "***************************\n",
      "Episode 148 finished (timesteps: 136522/10000000)\n",
      "Epsilon: 0.96, Episode reward: -21.0, Mean reward: -20.36\n",
      "***************************\n",
      "Episode 149 finished (timesteps: 137447/10000000)\n",
      "Epsilon: 0.96, Episode reward: -20.0, Mean reward: -20.36\n",
      "***************************\n",
      "Episode 150 finished (timesteps: 138613/10000000)\n",
      "Epsilon: 0.96, Episode reward: -20.0, Mean reward: -20.36\n",
      "***************************\n",
      "Episode 151 finished (timesteps: 139599/10000000)\n",
      "Epsilon: 0.96, Episode reward: -20.0, Mean reward: -20.35\n",
      "***************************\n",
      "Episode 152 finished (timesteps: 140585/10000000)\n",
      "Epsilon: 0.96, Episode reward: -20.0, Mean reward: -20.35\n",
      "***************************\n",
      "Episode 153 finished (timesteps: 141432/10000000)\n",
      "Epsilon: 0.96, Episode reward: -21.0, Mean reward: -20.35\n",
      "***************************\n",
      "Episode 154 finished (timesteps: 142374/10000000)\n",
      "Epsilon: 0.96, Episode reward: -20.0, Mean reward: -20.34\n",
      "***************************\n",
      "Episode 155 finished (timesteps: 143411/10000000)\n",
      "Epsilon: 0.96, Episode reward: -21.0, Mean reward: -20.34\n",
      "***************************\n",
      "Episode 156 finished (timesteps: 144263/10000000)\n",
      "Epsilon: 0.96, Episode reward: -21.0, Mean reward: -20.35\n",
      "***************************\n",
      "Episode 157 finished (timesteps: 145022/10000000)\n",
      "Epsilon: 0.96, Episode reward: -21.0, Mean reward: -20.35\n",
      "***************************\n",
      "Episode 158 finished (timesteps: 145966/10000000)\n",
      "Epsilon: 0.95, Episode reward: -21.0, Mean reward: -20.35\n",
      "***************************\n",
      "Episode 159 finished (timesteps: 146785/10000000)\n",
      "Epsilon: 0.95, Episode reward: -21.0, Mean reward: -20.35\n",
      "***************************\n",
      "Episode 160 finished (timesteps: 147925/10000000)\n",
      "Epsilon: 0.95, Episode reward: -20.0, Mean reward: -20.37\n",
      "***************************\n",
      "Episode 161 finished (timesteps: 148856/10000000)\n",
      "Epsilon: 0.95, Episode reward: -20.0, Mean reward: -20.38\n",
      "***************************\n",
      "Episode 162 finished (timesteps: 149745/10000000)\n",
      "Epsilon: 0.95, Episode reward: -21.0, Mean reward: -20.38\n",
      "***************************\n",
      "Episode 163 finished (timesteps: 150786/10000000)\n",
      "Epsilon: 0.95, Episode reward: -20.0, Mean reward: -20.37\n",
      "***************************\n",
      "Episode 164 finished (timesteps: 151701/10000000)\n",
      "Epsilon: 0.95, Episode reward: -21.0, Mean reward: -20.38\n",
      "***************************\n",
      "Episode 165 finished (timesteps: 152795/10000000)\n",
      "Epsilon: 0.95, Episode reward: -19.0, Mean reward: -20.37\n",
      "***************************\n",
      "Episode 166 finished (timesteps: 153714/10000000)\n",
      "Epsilon: 0.95, Episode reward: -20.0, Mean reward: -20.37\n",
      "***************************\n",
      "Episode 167 finished (timesteps: 154628/10000000)\n",
      "Epsilon: 0.95, Episode reward: -20.0, Mean reward: -20.36\n",
      "***************************\n",
      "Episode 168 finished (timesteps: 155415/10000000)\n",
      "Epsilon: 0.95, Episode reward: -21.0, Mean reward: -20.36\n",
      "***************************\n",
      "Episode 169 finished (timesteps: 156314/10000000)\n",
      "Epsilon: 0.94, Episode reward: -21.0, Mean reward: -20.36\n",
      "***************************\n",
      "Episode 170 finished (timesteps: 157486/10000000)\n",
      "Epsilon: 0.94, Episode reward: -19.0, Mean reward: -20.34\n",
      "***************************\n",
      "Episode 171 finished (timesteps: 158764/10000000)\n",
      "Epsilon: 0.94, Episode reward: -19.0, Mean reward: -20.34\n",
      "***************************\n",
      "Episode 172 finished (timesteps: 159684/10000000)\n",
      "Epsilon: 0.94, Episode reward: -20.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 173 finished (timesteps: 160629/10000000)\n",
      "Epsilon: 0.94, Episode reward: -19.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 174 finished (timesteps: 161448/10000000)\n",
      "Epsilon: 0.94, Episode reward: -21.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 175 finished (timesteps: 162303/10000000)\n",
      "Epsilon: 0.94, Episode reward: -20.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 176 finished (timesteps: 163187/10000000)\n",
      "Epsilon: 0.94, Episode reward: -21.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 177 finished (timesteps: 164197/10000000)\n",
      "Epsilon: 0.94, Episode reward: -20.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 178 finished (timesteps: 165074/10000000)\n",
      "Epsilon: 0.94, Episode reward: -21.0, Mean reward: -20.34\n",
      "***************************\n",
      "Episode 179 finished (timesteps: 166125/10000000)\n",
      "Epsilon: 0.93, Episode reward: -20.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 180 finished (timesteps: 167023/10000000)\n",
      "Epsilon: 0.93, Episode reward: -20.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 181 finished (timesteps: 167862/10000000)\n",
      "Epsilon: 0.93, Episode reward: -20.0, Mean reward: -20.34\n",
      "***************************\n",
      "Episode 182 finished (timesteps: 168670/10000000)\n",
      "Epsilon: 0.93, Episode reward: -21.0, Mean reward: -20.34\n",
      "***************************\n",
      "Episode 183 finished (timesteps: 169672/10000000)\n",
      "Epsilon: 0.93, Episode reward: -21.0, Mean reward: -20.35\n",
      "***************************\n",
      "Episode 184 finished (timesteps: 170608/10000000)\n",
      "Epsilon: 0.93, Episode reward: -21.0, Mean reward: -20.37\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 185 finished (timesteps: 171534/10000000)\n",
      "Epsilon: 0.93, Episode reward: -21.0, Mean reward: -20.37\n",
      "***************************\n",
      "Episode 186 finished (timesteps: 172591/10000000)\n",
      "Epsilon: 0.93, Episode reward: -18.0, Mean reward: -20.34\n",
      "***************************\n",
      "Episode 187 finished (timesteps: 173616/10000000)\n",
      "Epsilon: 0.93, Episode reward: -20.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 188 finished (timesteps: 174434/10000000)\n",
      "Epsilon: 0.93, Episode reward: -21.0, Mean reward: -20.34\n",
      "***************************\n",
      "Episode 189 finished (timesteps: 175504/10000000)\n",
      "Epsilon: 0.93, Episode reward: -19.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 190 finished (timesteps: 176328/10000000)\n",
      "Epsilon: 0.92, Episode reward: -21.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 191 finished (timesteps: 177226/10000000)\n",
      "Epsilon: 0.92, Episode reward: -21.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 192 finished (timesteps: 178148/10000000)\n",
      "Epsilon: 0.92, Episode reward: -21.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 193 finished (timesteps: 178999/10000000)\n",
      "Epsilon: 0.92, Episode reward: -21.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 194 finished (timesteps: 180049/10000000)\n",
      "Epsilon: 0.92, Episode reward: -20.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 195 finished (timesteps: 181004/10000000)\n",
      "Epsilon: 0.92, Episode reward: -19.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 196 finished (timesteps: 181844/10000000)\n",
      "Epsilon: 0.92, Episode reward: -21.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 197 finished (timesteps: 182832/10000000)\n",
      "Epsilon: 0.92, Episode reward: -19.0, Mean reward: -20.30\n",
      "***************************\n",
      "Episode 198 finished (timesteps: 183720/10000000)\n",
      "Epsilon: 0.92, Episode reward: -21.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 199 finished (timesteps: 184726/10000000)\n",
      "Epsilon: 0.92, Episode reward: -21.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 200 finished (timesteps: 185631/10000000)\n",
      "Epsilon: 0.92, Episode reward: -21.0, Mean reward: -20.32\n",
      "***************************\n",
      "Episode 201 finished (timesteps: 186562/10000000)\n",
      "Epsilon: 0.91, Episode reward: -19.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 202 finished (timesteps: 187500/10000000)\n",
      "Epsilon: 0.91, Episode reward: -20.0, Mean reward: -20.30\n",
      "***************************\n",
      "Episode 203 finished (timesteps: 188413/10000000)\n",
      "Epsilon: 0.91, Episode reward: -20.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 204 finished (timesteps: 189316/10000000)\n",
      "Epsilon: 0.91, Episode reward: -21.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 205 finished (timesteps: 190199/10000000)\n",
      "Epsilon: 0.91, Episode reward: -21.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 206 finished (timesteps: 191067/10000000)\n",
      "Epsilon: 0.91, Episode reward: -20.0, Mean reward: -20.30\n",
      "***************************\n",
      "Episode 207 finished (timesteps: 192115/10000000)\n",
      "Epsilon: 0.91, Episode reward: -19.0, Mean reward: -20.29\n",
      "***************************\n",
      "Episode 208 finished (timesteps: 193132/10000000)\n",
      "Epsilon: 0.91, Episode reward: -20.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 209 finished (timesteps: 194112/10000000)\n",
      "Epsilon: 0.91, Episode reward: -20.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 210 finished (timesteps: 195025/10000000)\n",
      "Epsilon: 0.91, Episode reward: -20.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 211 finished (timesteps: 195843/10000000)\n",
      "Epsilon: 0.91, Episode reward: -21.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 212 finished (timesteps: 196796/10000000)\n",
      "Epsilon: 0.90, Episode reward: -21.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 213 finished (timesteps: 197661/10000000)\n",
      "Epsilon: 0.90, Episode reward: -20.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 214 finished (timesteps: 198566/10000000)\n",
      "Epsilon: 0.90, Episode reward: -21.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 215 finished (timesteps: 199569/10000000)\n",
      "Epsilon: 0.90, Episode reward: -21.0, Mean reward: -20.29\n",
      "***************************\n",
      "Episode 216 finished (timesteps: 200575/10000000)\n",
      "Epsilon: 0.90, Episode reward: -21.0, Mean reward: -20.30\n",
      "***************************\n",
      "Episode 217 finished (timesteps: 201585/10000000)\n",
      "Epsilon: 0.90, Episode reward: -20.0, Mean reward: -20.29\n",
      "***************************\n",
      "Episode 218 finished (timesteps: 202547/10000000)\n",
      "Epsilon: 0.90, Episode reward: -20.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 219 finished (timesteps: 203487/10000000)\n",
      "Epsilon: 0.90, Episode reward: -20.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 220 finished (timesteps: 204538/10000000)\n",
      "Epsilon: 0.90, Episode reward: -21.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 221 finished (timesteps: 205459/10000000)\n",
      "Epsilon: 0.90, Episode reward: -20.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 222 finished (timesteps: 206438/10000000)\n",
      "Epsilon: 0.89, Episode reward: -19.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 223 finished (timesteps: 207491/10000000)\n",
      "Epsilon: 0.89, Episode reward: -20.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 224 finished (timesteps: 208386/10000000)\n",
      "Epsilon: 0.89, Episode reward: -20.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 225 finished (timesteps: 209263/10000000)\n",
      "Epsilon: 0.89, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 226 finished (timesteps: 210158/10000000)\n",
      "Epsilon: 0.89, Episode reward: -20.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 227 finished (timesteps: 211199/10000000)\n",
      "Epsilon: 0.89, Episode reward: -19.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 228 finished (timesteps: 212186/10000000)\n",
      "Epsilon: 0.89, Episode reward: -20.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 229 finished (timesteps: 213056/10000000)\n",
      "Epsilon: 0.89, Episode reward: -21.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 230 finished (timesteps: 214076/10000000)\n",
      "Epsilon: 0.89, Episode reward: -20.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 231 finished (timesteps: 215114/10000000)\n",
      "Epsilon: 0.89, Episode reward: -19.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 232 finished (timesteps: 215934/10000000)\n",
      "Epsilon: 0.89, Episode reward: -21.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 233 finished (timesteps: 216935/10000000)\n",
      "Epsilon: 0.88, Episode reward: -20.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 234 finished (timesteps: 217771/10000000)\n",
      "Epsilon: 0.88, Episode reward: -20.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 235 finished (timesteps: 218665/10000000)\n",
      "Epsilon: 0.88, Episode reward: -20.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 236 finished (timesteps: 219811/10000000)\n",
      "Epsilon: 0.88, Episode reward: -18.0, Mean reward: -20.21\n",
      "***************************\n",
      "Episode 237 finished (timesteps: 220590/10000000)\n",
      "Epsilon: 0.88, Episode reward: -21.0, Mean reward: -20.21\n",
      "***************************\n",
      "Episode 238 finished (timesteps: 221353/10000000)\n",
      "Epsilon: 0.88, Episode reward: -21.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 239 finished (timesteps: 222499/10000000)\n",
      "Epsilon: 0.88, Episode reward: -21.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 240 finished (timesteps: 223381/10000000)\n",
      "Epsilon: 0.88, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 241 finished (timesteps: 224264/10000000)\n",
      "Epsilon: 0.88, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 242 finished (timesteps: 225119/10000000)\n",
      "Epsilon: 0.88, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 243 finished (timesteps: 225880/10000000)\n",
      "Epsilon: 0.88, Episode reward: -21.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 244 finished (timesteps: 226672/10000000)\n",
      "Epsilon: 0.87, Episode reward: -21.0, Mean reward: -20.30\n",
      "***************************\n",
      "Episode 245 finished (timesteps: 227508/10000000)\n",
      "Epsilon: 0.87, Episode reward: -21.0, Mean reward: -20.32\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 246 finished (timesteps: 228666/10000000)\n",
      "Epsilon: 0.87, Episode reward: -18.0, Mean reward: -20.29\n",
      "***************************\n",
      "Episode 247 finished (timesteps: 229628/10000000)\n",
      "Epsilon: 0.87, Episode reward: -19.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 248 finished (timesteps: 230752/10000000)\n",
      "Epsilon: 0.87, Episode reward: -20.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 249 finished (timesteps: 231514/10000000)\n",
      "Epsilon: 0.87, Episode reward: -21.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 250 finished (timesteps: 232687/10000000)\n",
      "Epsilon: 0.87, Episode reward: -20.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 251 finished (timesteps: 233842/10000000)\n",
      "Epsilon: 0.87, Episode reward: -18.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 252 finished (timesteps: 234867/10000000)\n",
      "Epsilon: 0.87, Episode reward: -21.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 253 finished (timesteps: 235732/10000000)\n",
      "Epsilon: 0.87, Episode reward: -20.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 254 finished (timesteps: 236787/10000000)\n",
      "Epsilon: 0.86, Episode reward: -19.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 255 finished (timesteps: 237611/10000000)\n",
      "Epsilon: 0.86, Episode reward: -21.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 256 finished (timesteps: 238451/10000000)\n",
      "Epsilon: 0.86, Episode reward: -21.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 257 finished (timesteps: 239275/10000000)\n",
      "Epsilon: 0.86, Episode reward: -21.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 258 finished (timesteps: 240083/10000000)\n",
      "Epsilon: 0.86, Episode reward: -21.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 259 finished (timesteps: 241077/10000000)\n",
      "Epsilon: 0.86, Episode reward: -21.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 260 finished (timesteps: 242056/10000000)\n",
      "Epsilon: 0.86, Episode reward: -20.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 261 finished (timesteps: 243148/10000000)\n",
      "Epsilon: 0.86, Episode reward: -19.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 262 finished (timesteps: 244196/10000000)\n",
      "Epsilon: 0.86, Episode reward: -20.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 263 finished (timesteps: 245045/10000000)\n",
      "Epsilon: 0.86, Episode reward: -21.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 264 finished (timesteps: 245997/10000000)\n",
      "Epsilon: 0.86, Episode reward: -20.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 265 finished (timesteps: 246937/10000000)\n",
      "Epsilon: 0.85, Episode reward: -21.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 266 finished (timesteps: 247849/10000000)\n",
      "Epsilon: 0.85, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 267 finished (timesteps: 248890/10000000)\n",
      "Epsilon: 0.85, Episode reward: -20.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 268 finished (timesteps: 249776/10000000)\n",
      "Epsilon: 0.85, Episode reward: -20.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 269 finished (timesteps: 250719/10000000)\n",
      "Epsilon: 0.85, Episode reward: -20.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 270 finished (timesteps: 251478/10000000)\n",
      "Epsilon: 0.85, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 271 finished (timesteps: 252593/10000000)\n",
      "Epsilon: 0.85, Episode reward: -19.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 272 finished (timesteps: 253677/10000000)\n",
      "Epsilon: 0.85, Episode reward: -18.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 273 finished (timesteps: 254635/10000000)\n",
      "Epsilon: 0.85, Episode reward: -20.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 274 finished (timesteps: 255689/10000000)\n",
      "Epsilon: 0.85, Episode reward: -20.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 275 finished (timesteps: 256809/10000000)\n",
      "Epsilon: 0.84, Episode reward: -19.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 276 finished (timesteps: 257716/10000000)\n",
      "Epsilon: 0.84, Episode reward: -21.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 277 finished (timesteps: 258642/10000000)\n",
      "Epsilon: 0.84, Episode reward: -20.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 278 finished (timesteps: 259573/10000000)\n",
      "Epsilon: 0.84, Episode reward: -21.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 279 finished (timesteps: 260435/10000000)\n",
      "Epsilon: 0.84, Episode reward: -20.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 280 finished (timesteps: 261345/10000000)\n",
      "Epsilon: 0.84, Episode reward: -21.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 281 finished (timesteps: 262183/10000000)\n",
      "Epsilon: 0.84, Episode reward: -21.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 282 finished (timesteps: 263023/10000000)\n",
      "Epsilon: 0.84, Episode reward: -20.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 283 finished (timesteps: 264027/10000000)\n",
      "Epsilon: 0.84, Episode reward: -20.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 284 finished (timesteps: 264897/10000000)\n",
      "Epsilon: 0.84, Episode reward: -20.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 285 finished (timesteps: 265834/10000000)\n",
      "Epsilon: 0.84, Episode reward: -21.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 286 finished (timesteps: 266761/10000000)\n",
      "Epsilon: 0.83, Episode reward: -20.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 287 finished (timesteps: 267583/10000000)\n",
      "Epsilon: 0.83, Episode reward: -21.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 288 finished (timesteps: 268544/10000000)\n",
      "Epsilon: 0.83, Episode reward: -20.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 289 finished (timesteps: 269450/10000000)\n",
      "Epsilon: 0.83, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 290 finished (timesteps: 270333/10000000)\n",
      "Epsilon: 0.83, Episode reward: -20.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 291 finished (timesteps: 271249/10000000)\n",
      "Epsilon: 0.83, Episode reward: -20.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 292 finished (timesteps: 272189/10000000)\n",
      "Epsilon: 0.83, Episode reward: -21.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 293 finished (timesteps: 273196/10000000)\n",
      "Epsilon: 0.83, Episode reward: -20.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 294 finished (timesteps: 274255/10000000)\n",
      "Epsilon: 0.83, Episode reward: -20.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 295 finished (timesteps: 275072/10000000)\n",
      "Epsilon: 0.83, Episode reward: -21.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 296 finished (timesteps: 276030/10000000)\n",
      "Epsilon: 0.83, Episode reward: -20.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 297 finished (timesteps: 276972/10000000)\n",
      "Epsilon: 0.82, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 298 finished (timesteps: 277924/10000000)\n",
      "Epsilon: 0.82, Episode reward: -20.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 299 finished (timesteps: 278834/10000000)\n",
      "Epsilon: 0.82, Episode reward: -21.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 300 finished (timesteps: 279861/10000000)\n",
      "Epsilon: 0.82, Episode reward: -21.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 301 finished (timesteps: 280955/10000000)\n",
      "Epsilon: 0.82, Episode reward: -19.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 302 finished (timesteps: 281935/10000000)\n",
      "Epsilon: 0.82, Episode reward: -20.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 303 finished (timesteps: 283222/10000000)\n",
      "Epsilon: 0.82, Episode reward: -19.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 304 finished (timesteps: 284100/10000000)\n",
      "Epsilon: 0.82, Episode reward: -21.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 305 finished (timesteps: 284948/10000000)\n",
      "Epsilon: 0.82, Episode reward: -21.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 306 finished (timesteps: 285782/10000000)\n",
      "Epsilon: 0.82, Episode reward: -20.0, Mean reward: -20.24\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 307 finished (timesteps: 286767/10000000)\n",
      "Epsilon: 0.82, Episode reward: -20.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 308 finished (timesteps: 287634/10000000)\n",
      "Epsilon: 0.81, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 309 finished (timesteps: 288671/10000000)\n",
      "Epsilon: 0.81, Episode reward: -19.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 310 finished (timesteps: 289548/10000000)\n",
      "Epsilon: 0.81, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 311 finished (timesteps: 290384/10000000)\n",
      "Epsilon: 0.81, Episode reward: -20.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 312 finished (timesteps: 291420/10000000)\n",
      "Epsilon: 0.81, Episode reward: -19.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 313 finished (timesteps: 292402/10000000)\n",
      "Epsilon: 0.81, Episode reward: -20.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 314 finished (timesteps: 293540/10000000)\n",
      "Epsilon: 0.81, Episode reward: -19.0, Mean reward: -20.21\n",
      "***************************\n",
      "Episode 315 finished (timesteps: 294328/10000000)\n",
      "Epsilon: 0.81, Episode reward: -21.0, Mean reward: -20.21\n",
      "***************************\n",
      "Episode 316 finished (timesteps: 295226/10000000)\n",
      "Epsilon: 0.81, Episode reward: -20.0, Mean reward: -20.20\n",
      "***************************\n",
      "Episode 317 finished (timesteps: 296017/10000000)\n",
      "Epsilon: 0.81, Episode reward: -21.0, Mean reward: -20.21\n",
      "***************************\n",
      "Episode 318 finished (timesteps: 296866/10000000)\n",
      "Epsilon: 0.81, Episode reward: -21.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 319 finished (timesteps: 297649/10000000)\n",
      "Epsilon: 0.80, Episode reward: -21.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 320 finished (timesteps: 298572/10000000)\n",
      "Epsilon: 0.80, Episode reward: -20.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 321 finished (timesteps: 299496/10000000)\n",
      "Epsilon: 0.80, Episode reward: -20.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 322 finished (timesteps: 300572/10000000)\n",
      "Epsilon: 0.80, Episode reward: -19.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 323 finished (timesteps: 301475/10000000)\n",
      "Epsilon: 0.80, Episode reward: -20.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 324 finished (timesteps: 302492/10000000)\n",
      "Epsilon: 0.80, Episode reward: -20.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 325 finished (timesteps: 303482/10000000)\n",
      "Epsilon: 0.80, Episode reward: -20.0, Mean reward: -20.21\n",
      "***************************\n",
      "Episode 326 finished (timesteps: 304469/10000000)\n",
      "Epsilon: 0.80, Episode reward: -20.0, Mean reward: -20.21\n",
      "***************************\n",
      "Episode 327 finished (timesteps: 305485/10000000)\n",
      "Epsilon: 0.80, Episode reward: -21.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 328 finished (timesteps: 306382/10000000)\n",
      "Epsilon: 0.80, Episode reward: -20.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 329 finished (timesteps: 307219/10000000)\n",
      "Epsilon: 0.79, Episode reward: -21.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 330 finished (timesteps: 308115/10000000)\n",
      "Epsilon: 0.79, Episode reward: -21.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 331 finished (timesteps: 309045/10000000)\n",
      "Epsilon: 0.79, Episode reward: -20.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 332 finished (timesteps: 309920/10000000)\n",
      "Epsilon: 0.79, Episode reward: -21.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 333 finished (timesteps: 310770/10000000)\n",
      "Epsilon: 0.79, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 334 finished (timesteps: 311720/10000000)\n",
      "Epsilon: 0.79, Episode reward: -20.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 335 finished (timesteps: 312719/10000000)\n",
      "Epsilon: 0.79, Episode reward: -20.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 336 finished (timesteps: 313835/10000000)\n",
      "Epsilon: 0.79, Episode reward: -18.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 337 finished (timesteps: 314704/10000000)\n",
      "Epsilon: 0.79, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 338 finished (timesteps: 315628/10000000)\n",
      "Epsilon: 0.79, Episode reward: -20.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 339 finished (timesteps: 316636/10000000)\n",
      "Epsilon: 0.79, Episode reward: -19.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 340 finished (timesteps: 317483/10000000)\n",
      "Epsilon: 0.78, Episode reward: -21.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 341 finished (timesteps: 318482/10000000)\n",
      "Epsilon: 0.78, Episode reward: -21.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 342 finished (timesteps: 319273/10000000)\n",
      "Epsilon: 0.78, Episode reward: -21.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 343 finished (timesteps: 320238/10000000)\n",
      "Epsilon: 0.78, Episode reward: -20.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 344 finished (timesteps: 321117/10000000)\n",
      "Epsilon: 0.78, Episode reward: -21.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 345 finished (timesteps: 322095/10000000)\n",
      "Epsilon: 0.78, Episode reward: -19.0, Mean reward: -20.20\n",
      "***************************\n",
      "Episode 346 finished (timesteps: 322859/10000000)\n",
      "Epsilon: 0.78, Episode reward: -21.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 347 finished (timesteps: 323691/10000000)\n",
      "Epsilon: 0.78, Episode reward: -21.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 348 finished (timesteps: 324574/10000000)\n",
      "Epsilon: 0.78, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 349 finished (timesteps: 325598/10000000)\n",
      "Epsilon: 0.78, Episode reward: -18.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 350 finished (timesteps: 326518/10000000)\n",
      "Epsilon: 0.78, Episode reward: -21.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 351 finished (timesteps: 327368/10000000)\n",
      "Epsilon: 0.77, Episode reward: -21.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 352 finished (timesteps: 328312/10000000)\n",
      "Epsilon: 0.77, Episode reward: -20.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 353 finished (timesteps: 329162/10000000)\n",
      "Epsilon: 0.77, Episode reward: -21.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 354 finished (timesteps: 330245/10000000)\n",
      "Epsilon: 0.77, Episode reward: -20.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 355 finished (timesteps: 331062/10000000)\n",
      "Epsilon: 0.77, Episode reward: -21.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 356 finished (timesteps: 331867/10000000)\n",
      "Epsilon: 0.77, Episode reward: -21.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 357 finished (timesteps: 333011/10000000)\n",
      "Epsilon: 0.77, Episode reward: -19.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 358 finished (timesteps: 334006/10000000)\n",
      "Epsilon: 0.77, Episode reward: -19.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 359 finished (timesteps: 334770/10000000)\n",
      "Epsilon: 0.77, Episode reward: -21.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 360 finished (timesteps: 335812/10000000)\n",
      "Epsilon: 0.77, Episode reward: -19.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 361 finished (timesteps: 336653/10000000)\n",
      "Epsilon: 0.77, Episode reward: -21.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 362 finished (timesteps: 337415/10000000)\n",
      "Epsilon: 0.76, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 363 finished (timesteps: 338237/10000000)\n",
      "Epsilon: 0.76, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 364 finished (timesteps: 339233/10000000)\n",
      "Epsilon: 0.76, Episode reward: -21.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 365 finished (timesteps: 340130/10000000)\n",
      "Epsilon: 0.76, Episode reward: -21.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 366 finished (timesteps: 341088/10000000)\n",
      "Epsilon: 0.76, Episode reward: -20.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 367 finished (timesteps: 342033/10000000)\n",
      "Epsilon: 0.76, Episode reward: -21.0, Mean reward: -20.27\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 368 finished (timesteps: 342823/10000000)\n",
      "Epsilon: 0.76, Episode reward: -21.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 369 finished (timesteps: 343749/10000000)\n",
      "Epsilon: 0.76, Episode reward: -21.0, Mean reward: -20.29\n",
      "***************************\n",
      "Episode 370 finished (timesteps: 344647/10000000)\n",
      "Epsilon: 0.76, Episode reward: -20.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 371 finished (timesteps: 345647/10000000)\n",
      "Epsilon: 0.76, Episode reward: -19.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 372 finished (timesteps: 346773/10000000)\n",
      "Epsilon: 0.76, Episode reward: -19.0, Mean reward: -20.29\n",
      "***************************\n",
      "Episode 373 finished (timesteps: 347608/10000000)\n",
      "Epsilon: 0.75, Episode reward: -20.0, Mean reward: -20.29\n",
      "***************************\n",
      "Episode 374 finished (timesteps: 348486/10000000)\n",
      "Epsilon: 0.75, Episode reward: -21.0, Mean reward: -20.30\n",
      "***************************\n",
      "Episode 375 finished (timesteps: 349381/10000000)\n",
      "Epsilon: 0.75, Episode reward: -20.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 376 finished (timesteps: 350160/10000000)\n",
      "Epsilon: 0.75, Episode reward: -21.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 377 finished (timesteps: 351283/10000000)\n",
      "Epsilon: 0.75, Episode reward: -19.0, Mean reward: -20.30\n",
      "***************************\n",
      "Episode 378 finished (timesteps: 352238/10000000)\n",
      "Epsilon: 0.75, Episode reward: -19.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 379 finished (timesteps: 353084/10000000)\n",
      "Epsilon: 0.75, Episode reward: -21.0, Mean reward: -20.29\n",
      "***************************\n",
      "Episode 380 finished (timesteps: 354027/10000000)\n",
      "Epsilon: 0.75, Episode reward: -21.0, Mean reward: -20.29\n",
      "***************************\n",
      "Episode 381 finished (timesteps: 354876/10000000)\n",
      "Epsilon: 0.75, Episode reward: -21.0, Mean reward: -20.29\n",
      "***************************\n",
      "Episode 382 finished (timesteps: 355742/10000000)\n",
      "Epsilon: 0.75, Episode reward: -21.0, Mean reward: -20.30\n",
      "***************************\n",
      "Episode 383 finished (timesteps: 356791/10000000)\n",
      "Epsilon: 0.75, Episode reward: -19.0, Mean reward: -20.29\n",
      "***************************\n",
      "Episode 384 finished (timesteps: 357671/10000000)\n",
      "Epsilon: 0.74, Episode reward: -21.0, Mean reward: -20.30\n",
      "***************************\n",
      "Episode 385 finished (timesteps: 358573/10000000)\n",
      "Epsilon: 0.74, Episode reward: -21.0, Mean reward: -20.30\n",
      "***************************\n",
      "Episode 386 finished (timesteps: 359471/10000000)\n",
      "Epsilon: 0.74, Episode reward: -21.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 387 finished (timesteps: 360412/10000000)\n",
      "Epsilon: 0.74, Episode reward: -20.0, Mean reward: -20.30\n",
      "***************************\n",
      "Episode 388 finished (timesteps: 361607/10000000)\n",
      "Epsilon: 0.74, Episode reward: -17.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 389 finished (timesteps: 362619/10000000)\n",
      "Epsilon: 0.74, Episode reward: -20.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 390 finished (timesteps: 363558/10000000)\n",
      "Epsilon: 0.74, Episode reward: -21.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 391 finished (timesteps: 364380/10000000)\n",
      "Epsilon: 0.74, Episode reward: -21.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 392 finished (timesteps: 365416/10000000)\n",
      "Epsilon: 0.74, Episode reward: -20.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 393 finished (timesteps: 366354/10000000)\n",
      "Epsilon: 0.74, Episode reward: -20.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 394 finished (timesteps: 367511/10000000)\n",
      "Epsilon: 0.74, Episode reward: -20.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 395 finished (timesteps: 368552/10000000)\n",
      "Epsilon: 0.73, Episode reward: -21.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 396 finished (timesteps: 369453/10000000)\n",
      "Epsilon: 0.73, Episode reward: -21.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 397 finished (timesteps: 370290/10000000)\n",
      "Epsilon: 0.73, Episode reward: -20.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 398 finished (timesteps: 371313/10000000)\n",
      "Epsilon: 0.73, Episode reward: -21.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 399 finished (timesteps: 372269/10000000)\n",
      "Epsilon: 0.73, Episode reward: -20.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 400 finished (timesteps: 373133/10000000)\n",
      "Epsilon: 0.73, Episode reward: -20.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 401 finished (timesteps: 374023/10000000)\n",
      "Epsilon: 0.73, Episode reward: -20.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 402 finished (timesteps: 374859/10000000)\n",
      "Epsilon: 0.73, Episode reward: -20.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 403 finished (timesteps: 375868/10000000)\n",
      "Epsilon: 0.73, Episode reward: -20.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 404 finished (timesteps: 376781/10000000)\n",
      "Epsilon: 0.73, Episode reward: -21.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 405 finished (timesteps: 377664/10000000)\n",
      "Epsilon: 0.73, Episode reward: -21.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 406 finished (timesteps: 378639/10000000)\n",
      "Epsilon: 0.72, Episode reward: -20.0, Mean reward: -20.28\n",
      "***************************\n",
      "Episode 407 finished (timesteps: 379958/10000000)\n",
      "Epsilon: 0.72, Episode reward: -17.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 408 finished (timesteps: 380986/10000000)\n",
      "Epsilon: 0.72, Episode reward: -21.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 409 finished (timesteps: 382031/10000000)\n",
      "Epsilon: 0.72, Episode reward: -20.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 410 finished (timesteps: 382896/10000000)\n",
      "Epsilon: 0.72, Episode reward: -20.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 411 finished (timesteps: 383869/10000000)\n",
      "Epsilon: 0.72, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 412 finished (timesteps: 384850/10000000)\n",
      "Epsilon: 0.72, Episode reward: -20.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 413 finished (timesteps: 385926/10000000)\n",
      "Epsilon: 0.72, Episode reward: -18.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 414 finished (timesteps: 386740/10000000)\n",
      "Epsilon: 0.72, Episode reward: -21.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 415 finished (timesteps: 387577/10000000)\n",
      "Epsilon: 0.72, Episode reward: -21.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 416 finished (timesteps: 388464/10000000)\n",
      "Epsilon: 0.71, Episode reward: -20.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 417 finished (timesteps: 389503/10000000)\n",
      "Epsilon: 0.71, Episode reward: -20.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 418 finished (timesteps: 390431/10000000)\n",
      "Epsilon: 0.71, Episode reward: -20.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 419 finished (timesteps: 391408/10000000)\n",
      "Epsilon: 0.71, Episode reward: -19.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 420 finished (timesteps: 392378/10000000)\n",
      "Epsilon: 0.71, Episode reward: -19.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 421 finished (timesteps: 393204/10000000)\n",
      "Epsilon: 0.71, Episode reward: -21.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 422 finished (timesteps: 394257/10000000)\n",
      "Epsilon: 0.71, Episode reward: -19.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 423 finished (timesteps: 395127/10000000)\n",
      "Epsilon: 0.71, Episode reward: -20.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 424 finished (timesteps: 396068/10000000)\n",
      "Epsilon: 0.71, Episode reward: -21.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 425 finished (timesteps: 397073/10000000)\n",
      "Epsilon: 0.71, Episode reward: -19.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 426 finished (timesteps: 398030/10000000)\n",
      "Epsilon: 0.70, Episode reward: -20.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 427 finished (timesteps: 398943/10000000)\n",
      "Epsilon: 0.70, Episode reward: -19.0, Mean reward: -20.21\n",
      "***************************\n",
      "Episode 428 finished (timesteps: 399808/10000000)\n",
      "Epsilon: 0.70, Episode reward: -21.0, Mean reward: -20.22\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 429 finished (timesteps: 400717/10000000)\n",
      "Epsilon: 0.70, Episode reward: -21.0, Mean reward: -20.22\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-400000.mp4.\n",
      "Moviepy - Writing video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-400000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-400000.mp4\n",
      "Episode 430 finished (timesteps: 401713/10000000)\n",
      "Epsilon: 0.70, Episode reward: -19.0, Mean reward: -20.20\n",
      "***************************\n",
      "Episode 431 finished (timesteps: 402597/10000000)\n",
      "Epsilon: 0.70, Episode reward: -20.0, Mean reward: -20.20\n",
      "***************************\n",
      "Episode 432 finished (timesteps: 403535/10000000)\n",
      "Epsilon: 0.70, Episode reward: -21.0, Mean reward: -20.20\n",
      "***************************\n",
      "Episode 433 finished (timesteps: 404401/10000000)\n",
      "Epsilon: 0.70, Episode reward: -21.0, Mean reward: -20.20\n",
      "***************************\n",
      "Episode 434 finished (timesteps: 405311/10000000)\n",
      "Epsilon: 0.70, Episode reward: -21.0, Mean reward: -20.21\n",
      "***************************\n",
      "Episode 435 finished (timesteps: 406281/10000000)\n",
      "Epsilon: 0.70, Episode reward: -21.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 436 finished (timesteps: 407116/10000000)\n",
      "Epsilon: 0.70, Episode reward: -20.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 437 finished (timesteps: 407930/10000000)\n",
      "Epsilon: 0.70, Episode reward: -21.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 438 finished (timesteps: 408753/10000000)\n",
      "Epsilon: 0.69, Episode reward: -21.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 439 finished (timesteps: 409517/10000000)\n",
      "Epsilon: 0.69, Episode reward: -21.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 440 finished (timesteps: 410564/10000000)\n",
      "Epsilon: 0.69, Episode reward: -20.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 441 finished (timesteps: 411472/10000000)\n",
      "Epsilon: 0.69, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 442 finished (timesteps: 412415/10000000)\n",
      "Epsilon: 0.69, Episode reward: -19.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 443 finished (timesteps: 413352/10000000)\n",
      "Epsilon: 0.69, Episode reward: -20.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 444 finished (timesteps: 414192/10000000)\n",
      "Epsilon: 0.69, Episode reward: -21.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 445 finished (timesteps: 415259/10000000)\n",
      "Epsilon: 0.69, Episode reward: -19.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 446 finished (timesteps: 416137/10000000)\n",
      "Epsilon: 0.69, Episode reward: -21.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 447 finished (timesteps: 417003/10000000)\n",
      "Epsilon: 0.69, Episode reward: -20.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 448 finished (timesteps: 417962/10000000)\n",
      "Epsilon: 0.69, Episode reward: -20.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 449 finished (timesteps: 418840/10000000)\n",
      "Epsilon: 0.68, Episode reward: -21.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 450 finished (timesteps: 419626/10000000)\n",
      "Epsilon: 0.68, Episode reward: -21.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 451 finished (timesteps: 420436/10000000)\n",
      "Epsilon: 0.68, Episode reward: -21.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 452 finished (timesteps: 421316/10000000)\n",
      "Epsilon: 0.68, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 453 finished (timesteps: 422261/10000000)\n",
      "Epsilon: 0.68, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 454 finished (timesteps: 423439/10000000)\n",
      "Epsilon: 0.68, Episode reward: -19.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 455 finished (timesteps: 424501/10000000)\n",
      "Epsilon: 0.68, Episode reward: -19.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 456 finished (timesteps: 425597/10000000)\n",
      "Epsilon: 0.68, Episode reward: -18.0, Mean reward: -20.20\n",
      "***************************\n",
      "Episode 457 finished (timesteps: 426755/10000000)\n",
      "Epsilon: 0.68, Episode reward: -18.0, Mean reward: -20.19\n",
      "***************************\n",
      "Episode 458 finished (timesteps: 427715/10000000)\n",
      "Epsilon: 0.68, Episode reward: -20.0, Mean reward: -20.20\n",
      "***************************\n",
      "Episode 459 finished (timesteps: 428742/10000000)\n",
      "Epsilon: 0.67, Episode reward: -21.0, Mean reward: -20.20\n",
      "***************************\n",
      "Episode 460 finished (timesteps: 429587/10000000)\n",
      "Epsilon: 0.67, Episode reward: -21.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 461 finished (timesteps: 430470/10000000)\n",
      "Epsilon: 0.67, Episode reward: -21.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 462 finished (timesteps: 431354/10000000)\n",
      "Epsilon: 0.67, Episode reward: -21.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 463 finished (timesteps: 432420/10000000)\n",
      "Epsilon: 0.67, Episode reward: -19.0, Mean reward: -20.20\n",
      "***************************\n",
      "Episode 464 finished (timesteps: 433381/10000000)\n",
      "Epsilon: 0.67, Episode reward: -20.0, Mean reward: -20.19\n",
      "***************************\n",
      "Episode 465 finished (timesteps: 434275/10000000)\n",
      "Epsilon: 0.67, Episode reward: -20.0, Mean reward: -20.18\n",
      "***************************\n",
      "Episode 466 finished (timesteps: 435092/10000000)\n",
      "Epsilon: 0.67, Episode reward: -21.0, Mean reward: -20.19\n",
      "***************************\n",
      "Episode 467 finished (timesteps: 436068/10000000)\n",
      "Epsilon: 0.67, Episode reward: -20.0, Mean reward: -20.18\n",
      "***************************\n",
      "Episode 468 finished (timesteps: 436986/10000000)\n",
      "Epsilon: 0.67, Episode reward: -20.0, Mean reward: -20.17\n",
      "***************************\n",
      "Episode 469 finished (timesteps: 437837/10000000)\n",
      "Epsilon: 0.67, Episode reward: -21.0, Mean reward: -20.17\n",
      "***************************\n",
      "Episode 470 finished (timesteps: 438677/10000000)\n",
      "Epsilon: 0.66, Episode reward: -20.0, Mean reward: -20.17\n",
      "***************************\n",
      "Episode 471 finished (timesteps: 439495/10000000)\n",
      "Epsilon: 0.66, Episode reward: -21.0, Mean reward: -20.19\n",
      "***************************\n",
      "Episode 472 finished (timesteps: 440491/10000000)\n",
      "Epsilon: 0.66, Episode reward: -19.0, Mean reward: -20.19\n",
      "***************************\n",
      "Episode 473 finished (timesteps: 441560/10000000)\n",
      "Epsilon: 0.66, Episode reward: -20.0, Mean reward: -20.19\n",
      "***************************\n",
      "Episode 474 finished (timesteps: 442440/10000000)\n",
      "Epsilon: 0.66, Episode reward: -21.0, Mean reward: -20.19\n",
      "***************************\n",
      "Episode 475 finished (timesteps: 443337/10000000)\n",
      "Epsilon: 0.66, Episode reward: -21.0, Mean reward: -20.20\n",
      "***************************\n",
      "Episode 476 finished (timesteps: 444516/10000000)\n",
      "Epsilon: 0.66, Episode reward: -19.0, Mean reward: -20.18\n",
      "***************************\n",
      "Episode 477 finished (timesteps: 445365/10000000)\n",
      "Epsilon: 0.66, Episode reward: -21.0, Mean reward: -20.20\n",
      "***************************\n",
      "Episode 478 finished (timesteps: 446275/10000000)\n",
      "Epsilon: 0.66, Episode reward: -21.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 479 finished (timesteps: 447264/10000000)\n",
      "Epsilon: 0.66, Episode reward: -20.0, Mean reward: -20.21\n",
      "***************************\n",
      "Episode 480 finished (timesteps: 448284/10000000)\n",
      "Epsilon: 0.66, Episode reward: -20.0, Mean reward: -20.20\n",
      "***************************\n",
      "Episode 481 finished (timesteps: 449154/10000000)\n",
      "Epsilon: 0.65, Episode reward: -21.0, Mean reward: -20.20\n",
      "***************************\n",
      "Episode 482 finished (timesteps: 450010/10000000)\n",
      "Epsilon: 0.65, Episode reward: -21.0, Mean reward: -20.20\n",
      "***************************\n",
      "Episode 483 finished (timesteps: 451060/10000000)\n",
      "Epsilon: 0.65, Episode reward: -21.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 484 finished (timesteps: 451972/10000000)\n",
      "Epsilon: 0.65, Episode reward: -21.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 485 finished (timesteps: 452736/10000000)\n",
      "Epsilon: 0.65, Episode reward: -21.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 486 finished (timesteps: 453559/10000000)\n",
      "Epsilon: 0.65, Episode reward: -21.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 487 finished (timesteps: 454381/10000000)\n",
      "Epsilon: 0.65, Episode reward: -21.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 488 finished (timesteps: 455330/10000000)\n",
      "Epsilon: 0.65, Episode reward: -20.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 489 finished (timesteps: 456383/10000000)\n",
      "Epsilon: 0.65, Episode reward: -20.0, Mean reward: -20.26\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 490 finished (timesteps: 457285/10000000)\n",
      "Epsilon: 0.65, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 491 finished (timesteps: 458215/10000000)\n",
      "Epsilon: 0.65, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 492 finished (timesteps: 459158/10000000)\n",
      "Epsilon: 0.64, Episode reward: -21.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 493 finished (timesteps: 460192/10000000)\n",
      "Epsilon: 0.64, Episode reward: -19.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 494 finished (timesteps: 461075/10000000)\n",
      "Epsilon: 0.64, Episode reward: -20.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 495 finished (timesteps: 461985/10000000)\n",
      "Epsilon: 0.64, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 496 finished (timesteps: 462941/10000000)\n",
      "Epsilon: 0.64, Episode reward: -20.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 497 finished (timesteps: 464184/10000000)\n",
      "Epsilon: 0.64, Episode reward: -18.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 498 finished (timesteps: 465049/10000000)\n",
      "Epsilon: 0.64, Episode reward: -20.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 499 finished (timesteps: 465960/10000000)\n",
      "Epsilon: 0.64, Episode reward: -21.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 500 finished (timesteps: 466827/10000000)\n",
      "Epsilon: 0.64, Episode reward: -21.0, Mean reward: -20.24\n",
      "Saving checkpoint...\n",
      "Checkpoint saved into runs/checkpoints/CNN_DDQN_Pong-v5_11-04-2023_16:28:30\n",
      "***************************\n",
      "Episode 501 finished (timesteps: 467760/10000000)\n",
      "Epsilon: 0.64, Episode reward: -20.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 502 finished (timesteps: 468707/10000000)\n",
      "Epsilon: 0.63, Episode reward: -20.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 503 finished (timesteps: 469645/10000000)\n",
      "Epsilon: 0.63, Episode reward: -19.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 504 finished (timesteps: 470671/10000000)\n",
      "Epsilon: 0.63, Episode reward: -18.0, Mean reward: -20.20\n",
      "***************************\n",
      "Episode 505 finished (timesteps: 471601/10000000)\n",
      "Epsilon: 0.63, Episode reward: -20.0, Mean reward: -20.19\n",
      "***************************\n",
      "Episode 506 finished (timesteps: 472499/10000000)\n",
      "Epsilon: 0.63, Episode reward: -20.0, Mean reward: -20.19\n",
      "***************************\n",
      "Episode 507 finished (timesteps: 473345/10000000)\n",
      "Epsilon: 0.63, Episode reward: -21.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 508 finished (timesteps: 474401/10000000)\n",
      "Epsilon: 0.63, Episode reward: -18.0, Mean reward: -20.20\n",
      "***************************\n",
      "Episode 509 finished (timesteps: 475391/10000000)\n",
      "Epsilon: 0.63, Episode reward: -20.0, Mean reward: -20.20\n",
      "***************************\n",
      "Episode 510 finished (timesteps: 476327/10000000)\n",
      "Epsilon: 0.63, Episode reward: -21.0, Mean reward: -20.21\n",
      "***************************\n",
      "Episode 511 finished (timesteps: 477183/10000000)\n",
      "Epsilon: 0.63, Episode reward: -20.0, Mean reward: -20.20\n",
      "***************************\n",
      "Episode 512 finished (timesteps: 478125/10000000)\n",
      "Epsilon: 0.63, Episode reward: -21.0, Mean reward: -20.21\n",
      "***************************\n",
      "Episode 513 finished (timesteps: 479068/10000000)\n",
      "Epsilon: 0.62, Episode reward: -20.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 514 finished (timesteps: 479874/10000000)\n",
      "Epsilon: 0.62, Episode reward: -21.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 515 finished (timesteps: 480904/10000000)\n",
      "Epsilon: 0.62, Episode reward: -19.0, Mean reward: -20.21\n",
      "***************************\n",
      "Episode 516 finished (timesteps: 481727/10000000)\n",
      "Epsilon: 0.62, Episode reward: -21.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 517 finished (timesteps: 482509/10000000)\n",
      "Epsilon: 0.62, Episode reward: -21.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 518 finished (timesteps: 483459/10000000)\n",
      "Epsilon: 0.62, Episode reward: -19.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 519 finished (timesteps: 484483/10000000)\n",
      "Epsilon: 0.62, Episode reward: -19.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 520 finished (timesteps: 485494/10000000)\n",
      "Epsilon: 0.62, Episode reward: -19.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 521 finished (timesteps: 486391/10000000)\n",
      "Epsilon: 0.62, Episode reward: -20.0, Mean reward: -20.21\n",
      "***************************\n",
      "Episode 522 finished (timesteps: 487465/10000000)\n",
      "Epsilon: 0.62, Episode reward: -21.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 523 finished (timesteps: 488452/10000000)\n",
      "Epsilon: 0.62, Episode reward: -21.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 524 finished (timesteps: 489417/10000000)\n",
      "Epsilon: 0.61, Episode reward: -19.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 525 finished (timesteps: 490353/10000000)\n",
      "Epsilon: 0.61, Episode reward: -20.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 526 finished (timesteps: 491300/10000000)\n",
      "Epsilon: 0.61, Episode reward: -20.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 527 finished (timesteps: 492059/10000000)\n",
      "Epsilon: 0.61, Episode reward: -21.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 528 finished (timesteps: 492956/10000000)\n",
      "Epsilon: 0.61, Episode reward: -20.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 529 finished (timesteps: 493827/10000000)\n",
      "Epsilon: 0.61, Episode reward: -21.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 530 finished (timesteps: 494703/10000000)\n",
      "Epsilon: 0.61, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 531 finished (timesteps: 495650/10000000)\n",
      "Epsilon: 0.61, Episode reward: -21.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 532 finished (timesteps: 496556/10000000)\n",
      "Epsilon: 0.61, Episode reward: -21.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 533 finished (timesteps: 497456/10000000)\n",
      "Epsilon: 0.61, Episode reward: -20.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 534 finished (timesteps: 498377/10000000)\n",
      "Epsilon: 0.61, Episode reward: -20.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 535 finished (timesteps: 499285/10000000)\n",
      "Epsilon: 0.60, Episode reward: -21.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 536 finished (timesteps: 500227/10000000)\n",
      "Epsilon: 0.60, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 537 finished (timesteps: 501155/10000000)\n",
      "Epsilon: 0.60, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 538 finished (timesteps: 502192/10000000)\n",
      "Epsilon: 0.60, Episode reward: -20.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 539 finished (timesteps: 503163/10000000)\n",
      "Epsilon: 0.60, Episode reward: -21.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 540 finished (timesteps: 503981/10000000)\n",
      "Epsilon: 0.60, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 541 finished (timesteps: 504986/10000000)\n",
      "Epsilon: 0.60, Episode reward: -20.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 542 finished (timesteps: 505794/10000000)\n",
      "Epsilon: 0.60, Episode reward: -21.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 543 finished (timesteps: 506926/10000000)\n",
      "Epsilon: 0.60, Episode reward: -17.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 544 finished (timesteps: 507857/10000000)\n",
      "Epsilon: 0.60, Episode reward: -20.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 545 finished (timesteps: 508886/10000000)\n",
      "Epsilon: 0.60, Episode reward: -19.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 546 finished (timesteps: 509736/10000000)\n",
      "Epsilon: 0.59, Episode reward: -21.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 547 finished (timesteps: 510619/10000000)\n",
      "Epsilon: 0.59, Episode reward: -21.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 548 finished (timesteps: 511494/10000000)\n",
      "Epsilon: 0.59, Episode reward: -20.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 549 finished (timesteps: 512273/10000000)\n",
      "Epsilon: 0.59, Episode reward: -21.0, Mean reward: -20.24\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 550 finished (timesteps: 513201/10000000)\n",
      "Epsilon: 0.59, Episode reward: -19.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 551 finished (timesteps: 513987/10000000)\n",
      "Epsilon: 0.59, Episode reward: -21.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 552 finished (timesteps: 514947/10000000)\n",
      "Epsilon: 0.59, Episode reward: -21.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 553 finished (timesteps: 515849/10000000)\n",
      "Epsilon: 0.59, Episode reward: -20.0, Mean reward: -20.21\n",
      "***************************\n",
      "Episode 554 finished (timesteps: 516803/10000000)\n",
      "Epsilon: 0.59, Episode reward: -20.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 555 finished (timesteps: 517668/10000000)\n",
      "Epsilon: 0.59, Episode reward: -20.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 556 finished (timesteps: 518872/10000000)\n",
      "Epsilon: 0.59, Episode reward: -19.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 557 finished (timesteps: 519862/10000000)\n",
      "Epsilon: 0.58, Episode reward: -20.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 558 finished (timesteps: 520742/10000000)\n",
      "Epsilon: 0.58, Episode reward: -21.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 559 finished (timesteps: 521706/10000000)\n",
      "Epsilon: 0.58, Episode reward: -21.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 560 finished (timesteps: 522467/10000000)\n",
      "Epsilon: 0.58, Episode reward: -21.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 561 finished (timesteps: 523519/10000000)\n",
      "Epsilon: 0.58, Episode reward: -20.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 562 finished (timesteps: 524437/10000000)\n",
      "Epsilon: 0.58, Episode reward: -20.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 563 finished (timesteps: 525358/10000000)\n",
      "Epsilon: 0.58, Episode reward: -20.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 564 finished (timesteps: 526313/10000000)\n",
      "Epsilon: 0.58, Episode reward: -20.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 565 finished (timesteps: 527091/10000000)\n",
      "Epsilon: 0.58, Episode reward: -21.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 566 finished (timesteps: 527958/10000000)\n",
      "Epsilon: 0.58, Episode reward: -20.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 567 finished (timesteps: 528957/10000000)\n",
      "Epsilon: 0.58, Episode reward: -19.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 568 finished (timesteps: 529780/10000000)\n",
      "Epsilon: 0.57, Episode reward: -21.0, Mean reward: -20.26\n",
      "***************************\n",
      "Episode 569 finished (timesteps: 530759/10000000)\n",
      "Epsilon: 0.57, Episode reward: -20.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 570 finished (timesteps: 531953/10000000)\n",
      "Epsilon: 0.57, Episode reward: -19.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 571 finished (timesteps: 532877/10000000)\n",
      "Epsilon: 0.57, Episode reward: -20.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 572 finished (timesteps: 533752/10000000)\n",
      "Epsilon: 0.57, Episode reward: -20.0, Mean reward: -20.24\n",
      "***************************\n",
      "Episode 573 finished (timesteps: 534691/10000000)\n",
      "Epsilon: 0.57, Episode reward: -21.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 574 finished (timesteps: 535510/10000000)\n",
      "Epsilon: 0.57, Episode reward: -21.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 575 finished (timesteps: 536500/10000000)\n",
      "Epsilon: 0.57, Episode reward: -21.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 576 finished (timesteps: 537368/10000000)\n",
      "Epsilon: 0.57, Episode reward: -21.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 577 finished (timesteps: 538717/10000000)\n",
      "Epsilon: 0.57, Episode reward: -17.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 578 finished (timesteps: 539523/10000000)\n",
      "Epsilon: 0.56, Episode reward: -21.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 579 finished (timesteps: 540448/10000000)\n",
      "Epsilon: 0.56, Episode reward: -20.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 580 finished (timesteps: 541378/10000000)\n",
      "Epsilon: 0.56, Episode reward: -20.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 581 finished (timesteps: 542228/10000000)\n",
      "Epsilon: 0.56, Episode reward: -21.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 582 finished (timesteps: 543366/10000000)\n",
      "Epsilon: 0.56, Episode reward: -18.0, Mean reward: -20.20\n",
      "***************************\n",
      "Episode 583 finished (timesteps: 544366/10000000)\n",
      "Epsilon: 0.56, Episode reward: -21.0, Mean reward: -20.20\n",
      "***************************\n",
      "Episode 584 finished (timesteps: 545183/10000000)\n",
      "Epsilon: 0.56, Episode reward: -21.0, Mean reward: -20.20\n",
      "***************************\n",
      "Episode 585 finished (timesteps: 546080/10000000)\n",
      "Epsilon: 0.56, Episode reward: -20.0, Mean reward: -20.19\n",
      "***************************\n",
      "Episode 586 finished (timesteps: 547231/10000000)\n",
      "Epsilon: 0.56, Episode reward: -18.0, Mean reward: -20.16\n",
      "***************************\n",
      "Episode 587 finished (timesteps: 548249/10000000)\n",
      "Epsilon: 0.56, Episode reward: -21.0, Mean reward: -20.16\n",
      "***************************\n",
      "Episode 588 finished (timesteps: 549297/10000000)\n",
      "Epsilon: 0.56, Episode reward: -18.0, Mean reward: -20.14\n",
      "***************************\n",
      "Episode 589 finished (timesteps: 550087/10000000)\n",
      "Epsilon: 0.55, Episode reward: -21.0, Mean reward: -20.15\n",
      "***************************\n",
      "Episode 590 finished (timesteps: 550995/10000000)\n",
      "Epsilon: 0.55, Episode reward: -20.0, Mean reward: -20.14\n",
      "***************************\n",
      "Episode 591 finished (timesteps: 552046/10000000)\n",
      "Epsilon: 0.55, Episode reward: -19.0, Mean reward: -20.12\n",
      "***************************\n",
      "Episode 592 finished (timesteps: 553121/10000000)\n",
      "Epsilon: 0.55, Episode reward: -20.0, Mean reward: -20.11\n",
      "***************************\n",
      "Episode 593 finished (timesteps: 554077/10000000)\n",
      "Epsilon: 0.55, Episode reward: -20.0, Mean reward: -20.12\n",
      "***************************\n",
      "Episode 594 finished (timesteps: 555104/10000000)\n",
      "Epsilon: 0.55, Episode reward: -18.0, Mean reward: -20.10\n",
      "***************************\n",
      "Episode 595 finished (timesteps: 556062/10000000)\n",
      "Epsilon: 0.55, Episode reward: -20.0, Mean reward: -20.09\n",
      "***************************\n",
      "Episode 596 finished (timesteps: 557173/10000000)\n",
      "Epsilon: 0.55, Episode reward: -20.0, Mean reward: -20.09\n",
      "***************************\n",
      "Episode 597 finished (timesteps: 557994/10000000)\n",
      "Epsilon: 0.55, Episode reward: -21.0, Mean reward: -20.12\n",
      "***************************\n",
      "Episode 598 finished (timesteps: 558907/10000000)\n",
      "Epsilon: 0.55, Episode reward: -21.0, Mean reward: -20.13\n",
      "***************************\n",
      "Episode 599 finished (timesteps: 559813/10000000)\n",
      "Epsilon: 0.54, Episode reward: -21.0, Mean reward: -20.13\n",
      "***************************\n",
      "Episode 600 finished (timesteps: 560633/10000000)\n",
      "Epsilon: 0.54, Episode reward: -21.0, Mean reward: -20.13\n",
      "***************************\n",
      "Episode 601 finished (timesteps: 561744/10000000)\n",
      "Epsilon: 0.54, Episode reward: -20.0, Mean reward: -20.13\n",
      "***************************\n",
      "Episode 602 finished (timesteps: 562561/10000000)\n",
      "Epsilon: 0.54, Episode reward: -21.0, Mean reward: -20.14\n",
      "***************************\n",
      "Episode 603 finished (timesteps: 563653/10000000)\n",
      "Epsilon: 0.54, Episode reward: -21.0, Mean reward: -20.16\n",
      "***************************\n",
      "Episode 604 finished (timesteps: 564578/10000000)\n",
      "Epsilon: 0.54, Episode reward: -21.0, Mean reward: -20.19\n",
      "***************************\n",
      "Episode 605 finished (timesteps: 565550/10000000)\n",
      "Epsilon: 0.54, Episode reward: -20.0, Mean reward: -20.19\n",
      "***************************\n",
      "Episode 606 finished (timesteps: 566612/10000000)\n",
      "Epsilon: 0.54, Episode reward: -19.0, Mean reward: -20.18\n",
      "***************************\n",
      "Episode 607 finished (timesteps: 567794/10000000)\n",
      "Epsilon: 0.54, Episode reward: -19.0, Mean reward: -20.16\n",
      "***************************\n",
      "Episode 608 finished (timesteps: 568819/10000000)\n",
      "Epsilon: 0.54, Episode reward: -21.0, Mean reward: -20.19\n",
      "***************************\n",
      "Episode 609 finished (timesteps: 569848/10000000)\n",
      "Epsilon: 0.53, Episode reward: -21.0, Mean reward: -20.20\n",
      "***************************\n",
      "Episode 610 finished (timesteps: 570686/10000000)\n",
      "Epsilon: 0.53, Episode reward: -21.0, Mean reward: -20.20\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 611 finished (timesteps: 571444/10000000)\n",
      "Epsilon: 0.53, Episode reward: -21.0, Mean reward: -20.21\n",
      "***************************\n",
      "Episode 612 finished (timesteps: 572337/10000000)\n",
      "Epsilon: 0.53, Episode reward: -21.0, Mean reward: -20.21\n",
      "***************************\n",
      "Episode 613 finished (timesteps: 573217/10000000)\n",
      "Epsilon: 0.53, Episode reward: -21.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 614 finished (timesteps: 574217/10000000)\n",
      "Epsilon: 0.53, Episode reward: -20.0, Mean reward: -20.21\n",
      "***************************\n",
      "Episode 615 finished (timesteps: 575120/10000000)\n",
      "Epsilon: 0.53, Episode reward: -21.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 616 finished (timesteps: 576135/10000000)\n",
      "Epsilon: 0.53, Episode reward: -20.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 617 finished (timesteps: 576999/10000000)\n",
      "Epsilon: 0.53, Episode reward: -20.0, Mean reward: -20.21\n",
      "***************************\n",
      "Episode 618 finished (timesteps: 577837/10000000)\n",
      "Epsilon: 0.53, Episode reward: -20.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 619 finished (timesteps: 578732/10000000)\n",
      "Epsilon: 0.53, Episode reward: -20.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 620 finished (timesteps: 579948/10000000)\n",
      "Epsilon: 0.52, Episode reward: -19.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 621 finished (timesteps: 581121/10000000)\n",
      "Epsilon: 0.52, Episode reward: -19.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 622 finished (timesteps: 581901/10000000)\n",
      "Epsilon: 0.52, Episode reward: -21.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 623 finished (timesteps: 582984/10000000)\n",
      "Epsilon: 0.52, Episode reward: -19.0, Mean reward: -20.20\n",
      "***************************\n",
      "Episode 624 finished (timesteps: 584026/10000000)\n",
      "Epsilon: 0.52, Episode reward: -21.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 625 finished (timesteps: 585019/10000000)\n",
      "Epsilon: 0.52, Episode reward: -20.0, Mean reward: -20.22\n",
      "***************************\n",
      "Episode 626 finished (timesteps: 585843/10000000)\n",
      "Epsilon: 0.52, Episode reward: -21.0, Mean reward: -20.23\n",
      "***************************\n",
      "Episode 627 finished (timesteps: 587195/10000000)\n",
      "Epsilon: 0.52, Episode reward: -18.0, Mean reward: -20.20\n",
      "***************************\n",
      "Episode 628 finished (timesteps: 588305/10000000)\n",
      "Epsilon: 0.52, Episode reward: -21.0, Mean reward: -20.21\n",
      "***************************\n",
      "Episode 629 finished (timesteps: 589393/10000000)\n",
      "Epsilon: 0.52, Episode reward: -19.0, Mean reward: -20.19\n",
      "***************************\n",
      "Episode 630 finished (timesteps: 590376/10000000)\n",
      "Epsilon: 0.51, Episode reward: -20.0, Mean reward: -20.18\n",
      "***************************\n",
      "Episode 631 finished (timesteps: 591381/10000000)\n",
      "Epsilon: 0.51, Episode reward: -20.0, Mean reward: -20.17\n",
      "***************************\n",
      "Episode 632 finished (timesteps: 592190/10000000)\n",
      "Epsilon: 0.51, Episode reward: -21.0, Mean reward: -20.17\n",
      "***************************\n",
      "Episode 633 finished (timesteps: 593016/10000000)\n",
      "Epsilon: 0.51, Episode reward: -21.0, Mean reward: -20.18\n",
      "***************************\n",
      "Episode 634 finished (timesteps: 594098/10000000)\n",
      "Epsilon: 0.51, Episode reward: -19.0, Mean reward: -20.17\n",
      "***************************\n",
      "Episode 635 finished (timesteps: 595228/10000000)\n",
      "Epsilon: 0.51, Episode reward: -21.0, Mean reward: -20.17\n",
      "***************************\n",
      "Episode 636 finished (timesteps: 596344/10000000)\n",
      "Epsilon: 0.51, Episode reward: -19.0, Mean reward: -20.15\n",
      "***************************\n",
      "Episode 637 finished (timesteps: 597259/10000000)\n",
      "Epsilon: 0.51, Episode reward: -20.0, Mean reward: -20.14\n",
      "***************************\n",
      "Episode 638 finished (timesteps: 598193/10000000)\n",
      "Epsilon: 0.51, Episode reward: -21.0, Mean reward: -20.15\n",
      "***************************\n",
      "Episode 639 finished (timesteps: 599073/10000000)\n",
      "Epsilon: 0.51, Episode reward: -21.0, Mean reward: -20.15\n",
      "***************************\n",
      "Episode 640 finished (timesteps: 600002/10000000)\n",
      "Epsilon: 0.50, Episode reward: -21.0, Mean reward: -20.15\n",
      "***************************\n",
      "Episode 641 finished (timesteps: 600967/10000000)\n",
      "Epsilon: 0.50, Episode reward: -19.0, Mean reward: -20.14\n",
      "***************************\n",
      "Episode 642 finished (timesteps: 601939/10000000)\n",
      "Epsilon: 0.50, Episode reward: -21.0, Mean reward: -20.14\n",
      "***************************\n",
      "Episode 643 finished (timesteps: 603024/10000000)\n",
      "Epsilon: 0.50, Episode reward: -20.0, Mean reward: -20.17\n",
      "***************************\n",
      "Episode 644 finished (timesteps: 604115/10000000)\n",
      "Epsilon: 0.50, Episode reward: -19.0, Mean reward: -20.16\n",
      "***************************\n",
      "Episode 645 finished (timesteps: 605147/10000000)\n",
      "Epsilon: 0.50, Episode reward: -20.0, Mean reward: -20.17\n",
      "***************************\n",
      "Episode 646 finished (timesteps: 606360/10000000)\n",
      "Epsilon: 0.50, Episode reward: -19.0, Mean reward: -20.15\n",
      "***************************\n",
      "Episode 647 finished (timesteps: 607379/10000000)\n",
      "Epsilon: 0.50, Episode reward: -20.0, Mean reward: -20.14\n",
      "***************************\n",
      "Episode 648 finished (timesteps: 608647/10000000)\n",
      "Epsilon: 0.50, Episode reward: -20.0, Mean reward: -20.14\n",
      "***************************\n",
      "Episode 649 finished (timesteps: 609498/10000000)\n",
      "Epsilon: 0.50, Episode reward: -21.0, Mean reward: -20.14\n",
      "***************************\n",
      "Episode 650 finished (timesteps: 610559/10000000)\n",
      "Epsilon: 0.49, Episode reward: -20.0, Mean reward: -20.15\n",
      "***************************\n",
      "Episode 651 finished (timesteps: 611664/10000000)\n",
      "Epsilon: 0.49, Episode reward: -20.0, Mean reward: -20.14\n",
      "***************************\n",
      "Episode 652 finished (timesteps: 612746/10000000)\n",
      "Epsilon: 0.49, Episode reward: -19.0, Mean reward: -20.12\n",
      "***************************\n",
      "Episode 653 finished (timesteps: 614022/10000000)\n",
      "Epsilon: 0.49, Episode reward: -18.0, Mean reward: -20.10\n",
      "***************************\n",
      "Episode 654 finished (timesteps: 614951/10000000)\n",
      "Epsilon: 0.49, Episode reward: -20.0, Mean reward: -20.10\n",
      "***************************\n",
      "Episode 655 finished (timesteps: 616113/10000000)\n",
      "Epsilon: 0.49, Episode reward: -19.0, Mean reward: -20.09\n",
      "***************************\n",
      "Episode 656 finished (timesteps: 617053/10000000)\n",
      "Epsilon: 0.49, Episode reward: -20.0, Mean reward: -20.10\n",
      "***************************\n",
      "Episode 657 finished (timesteps: 617885/10000000)\n",
      "Epsilon: 0.49, Episode reward: -20.0, Mean reward: -20.10\n",
      "***************************\n",
      "Episode 658 finished (timesteps: 619068/10000000)\n",
      "Epsilon: 0.49, Episode reward: -18.0, Mean reward: -20.07\n",
      "***************************\n",
      "Episode 659 finished (timesteps: 619902/10000000)\n",
      "Epsilon: 0.49, Episode reward: -21.0, Mean reward: -20.07\n",
      "***************************\n",
      "Episode 660 finished (timesteps: 620799/10000000)\n",
      "Epsilon: 0.48, Episode reward: -20.0, Mean reward: -20.06\n",
      "***************************\n",
      "Episode 661 finished (timesteps: 621810/10000000)\n",
      "Epsilon: 0.48, Episode reward: -20.0, Mean reward: -20.06\n",
      "***************************\n",
      "Episode 662 finished (timesteps: 623200/10000000)\n",
      "Epsilon: 0.48, Episode reward: -19.0, Mean reward: -20.05\n",
      "***************************\n",
      "Episode 663 finished (timesteps: 624194/10000000)\n",
      "Epsilon: 0.48, Episode reward: -19.0, Mean reward: -20.04\n",
      "***************************\n",
      "Episode 664 finished (timesteps: 625175/10000000)\n",
      "Epsilon: 0.48, Episode reward: -20.0, Mean reward: -20.04\n",
      "***************************\n",
      "Episode 665 finished (timesteps: 626017/10000000)\n",
      "Epsilon: 0.48, Episode reward: -20.0, Mean reward: -20.03\n",
      "***************************\n",
      "Episode 666 finished (timesteps: 627108/10000000)\n",
      "Epsilon: 0.48, Episode reward: -21.0, Mean reward: -20.04\n",
      "***************************\n",
      "Episode 667 finished (timesteps: 628247/10000000)\n",
      "Epsilon: 0.48, Episode reward: -20.0, Mean reward: -20.05\n",
      "***************************\n",
      "Episode 668 finished (timesteps: 629137/10000000)\n",
      "Epsilon: 0.48, Episode reward: -20.0, Mean reward: -20.04\n",
      "***************************\n",
      "Episode 669 finished (timesteps: 630114/10000000)\n",
      "Epsilon: 0.48, Episode reward: -19.0, Mean reward: -20.03\n",
      "***************************\n",
      "Episode 670 finished (timesteps: 630963/10000000)\n",
      "Epsilon: 0.47, Episode reward: -21.0, Mean reward: -20.05\n",
      "***************************\n",
      "Episode 671 finished (timesteps: 631966/10000000)\n",
      "Epsilon: 0.47, Episode reward: -20.0, Mean reward: -20.05\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 672 finished (timesteps: 632983/10000000)\n",
      "Epsilon: 0.47, Episode reward: -21.0, Mean reward: -20.06\n",
      "***************************\n",
      "Episode 673 finished (timesteps: 633866/10000000)\n",
      "Epsilon: 0.47, Episode reward: -21.0, Mean reward: -20.06\n",
      "***************************\n",
      "Episode 674 finished (timesteps: 635258/10000000)\n",
      "Epsilon: 0.47, Episode reward: -18.0, Mean reward: -20.03\n",
      "***************************\n",
      "Episode 675 finished (timesteps: 636105/10000000)\n",
      "Epsilon: 0.47, Episode reward: -21.0, Mean reward: -20.03\n",
      "***************************\n",
      "Episode 676 finished (timesteps: 637491/10000000)\n",
      "Epsilon: 0.47, Episode reward: -18.0, Mean reward: -20.00\n",
      "***************************\n",
      "Episode 677 finished (timesteps: 638731/10000000)\n",
      "Epsilon: 0.47, Episode reward: -19.0, Mean reward: -20.02\n",
      "***************************\n",
      "Episode 678 finished (timesteps: 639539/10000000)\n",
      "Epsilon: 0.47, Episode reward: -21.0, Mean reward: -20.02\n",
      "***************************\n",
      "Episode 679 finished (timesteps: 640643/10000000)\n",
      "Epsilon: 0.46, Episode reward: -20.0, Mean reward: -20.02\n",
      "***************************\n",
      "Episode 680 finished (timesteps: 641564/10000000)\n",
      "Epsilon: 0.46, Episode reward: -21.0, Mean reward: -20.03\n",
      "***************************\n",
      "Episode 681 finished (timesteps: 642537/10000000)\n",
      "Epsilon: 0.46, Episode reward: -19.0, Mean reward: -20.01\n",
      "***************************\n",
      "Episode 682 finished (timesteps: 643703/10000000)\n",
      "Epsilon: 0.46, Episode reward: -19.0, Mean reward: -20.02\n",
      "***************************\n",
      "Episode 683 finished (timesteps: 644705/10000000)\n",
      "Epsilon: 0.46, Episode reward: -20.0, Mean reward: -20.01\n",
      "***************************\n",
      "Episode 684 finished (timesteps: 645618/10000000)\n",
      "Epsilon: 0.46, Episode reward: -19.0, Mean reward: -19.99\n",
      "***************************\n",
      "Episode 685 finished (timesteps: 646706/10000000)\n",
      "Epsilon: 0.46, Episode reward: -21.0, Mean reward: -20.00\n",
      "***************************\n",
      "Episode 686 finished (timesteps: 647765/10000000)\n",
      "Epsilon: 0.46, Episode reward: -21.0, Mean reward: -20.03\n",
      "***************************\n",
      "Episode 687 finished (timesteps: 648919/10000000)\n",
      "Epsilon: 0.46, Episode reward: -20.0, Mean reward: -20.02\n",
      "***************************\n",
      "Episode 688 finished (timesteps: 650059/10000000)\n",
      "Epsilon: 0.46, Episode reward: -20.0, Mean reward: -20.04\n",
      "***************************\n",
      "Episode 689 finished (timesteps: 651105/10000000)\n",
      "Epsilon: 0.45, Episode reward: -19.0, Mean reward: -20.02\n",
      "***************************\n",
      "Episode 690 finished (timesteps: 652013/10000000)\n",
      "Epsilon: 0.45, Episode reward: -21.0, Mean reward: -20.03\n",
      "***************************\n",
      "Episode 691 finished (timesteps: 653001/10000000)\n",
      "Epsilon: 0.45, Episode reward: -20.0, Mean reward: -20.04\n",
      "***************************\n",
      "Episode 692 finished (timesteps: 654161/10000000)\n",
      "Epsilon: 0.45, Episode reward: -20.0, Mean reward: -20.04\n",
      "***************************\n",
      "Episode 693 finished (timesteps: 655277/10000000)\n",
      "Epsilon: 0.45, Episode reward: -21.0, Mean reward: -20.05\n",
      "***************************\n",
      "Episode 694 finished (timesteps: 656450/10000000)\n",
      "Epsilon: 0.45, Episode reward: -20.0, Mean reward: -20.07\n",
      "***************************\n",
      "Episode 695 finished (timesteps: 657440/10000000)\n",
      "Epsilon: 0.45, Episode reward: -20.0, Mean reward: -20.07\n",
      "***************************\n",
      "Episode 696 finished (timesteps: 658441/10000000)\n",
      "Epsilon: 0.45, Episode reward: -21.0, Mean reward: -20.08\n",
      "***************************\n",
      "Episode 697 finished (timesteps: 659566/10000000)\n",
      "Epsilon: 0.45, Episode reward: -21.0, Mean reward: -20.08\n",
      "***************************\n",
      "Episode 698 finished (timesteps: 660772/10000000)\n",
      "Epsilon: 0.44, Episode reward: -18.0, Mean reward: -20.05\n",
      "***************************\n",
      "Episode 699 finished (timesteps: 661607/10000000)\n",
      "Epsilon: 0.44, Episode reward: -21.0, Mean reward: -20.05\n",
      "***************************\n",
      "Episode 700 finished (timesteps: 662764/10000000)\n",
      "Epsilon: 0.44, Episode reward: -20.0, Mean reward: -20.04\n",
      "***************************\n",
      "Episode 701 finished (timesteps: 663765/10000000)\n",
      "Epsilon: 0.44, Episode reward: -21.0, Mean reward: -20.05\n",
      "***************************\n",
      "Episode 702 finished (timesteps: 665164/10000000)\n",
      "Epsilon: 0.44, Episode reward: -16.0, Mean reward: -20.00\n",
      "***************************\n",
      "Episode 703 finished (timesteps: 666508/10000000)\n",
      "Epsilon: 0.44, Episode reward: -18.0, Mean reward: -19.97\n",
      "***************************\n",
      "Episode 704 finished (timesteps: 667655/10000000)\n",
      "Epsilon: 0.44, Episode reward: -19.0, Mean reward: -19.95\n",
      "***************************\n",
      "Episode 705 finished (timesteps: 669004/10000000)\n",
      "Epsilon: 0.44, Episode reward: -20.0, Mean reward: -19.95\n",
      "***************************\n",
      "Episode 706 finished (timesteps: 670102/10000000)\n",
      "Epsilon: 0.44, Episode reward: -20.0, Mean reward: -19.96\n",
      "***************************\n",
      "Episode 707 finished (timesteps: 671354/10000000)\n",
      "Epsilon: 0.43, Episode reward: -19.0, Mean reward: -19.96\n",
      "***************************\n",
      "Episode 708 finished (timesteps: 672448/10000000)\n",
      "Epsilon: 0.43, Episode reward: -20.0, Mean reward: -19.95\n",
      "***************************\n",
      "Episode 709 finished (timesteps: 674004/10000000)\n",
      "Epsilon: 0.43, Episode reward: -20.0, Mean reward: -19.94\n",
      "***************************\n",
      "Episode 710 finished (timesteps: 674947/10000000)\n",
      "Epsilon: 0.43, Episode reward: -21.0, Mean reward: -19.94\n",
      "***************************\n",
      "Episode 711 finished (timesteps: 675993/10000000)\n",
      "Epsilon: 0.43, Episode reward: -20.0, Mean reward: -19.93\n",
      "***************************\n",
      "Episode 712 finished (timesteps: 677075/10000000)\n",
      "Epsilon: 0.43, Episode reward: -20.0, Mean reward: -19.92\n",
      "***************************\n",
      "Episode 713 finished (timesteps: 678325/10000000)\n",
      "Epsilon: 0.43, Episode reward: -19.0, Mean reward: -19.90\n",
      "***************************\n",
      "Episode 714 finished (timesteps: 679556/10000000)\n",
      "Epsilon: 0.43, Episode reward: -21.0, Mean reward: -19.91\n",
      "***************************\n",
      "Episode 715 finished (timesteps: 680835/10000000)\n",
      "Epsilon: 0.42, Episode reward: -20.0, Mean reward: -19.90\n",
      "***************************\n",
      "Episode 716 finished (timesteps: 682035/10000000)\n",
      "Epsilon: 0.42, Episode reward: -19.0, Mean reward: -19.89\n",
      "***************************\n",
      "Episode 717 finished (timesteps: 683320/10000000)\n",
      "Epsilon: 0.42, Episode reward: -18.0, Mean reward: -19.87\n",
      "***************************\n",
      "Episode 718 finished (timesteps: 684634/10000000)\n",
      "Epsilon: 0.42, Episode reward: -17.0, Mean reward: -19.84\n",
      "***************************\n",
      "Episode 719 finished (timesteps: 685646/10000000)\n",
      "Epsilon: 0.42, Episode reward: -20.0, Mean reward: -19.84\n",
      "***************************\n",
      "Episode 720 finished (timesteps: 686574/10000000)\n",
      "Epsilon: 0.42, Episode reward: -21.0, Mean reward: -19.86\n",
      "***************************\n",
      "Episode 721 finished (timesteps: 687842/10000000)\n",
      "Epsilon: 0.42, Episode reward: -19.0, Mean reward: -19.86\n",
      "***************************\n",
      "Episode 722 finished (timesteps: 689176/10000000)\n",
      "Epsilon: 0.42, Episode reward: -19.0, Mean reward: -19.84\n",
      "***************************\n",
      "Episode 723 finished (timesteps: 690265/10000000)\n",
      "Epsilon: 0.42, Episode reward: -20.0, Mean reward: -19.85\n",
      "***************************\n",
      "Episode 724 finished (timesteps: 691515/10000000)\n",
      "Epsilon: 0.41, Episode reward: -18.0, Mean reward: -19.82\n",
      "***************************\n",
      "Episode 725 finished (timesteps: 693169/10000000)\n",
      "Epsilon: 0.41, Episode reward: -17.0, Mean reward: -19.79\n",
      "***************************\n",
      "Episode 726 finished (timesteps: 694377/10000000)\n",
      "Epsilon: 0.41, Episode reward: -19.0, Mean reward: -19.77\n",
      "***************************\n",
      "Episode 727 finished (timesteps: 695785/10000000)\n",
      "Epsilon: 0.41, Episode reward: -19.0, Mean reward: -19.78\n",
      "***************************\n",
      "Episode 728 finished (timesteps: 696912/10000000)\n",
      "Epsilon: 0.41, Episode reward: -21.0, Mean reward: -19.78\n",
      "***************************\n",
      "Episode 729 finished (timesteps: 697915/10000000)\n",
      "Epsilon: 0.41, Episode reward: -20.0, Mean reward: -19.79\n",
      "***************************\n",
      "Episode 730 finished (timesteps: 699065/10000000)\n",
      "Epsilon: 0.41, Episode reward: -19.0, Mean reward: -19.78\n",
      "***************************\n",
      "Episode 731 finished (timesteps: 700205/10000000)\n",
      "Epsilon: 0.41, Episode reward: -20.0, Mean reward: -19.78\n",
      "***************************\n",
      "Episode 732 finished (timesteps: 701239/10000000)\n",
      "Epsilon: 0.40, Episode reward: -20.0, Mean reward: -19.77\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 733 finished (timesteps: 702352/10000000)\n",
      "Epsilon: 0.40, Episode reward: -19.0, Mean reward: -19.75\n",
      "***************************\n",
      "Episode 734 finished (timesteps: 703336/10000000)\n",
      "Epsilon: 0.40, Episode reward: -20.0, Mean reward: -19.76\n",
      "***************************\n",
      "Episode 735 finished (timesteps: 704507/10000000)\n",
      "Epsilon: 0.40, Episode reward: -19.0, Mean reward: -19.74\n",
      "***************************\n",
      "Episode 736 finished (timesteps: 705466/10000000)\n",
      "Epsilon: 0.40, Episode reward: -20.0, Mean reward: -19.75\n",
      "***************************\n",
      "Episode 737 finished (timesteps: 706686/10000000)\n",
      "Epsilon: 0.40, Episode reward: -20.0, Mean reward: -19.75\n",
      "***************************\n",
      "Episode 738 finished (timesteps: 708098/10000000)\n",
      "Epsilon: 0.40, Episode reward: -17.0, Mean reward: -19.71\n",
      "***************************\n",
      "Episode 739 finished (timesteps: 709365/10000000)\n",
      "Epsilon: 0.40, Episode reward: -18.0, Mean reward: -19.68\n",
      "***************************\n",
      "Episode 740 finished (timesteps: 710526/10000000)\n",
      "Epsilon: 0.40, Episode reward: -20.0, Mean reward: -19.67\n",
      "***************************\n",
      "Episode 741 finished (timesteps: 711437/10000000)\n",
      "Epsilon: 0.39, Episode reward: -21.0, Mean reward: -19.69\n",
      "***************************\n",
      "Episode 742 finished (timesteps: 712842/10000000)\n",
      "Epsilon: 0.39, Episode reward: -19.0, Mean reward: -19.67\n",
      "***************************\n",
      "Episode 743 finished (timesteps: 713877/10000000)\n",
      "Epsilon: 0.39, Episode reward: -20.0, Mean reward: -19.67\n",
      "***************************\n",
      "Episode 744 finished (timesteps: 714997/10000000)\n",
      "Epsilon: 0.39, Episode reward: -18.0, Mean reward: -19.66\n",
      "***************************\n",
      "Episode 745 finished (timesteps: 716232/10000000)\n",
      "Epsilon: 0.39, Episode reward: -19.0, Mean reward: -19.65\n",
      "***************************\n",
      "Episode 746 finished (timesteps: 717624/10000000)\n",
      "Epsilon: 0.39, Episode reward: -18.0, Mean reward: -19.64\n",
      "***************************\n",
      "Episode 747 finished (timesteps: 718905/10000000)\n",
      "Epsilon: 0.39, Episode reward: -17.0, Mean reward: -19.61\n",
      "***************************\n",
      "Episode 748 finished (timesteps: 720033/10000000)\n",
      "Epsilon: 0.39, Episode reward: -20.0, Mean reward: -19.61\n",
      "***************************\n",
      "Episode 749 finished (timesteps: 721398/10000000)\n",
      "Epsilon: 0.38, Episode reward: -19.0, Mean reward: -19.59\n",
      "***************************\n",
      "Episode 750 finished (timesteps: 722609/10000000)\n",
      "Epsilon: 0.38, Episode reward: -19.0, Mean reward: -19.58\n",
      "***************************\n",
      "Episode 751 finished (timesteps: 723927/10000000)\n",
      "Epsilon: 0.38, Episode reward: -21.0, Mean reward: -19.59\n",
      "***************************\n",
      "Episode 752 finished (timesteps: 725374/10000000)\n",
      "Epsilon: 0.38, Episode reward: -19.0, Mean reward: -19.59\n",
      "***************************\n",
      "Episode 753 finished (timesteps: 726756/10000000)\n",
      "Epsilon: 0.38, Episode reward: -17.0, Mean reward: -19.58\n",
      "***************************\n",
      "Episode 754 finished (timesteps: 728167/10000000)\n",
      "Epsilon: 0.38, Episode reward: -20.0, Mean reward: -19.58\n",
      "***************************\n",
      "Episode 755 finished (timesteps: 729724/10000000)\n",
      "Epsilon: 0.38, Episode reward: -21.0, Mean reward: -19.60\n",
      "***************************\n",
      "Episode 756 finished (timesteps: 730922/10000000)\n",
      "Epsilon: 0.38, Episode reward: -21.0, Mean reward: -19.61\n",
      "***************************\n",
      "Episode 757 finished (timesteps: 732200/10000000)\n",
      "Epsilon: 0.37, Episode reward: -18.0, Mean reward: -19.59\n",
      "***************************\n",
      "Episode 758 finished (timesteps: 733173/10000000)\n",
      "Epsilon: 0.37, Episode reward: -21.0, Mean reward: -19.62\n",
      "***************************\n",
      "Episode 759 finished (timesteps: 734272/10000000)\n",
      "Epsilon: 0.37, Episode reward: -20.0, Mean reward: -19.61\n",
      "***************************\n",
      "Episode 760 finished (timesteps: 735523/10000000)\n",
      "Epsilon: 0.37, Episode reward: -20.0, Mean reward: -19.61\n",
      "***************************\n",
      "Episode 761 finished (timesteps: 736668/10000000)\n",
      "Epsilon: 0.37, Episode reward: -19.0, Mean reward: -19.60\n",
      "***************************\n",
      "Episode 762 finished (timesteps: 737708/10000000)\n",
      "Epsilon: 0.37, Episode reward: -20.0, Mean reward: -19.61\n",
      "***************************\n",
      "Episode 763 finished (timesteps: 739193/10000000)\n",
      "Epsilon: 0.37, Episode reward: -18.0, Mean reward: -19.60\n",
      "***************************\n",
      "Episode 764 finished (timesteps: 740709/10000000)\n",
      "Epsilon: 0.37, Episode reward: -19.0, Mean reward: -19.59\n",
      "***************************\n",
      "Episode 765 finished (timesteps: 741889/10000000)\n",
      "Epsilon: 0.36, Episode reward: -18.0, Mean reward: -19.57\n",
      "***************************\n",
      "Episode 766 finished (timesteps: 743122/10000000)\n",
      "Epsilon: 0.36, Episode reward: -21.0, Mean reward: -19.57\n",
      "***************************\n",
      "Episode 767 finished (timesteps: 744194/10000000)\n",
      "Epsilon: 0.36, Episode reward: -19.0, Mean reward: -19.56\n",
      "***************************\n",
      "Episode 768 finished (timesteps: 745179/10000000)\n",
      "Epsilon: 0.36, Episode reward: -20.0, Mean reward: -19.56\n",
      "***************************\n",
      "Episode 769 finished (timesteps: 746222/10000000)\n",
      "Epsilon: 0.36, Episode reward: -19.0, Mean reward: -19.56\n",
      "***************************\n",
      "Episode 770 finished (timesteps: 747555/10000000)\n",
      "Epsilon: 0.36, Episode reward: -19.0, Mean reward: -19.54\n",
      "***************************\n",
      "Episode 771 finished (timesteps: 748526/10000000)\n",
      "Epsilon: 0.36, Episode reward: -21.0, Mean reward: -19.55\n",
      "***************************\n",
      "Episode 772 finished (timesteps: 750022/10000000)\n",
      "Epsilon: 0.36, Episode reward: -19.0, Mean reward: -19.53\n",
      "***************************\n",
      "Episode 773 finished (timesteps: 751146/10000000)\n",
      "Epsilon: 0.36, Episode reward: -21.0, Mean reward: -19.53\n",
      "***************************\n",
      "Episode 774 finished (timesteps: 752229/10000000)\n",
      "Epsilon: 0.35, Episode reward: -21.0, Mean reward: -19.56\n",
      "***************************\n",
      "Episode 775 finished (timesteps: 753776/10000000)\n",
      "Epsilon: 0.35, Episode reward: -18.0, Mean reward: -19.53\n",
      "***************************\n",
      "Episode 776 finished (timesteps: 754996/10000000)\n",
      "Epsilon: 0.35, Episode reward: -20.0, Mean reward: -19.55\n",
      "***************************\n",
      "Episode 777 finished (timesteps: 756258/10000000)\n",
      "Epsilon: 0.35, Episode reward: -19.0, Mean reward: -19.55\n",
      "***************************\n",
      "Episode 778 finished (timesteps: 757831/10000000)\n",
      "Epsilon: 0.35, Episode reward: -18.0, Mean reward: -19.52\n",
      "***************************\n",
      "Episode 779 finished (timesteps: 759140/10000000)\n",
      "Epsilon: 0.35, Episode reward: -21.0, Mean reward: -19.53\n",
      "***************************\n",
      "Episode 780 finished (timesteps: 760323/10000000)\n",
      "Epsilon: 0.35, Episode reward: -18.0, Mean reward: -19.50\n",
      "***************************\n",
      "Episode 781 finished (timesteps: 761274/10000000)\n",
      "Epsilon: 0.35, Episode reward: -20.0, Mean reward: -19.51\n",
      "***************************\n",
      "Episode 782 finished (timesteps: 762575/10000000)\n",
      "Epsilon: 0.34, Episode reward: -19.0, Mean reward: -19.51\n",
      "***************************\n",
      "Episode 783 finished (timesteps: 763822/10000000)\n",
      "Epsilon: 0.34, Episode reward: -19.0, Mean reward: -19.50\n",
      "***************************\n",
      "Episode 784 finished (timesteps: 765341/10000000)\n",
      "Epsilon: 0.34, Episode reward: -16.0, Mean reward: -19.47\n",
      "***************************\n",
      "Episode 785 finished (timesteps: 766674/10000000)\n",
      "Epsilon: 0.34, Episode reward: -19.0, Mean reward: -19.45\n",
      "***************************\n",
      "Episode 786 finished (timesteps: 768087/10000000)\n",
      "Epsilon: 0.34, Episode reward: -18.0, Mean reward: -19.42\n",
      "***************************\n",
      "Episode 787 finished (timesteps: 769111/10000000)\n",
      "Epsilon: 0.34, Episode reward: -20.0, Mean reward: -19.42\n",
      "***************************\n",
      "Episode 788 finished (timesteps: 770087/10000000)\n",
      "Epsilon: 0.34, Episode reward: -21.0, Mean reward: -19.43\n",
      "***************************\n",
      "Episode 789 finished (timesteps: 771346/10000000)\n",
      "Epsilon: 0.34, Episode reward: -21.0, Mean reward: -19.45\n",
      "***************************\n",
      "Episode 790 finished (timesteps: 772320/10000000)\n",
      "Epsilon: 0.33, Episode reward: -21.0, Mean reward: -19.45\n",
      "***************************\n",
      "Episode 791 finished (timesteps: 773491/10000000)\n",
      "Epsilon: 0.33, Episode reward: -18.0, Mean reward: -19.43\n",
      "***************************\n",
      "Episode 792 finished (timesteps: 774896/10000000)\n",
      "Epsilon: 0.33, Episode reward: -20.0, Mean reward: -19.43\n",
      "***************************\n",
      "Episode 793 finished (timesteps: 776259/10000000)\n",
      "Epsilon: 0.33, Episode reward: -21.0, Mean reward: -19.43\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 794 finished (timesteps: 777493/10000000)\n",
      "Epsilon: 0.33, Episode reward: -17.0, Mean reward: -19.40\n",
      "***************************\n",
      "Episode 795 finished (timesteps: 778795/10000000)\n",
      "Epsilon: 0.33, Episode reward: -20.0, Mean reward: -19.40\n",
      "***************************\n",
      "Episode 796 finished (timesteps: 780110/10000000)\n",
      "Epsilon: 0.33, Episode reward: -19.0, Mean reward: -19.38\n",
      "***************************\n",
      "Episode 797 finished (timesteps: 781581/10000000)\n",
      "Epsilon: 0.33, Episode reward: -17.0, Mean reward: -19.34\n",
      "***************************\n",
      "Episode 798 finished (timesteps: 782728/10000000)\n",
      "Epsilon: 0.32, Episode reward: -18.0, Mean reward: -19.34\n",
      "***************************\n",
      "Episode 799 finished (timesteps: 784206/10000000)\n",
      "Epsilon: 0.32, Episode reward: -17.0, Mean reward: -19.30\n",
      "***************************\n",
      "Episode 800 finished (timesteps: 785443/10000000)\n",
      "Epsilon: 0.32, Episode reward: -20.0, Mean reward: -19.30\n",
      "***************************\n",
      "Episode 801 finished (timesteps: 786680/10000000)\n",
      "Epsilon: 0.32, Episode reward: -20.0, Mean reward: -19.29\n",
      "***************************\n",
      "Episode 802 finished (timesteps: 788405/10000000)\n",
      "Epsilon: 0.32, Episode reward: -16.0, Mean reward: -19.29\n",
      "***************************\n",
      "Episode 803 finished (timesteps: 790088/10000000)\n",
      "Epsilon: 0.32, Episode reward: -18.0, Mean reward: -19.29\n",
      "***************************\n",
      "Episode 804 finished (timesteps: 791647/10000000)\n",
      "Epsilon: 0.32, Episode reward: -17.0, Mean reward: -19.27\n",
      "***************************\n",
      "Episode 805 finished (timesteps: 793023/10000000)\n",
      "Epsilon: 0.31, Episode reward: -18.0, Mean reward: -19.25\n",
      "***************************\n",
      "Episode 806 finished (timesteps: 794426/10000000)\n",
      "Epsilon: 0.31, Episode reward: -20.0, Mean reward: -19.25\n",
      "***************************\n",
      "Episode 807 finished (timesteps: 795877/10000000)\n",
      "Epsilon: 0.31, Episode reward: -17.0, Mean reward: -19.23\n",
      "***************************\n",
      "Episode 808 finished (timesteps: 797077/10000000)\n",
      "Epsilon: 0.31, Episode reward: -20.0, Mean reward: -19.23\n",
      "***************************\n",
      "Episode 809 finished (timesteps: 798098/10000000)\n",
      "Epsilon: 0.31, Episode reward: -20.0, Mean reward: -19.23\n",
      "***************************\n",
      "Episode 810 finished (timesteps: 799266/10000000)\n",
      "Epsilon: 0.31, Episode reward: -21.0, Mean reward: -19.23\n",
      "***************************\n",
      "Episode 811 finished (timesteps: 800620/10000000)\n",
      "Epsilon: 0.31, Episode reward: -16.0, Mean reward: -19.19\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-800000.mp4.\n",
      "Moviepy - Writing video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-800000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-800000.mp4\n",
      "Episode 812 finished (timesteps: 801923/10000000)\n",
      "Epsilon: 0.31, Episode reward: -20.0, Mean reward: -19.19\n",
      "***************************\n",
      "Episode 813 finished (timesteps: 803172/10000000)\n",
      "Epsilon: 0.30, Episode reward: -20.0, Mean reward: -19.20\n",
      "***************************\n",
      "Episode 814 finished (timesteps: 804452/10000000)\n",
      "Epsilon: 0.30, Episode reward: -19.0, Mean reward: -19.18\n",
      "***************************\n",
      "Episode 815 finished (timesteps: 805489/10000000)\n",
      "Epsilon: 0.30, Episode reward: -21.0, Mean reward: -19.19\n",
      "***************************\n",
      "Episode 816 finished (timesteps: 806732/10000000)\n",
      "Epsilon: 0.30, Episode reward: -19.0, Mean reward: -19.19\n",
      "***************************\n",
      "Episode 817 finished (timesteps: 807929/10000000)\n",
      "Epsilon: 0.30, Episode reward: -18.0, Mean reward: -19.19\n",
      "***************************\n",
      "Episode 818 finished (timesteps: 809016/10000000)\n",
      "Epsilon: 0.30, Episode reward: -20.0, Mean reward: -19.22\n",
      "***************************\n",
      "Episode 819 finished (timesteps: 810481/10000000)\n",
      "Epsilon: 0.30, Episode reward: -17.0, Mean reward: -19.19\n",
      "***************************\n",
      "Episode 820 finished (timesteps: 811876/10000000)\n",
      "Epsilon: 0.30, Episode reward: -20.0, Mean reward: -19.18\n",
      "***************************\n",
      "Episode 821 finished (timesteps: 813162/10000000)\n",
      "Epsilon: 0.29, Episode reward: -20.0, Mean reward: -19.19\n",
      "***************************\n",
      "Episode 822 finished (timesteps: 814221/10000000)\n",
      "Epsilon: 0.29, Episode reward: -21.0, Mean reward: -19.21\n",
      "***************************\n",
      "Episode 823 finished (timesteps: 815979/10000000)\n",
      "Epsilon: 0.29, Episode reward: -17.0, Mean reward: -19.18\n",
      "***************************\n",
      "Episode 824 finished (timesteps: 817265/10000000)\n",
      "Epsilon: 0.29, Episode reward: -19.0, Mean reward: -19.19\n",
      "***************************\n",
      "Episode 825 finished (timesteps: 818678/10000000)\n",
      "Epsilon: 0.29, Episode reward: -17.0, Mean reward: -19.19\n",
      "***************************\n",
      "Episode 826 finished (timesteps: 820312/10000000)\n",
      "Epsilon: 0.29, Episode reward: -16.0, Mean reward: -19.16\n",
      "***************************\n",
      "Episode 827 finished (timesteps: 822138/10000000)\n",
      "Epsilon: 0.29, Episode reward: -14.0, Mean reward: -19.11\n",
      "***************************\n",
      "Episode 828 finished (timesteps: 823483/10000000)\n",
      "Epsilon: 0.28, Episode reward: -18.0, Mean reward: -19.08\n",
      "***************************\n",
      "Episode 829 finished (timesteps: 825153/10000000)\n",
      "Epsilon: 0.28, Episode reward: -16.0, Mean reward: -19.04\n",
      "***************************\n",
      "Episode 830 finished (timesteps: 826673/10000000)\n",
      "Epsilon: 0.28, Episode reward: -18.0, Mean reward: -19.03\n",
      "***************************\n",
      "Episode 831 finished (timesteps: 828039/10000000)\n",
      "Epsilon: 0.28, Episode reward: -18.0, Mean reward: -19.01\n",
      "***************************\n",
      "Episode 832 finished (timesteps: 829239/10000000)\n",
      "Epsilon: 0.28, Episode reward: -20.0, Mean reward: -19.01\n",
      "***************************\n",
      "Episode 833 finished (timesteps: 830647/10000000)\n",
      "Epsilon: 0.28, Episode reward: -20.0, Mean reward: -19.02\n",
      "***************************\n",
      "Episode 834 finished (timesteps: 832178/10000000)\n",
      "Epsilon: 0.28, Episode reward: -18.0, Mean reward: -19.00\n",
      "***************************\n",
      "Episode 835 finished (timesteps: 833189/10000000)\n",
      "Epsilon: 0.27, Episode reward: -21.0, Mean reward: -19.02\n",
      "***************************\n",
      "Episode 836 finished (timesteps: 835012/10000000)\n",
      "Epsilon: 0.27, Episode reward: -14.0, Mean reward: -18.96\n",
      "***************************\n",
      "Episode 837 finished (timesteps: 836184/10000000)\n",
      "Epsilon: 0.27, Episode reward: -20.0, Mean reward: -18.96\n",
      "***************************\n",
      "Episode 838 finished (timesteps: 838011/10000000)\n",
      "Epsilon: 0.27, Episode reward: -16.0, Mean reward: -18.95\n",
      "***************************\n",
      "Episode 839 finished (timesteps: 839668/10000000)\n",
      "Epsilon: 0.27, Episode reward: -15.0, Mean reward: -18.92\n",
      "***************************\n",
      "Episode 840 finished (timesteps: 841030/10000000)\n",
      "Epsilon: 0.27, Episode reward: -18.0, Mean reward: -18.90\n",
      "***************************\n",
      "Episode 841 finished (timesteps: 842286/10000000)\n",
      "Epsilon: 0.27, Episode reward: -19.0, Mean reward: -18.88\n",
      "***************************\n",
      "Episode 842 finished (timesteps: 843827/10000000)\n",
      "Epsilon: 0.26, Episode reward: -18.0, Mean reward: -18.87\n",
      "***************************\n",
      "Episode 843 finished (timesteps: 845444/10000000)\n",
      "Epsilon: 0.26, Episode reward: -18.0, Mean reward: -18.85\n",
      "***************************\n",
      "Episode 844 finished (timesteps: 847042/10000000)\n",
      "Epsilon: 0.26, Episode reward: -18.0, Mean reward: -18.85\n",
      "***************************\n",
      "Episode 845 finished (timesteps: 848450/10000000)\n",
      "Epsilon: 0.26, Episode reward: -20.0, Mean reward: -18.86\n",
      "***************************\n",
      "Episode 846 finished (timesteps: 849955/10000000)\n",
      "Epsilon: 0.26, Episode reward: -20.0, Mean reward: -18.88\n",
      "***************************\n",
      "Episode 847 finished (timesteps: 851550/10000000)\n",
      "Epsilon: 0.26, Episode reward: -15.0, Mean reward: -18.86\n",
      "***************************\n",
      "Episode 848 finished (timesteps: 853097/10000000)\n",
      "Epsilon: 0.25, Episode reward: -18.0, Mean reward: -18.84\n",
      "***************************\n",
      "Episode 849 finished (timesteps: 854710/10000000)\n",
      "Epsilon: 0.25, Episode reward: -15.0, Mean reward: -18.80\n",
      "***************************\n",
      "Episode 850 finished (timesteps: 856114/10000000)\n",
      "Epsilon: 0.25, Episode reward: -17.0, Mean reward: -18.78\n",
      "***************************\n",
      "Episode 851 finished (timesteps: 857712/10000000)\n",
      "Epsilon: 0.25, Episode reward: -16.0, Mean reward: -18.73\n",
      "***************************\n",
      "Episode 852 finished (timesteps: 859155/10000000)\n",
      "Epsilon: 0.25, Episode reward: -18.0, Mean reward: -18.72\n",
      "***************************\n",
      "Episode 853 finished (timesteps: 860470/10000000)\n",
      "Epsilon: 0.25, Episode reward: -19.0, Mean reward: -18.74\n",
      "***************************\n",
      "Episode 854 finished (timesteps: 861655/10000000)\n",
      "Epsilon: 0.25, Episode reward: -19.0, Mean reward: -18.73\n",
      "***************************\n",
      "Episode 855 finished (timesteps: 862949/10000000)\n",
      "Epsilon: 0.24, Episode reward: -17.0, Mean reward: -18.69\n",
      "***************************\n",
      "Episode 856 finished (timesteps: 864146/10000000)\n",
      "Epsilon: 0.24, Episode reward: -20.0, Mean reward: -18.68\n",
      "***************************\n",
      "Episode 857 finished (timesteps: 865865/10000000)\n",
      "Epsilon: 0.24, Episode reward: -15.0, Mean reward: -18.65\n",
      "***************************\n",
      "Episode 858 finished (timesteps: 867340/10000000)\n",
      "Epsilon: 0.24, Episode reward: -19.0, Mean reward: -18.63\n",
      "***************************\n",
      "Episode 859 finished (timesteps: 869050/10000000)\n",
      "Epsilon: 0.24, Episode reward: -15.0, Mean reward: -18.58\n",
      "***************************\n",
      "Episode 860 finished (timesteps: 870415/10000000)\n",
      "Epsilon: 0.24, Episode reward: -17.0, Mean reward: -18.55\n",
      "***************************\n",
      "Episode 861 finished (timesteps: 871753/10000000)\n",
      "Epsilon: 0.24, Episode reward: -17.0, Mean reward: -18.53\n",
      "***************************\n",
      "Episode 862 finished (timesteps: 873032/10000000)\n",
      "Epsilon: 0.23, Episode reward: -19.0, Mean reward: -18.52\n",
      "***************************\n",
      "Episode 863 finished (timesteps: 874189/10000000)\n",
      "Epsilon: 0.23, Episode reward: -19.0, Mean reward: -18.53\n",
      "***************************\n",
      "Episode 864 finished (timesteps: 875519/10000000)\n",
      "Epsilon: 0.23, Episode reward: -18.0, Mean reward: -18.52\n",
      "***************************\n",
      "Episode 865 finished (timesteps: 877232/10000000)\n",
      "Epsilon: 0.23, Episode reward: -14.0, Mean reward: -18.48\n",
      "***************************\n",
      "Episode 866 finished (timesteps: 878737/10000000)\n",
      "Epsilon: 0.23, Episode reward: -17.0, Mean reward: -18.44\n",
      "***************************\n",
      "Episode 867 finished (timesteps: 880714/10000000)\n",
      "Epsilon: 0.23, Episode reward: -13.0, Mean reward: -18.38\n",
      "***************************\n",
      "Episode 868 finished (timesteps: 882065/10000000)\n",
      "Epsilon: 0.23, Episode reward: -20.0, Mean reward: -18.38\n",
      "***************************\n",
      "Episode 869 finished (timesteps: 883238/10000000)\n",
      "Epsilon: 0.22, Episode reward: -19.0, Mean reward: -18.38\n",
      "***************************\n",
      "Episode 870 finished (timesteps: 884385/10000000)\n",
      "Epsilon: 0.22, Episode reward: -21.0, Mean reward: -18.40\n",
      "***************************\n",
      "Episode 871 finished (timesteps: 885546/10000000)\n",
      "Epsilon: 0.22, Episode reward: -19.0, Mean reward: -18.38\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 872 finished (timesteps: 886941/10000000)\n",
      "Epsilon: 0.22, Episode reward: -17.0, Mean reward: -18.36\n",
      "***************************\n",
      "Episode 873 finished (timesteps: 888157/10000000)\n",
      "Epsilon: 0.22, Episode reward: -20.0, Mean reward: -18.35\n",
      "***************************\n",
      "Episode 874 finished (timesteps: 889704/10000000)\n",
      "Epsilon: 0.22, Episode reward: -15.0, Mean reward: -18.29\n",
      "***************************\n",
      "Episode 875 finished (timesteps: 890978/10000000)\n",
      "Epsilon: 0.22, Episode reward: -19.0, Mean reward: -18.30\n",
      "***************************\n",
      "Episode 876 finished (timesteps: 892635/10000000)\n",
      "Epsilon: 0.22, Episode reward: -19.0, Mean reward: -18.29\n",
      "***************************\n",
      "Episode 877 finished (timesteps: 894068/10000000)\n",
      "Epsilon: 0.21, Episode reward: -17.0, Mean reward: -18.27\n",
      "***************************\n",
      "Episode 878 finished (timesteps: 895583/10000000)\n",
      "Epsilon: 0.21, Episode reward: -16.0, Mean reward: -18.25\n",
      "***************************\n",
      "Episode 879 finished (timesteps: 896688/10000000)\n",
      "Epsilon: 0.21, Episode reward: -21.0, Mean reward: -18.25\n",
      "***************************\n",
      "Episode 880 finished (timesteps: 898116/10000000)\n",
      "Epsilon: 0.21, Episode reward: -20.0, Mean reward: -18.27\n",
      "***************************\n",
      "Episode 881 finished (timesteps: 899970/10000000)\n",
      "Epsilon: 0.21, Episode reward: -16.0, Mean reward: -18.23\n",
      "***************************\n",
      "Episode 882 finished (timesteps: 901394/10000000)\n",
      "Epsilon: 0.21, Episode reward: -18.0, Mean reward: -18.22\n",
      "***************************\n",
      "Episode 883 finished (timesteps: 903022/10000000)\n",
      "Epsilon: 0.21, Episode reward: -15.0, Mean reward: -18.18\n",
      "***************************\n",
      "Episode 884 finished (timesteps: 904775/10000000)\n",
      "Epsilon: 0.20, Episode reward: -16.0, Mean reward: -18.18\n",
      "***************************\n",
      "Episode 885 finished (timesteps: 906290/10000000)\n",
      "Epsilon: 0.20, Episode reward: -16.0, Mean reward: -18.15\n",
      "***************************\n",
      "Episode 886 finished (timesteps: 907870/10000000)\n",
      "Epsilon: 0.20, Episode reward: -18.0, Mean reward: -18.15\n",
      "***************************\n",
      "Episode 887 finished (timesteps: 909132/10000000)\n",
      "Epsilon: 0.20, Episode reward: -20.0, Mean reward: -18.15\n",
      "***************************\n",
      "Episode 888 finished (timesteps: 910538/10000000)\n",
      "Epsilon: 0.20, Episode reward: -17.0, Mean reward: -18.11\n",
      "***************************\n",
      "Episode 889 finished (timesteps: 912200/10000000)\n",
      "Epsilon: 0.20, Episode reward: -18.0, Mean reward: -18.08\n",
      "***************************\n",
      "Episode 890 finished (timesteps: 913410/10000000)\n",
      "Epsilon: 0.19, Episode reward: -20.0, Mean reward: -18.07\n",
      "***************************\n",
      "Episode 891 finished (timesteps: 915398/10000000)\n",
      "Epsilon: 0.19, Episode reward: -18.0, Mean reward: -18.07\n",
      "***************************\n",
      "Episode 892 finished (timesteps: 917129/10000000)\n",
      "Epsilon: 0.19, Episode reward: -20.0, Mean reward: -18.07\n",
      "***************************\n",
      "Episode 893 finished (timesteps: 918503/10000000)\n",
      "Epsilon: 0.19, Episode reward: -19.0, Mean reward: -18.05\n",
      "***************************\n",
      "Episode 894 finished (timesteps: 919925/10000000)\n",
      "Epsilon: 0.19, Episode reward: -19.0, Mean reward: -18.07\n",
      "***************************\n",
      "Episode 895 finished (timesteps: 921766/10000000)\n",
      "Epsilon: 0.19, Episode reward: -17.0, Mean reward: -18.04\n",
      "***************************\n",
      "Episode 896 finished (timesteps: 923422/10000000)\n",
      "Epsilon: 0.18, Episode reward: -20.0, Mean reward: -18.05\n",
      "***************************\n",
      "Episode 897 finished (timesteps: 924837/10000000)\n",
      "Epsilon: 0.18, Episode reward: -19.0, Mean reward: -18.07\n",
      "***************************\n",
      "Episode 898 finished (timesteps: 926617/10000000)\n",
      "Epsilon: 0.18, Episode reward: -17.0, Mean reward: -18.06\n",
      "***************************\n",
      "Episode 899 finished (timesteps: 928272/10000000)\n",
      "Epsilon: 0.18, Episode reward: -17.0, Mean reward: -18.06\n",
      "***************************\n",
      "Episode 900 finished (timesteps: 929844/10000000)\n",
      "Epsilon: 0.18, Episode reward: -20.0, Mean reward: -18.06\n",
      "***************************\n",
      "Episode 901 finished (timesteps: 931801/10000000)\n",
      "Epsilon: 0.18, Episode reward: -17.0, Mean reward: -18.03\n",
      "***************************\n",
      "Episode 902 finished (timesteps: 933385/10000000)\n",
      "Epsilon: 0.17, Episode reward: -17.0, Mean reward: -18.04\n",
      "***************************\n",
      "Episode 903 finished (timesteps: 935049/10000000)\n",
      "Epsilon: 0.17, Episode reward: -19.0, Mean reward: -18.05\n",
      "***************************\n",
      "Episode 904 finished (timesteps: 936693/10000000)\n",
      "Epsilon: 0.17, Episode reward: -17.0, Mean reward: -18.05\n",
      "***************************\n",
      "Episode 905 finished (timesteps: 938233/10000000)\n",
      "Epsilon: 0.17, Episode reward: -19.0, Mean reward: -18.06\n",
      "***************************\n",
      "Episode 906 finished (timesteps: 939318/10000000)\n",
      "Epsilon: 0.17, Episode reward: -21.0, Mean reward: -18.07\n",
      "***************************\n",
      "Episode 907 finished (timesteps: 940944/10000000)\n",
      "Epsilon: 0.17, Episode reward: -17.0, Mean reward: -18.07\n",
      "***************************\n",
      "Episode 908 finished (timesteps: 943094/10000000)\n",
      "Epsilon: 0.17, Episode reward: -15.0, Mean reward: -18.02\n",
      "***************************\n",
      "Episode 909 finished (timesteps: 945005/10000000)\n",
      "Epsilon: 0.16, Episode reward: -14.0, Mean reward: -17.96\n",
      "***************************\n",
      "Episode 910 finished (timesteps: 946457/10000000)\n",
      "Epsilon: 0.16, Episode reward: -17.0, Mean reward: -17.92\n",
      "***************************\n",
      "Episode 911 finished (timesteps: 948200/10000000)\n",
      "Epsilon: 0.16, Episode reward: -16.0, Mean reward: -17.92\n",
      "***************************\n",
      "Episode 912 finished (timesteps: 949395/10000000)\n",
      "Epsilon: 0.16, Episode reward: -20.0, Mean reward: -17.92\n",
      "***************************\n",
      "Episode 913 finished (timesteps: 951117/10000000)\n",
      "Epsilon: 0.16, Episode reward: -14.0, Mean reward: -17.86\n",
      "***************************\n",
      "Episode 914 finished (timesteps: 952430/10000000)\n",
      "Epsilon: 0.16, Episode reward: -21.0, Mean reward: -17.88\n",
      "***************************\n",
      "Episode 915 finished (timesteps: 953948/10000000)\n",
      "Epsilon: 0.15, Episode reward: -18.0, Mean reward: -17.85\n",
      "***************************\n",
      "Episode 916 finished (timesteps: 955447/10000000)\n",
      "Epsilon: 0.15, Episode reward: -18.0, Mean reward: -17.84\n",
      "***************************\n",
      "Episode 917 finished (timesteps: 957226/10000000)\n",
      "Epsilon: 0.15, Episode reward: -15.0, Mean reward: -17.81\n",
      "***************************\n",
      "Episode 918 finished (timesteps: 958565/10000000)\n",
      "Epsilon: 0.15, Episode reward: -20.0, Mean reward: -17.81\n",
      "***************************\n",
      "Episode 919 finished (timesteps: 960248/10000000)\n",
      "Epsilon: 0.15, Episode reward: -15.0, Mean reward: -17.79\n",
      "***************************\n",
      "Episode 920 finished (timesteps: 962021/10000000)\n",
      "Epsilon: 0.15, Episode reward: -19.0, Mean reward: -17.78\n",
      "***************************\n",
      "Episode 921 finished (timesteps: 963149/10000000)\n",
      "Epsilon: 0.15, Episode reward: -20.0, Mean reward: -17.78\n",
      "***************************\n",
      "Episode 922 finished (timesteps: 964977/10000000)\n",
      "Epsilon: 0.14, Episode reward: -12.0, Mean reward: -17.69\n",
      "***************************\n",
      "Episode 923 finished (timesteps: 966553/10000000)\n",
      "Epsilon: 0.14, Episode reward: -17.0, Mean reward: -17.69\n",
      "***************************\n",
      "Episode 924 finished (timesteps: 967688/10000000)\n",
      "Epsilon: 0.14, Episode reward: -19.0, Mean reward: -17.69\n",
      "***************************\n",
      "Episode 925 finished (timesteps: 969204/10000000)\n",
      "Epsilon: 0.14, Episode reward: -17.0, Mean reward: -17.69\n",
      "***************************\n",
      "Episode 926 finished (timesteps: 970248/10000000)\n",
      "Epsilon: 0.14, Episode reward: -20.0, Mean reward: -17.73\n",
      "***************************\n",
      "Episode 927 finished (timesteps: 971902/10000000)\n",
      "Epsilon: 0.14, Episode reward: -16.0, Mean reward: -17.75\n",
      "***************************\n",
      "Episode 928 finished (timesteps: 973380/10000000)\n",
      "Epsilon: 0.14, Episode reward: -18.0, Mean reward: -17.75\n",
      "***************************\n",
      "Episode 929 finished (timesteps: 974574/10000000)\n",
      "Epsilon: 0.13, Episode reward: -21.0, Mean reward: -17.80\n",
      "***************************\n",
      "Episode 930 finished (timesteps: 975969/10000000)\n",
      "Epsilon: 0.13, Episode reward: -20.0, Mean reward: -17.82\n",
      "***************************\n",
      "Episode 931 finished (timesteps: 977794/10000000)\n",
      "Epsilon: 0.13, Episode reward: -17.0, Mean reward: -17.81\n",
      "***************************\n",
      "Episode 932 finished (timesteps: 978716/10000000)\n",
      "Epsilon: 0.13, Episode reward: -20.0, Mean reward: -17.81\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 933 finished (timesteps: 980314/10000000)\n",
      "Epsilon: 0.13, Episode reward: -17.0, Mean reward: -17.78\n",
      "***************************\n",
      "Episode 934 finished (timesteps: 981831/10000000)\n",
      "Epsilon: 0.13, Episode reward: -19.0, Mean reward: -17.79\n",
      "***************************\n",
      "Episode 935 finished (timesteps: 982922/10000000)\n",
      "Epsilon: 0.13, Episode reward: -20.0, Mean reward: -17.78\n",
      "***************************\n",
      "Episode 936 finished (timesteps: 984621/10000000)\n",
      "Epsilon: 0.12, Episode reward: -16.0, Mean reward: -17.80\n",
      "***************************\n",
      "Episode 937 finished (timesteps: 985949/10000000)\n",
      "Epsilon: 0.12, Episode reward: -20.0, Mean reward: -17.80\n",
      "***************************\n",
      "Episode 938 finished (timesteps: 987261/10000000)\n",
      "Epsilon: 0.12, Episode reward: -19.0, Mean reward: -17.83\n",
      "***************************\n",
      "Episode 939 finished (timesteps: 988186/10000000)\n",
      "Epsilon: 0.12, Episode reward: -21.0, Mean reward: -17.89\n",
      "***************************\n",
      "Episode 940 finished (timesteps: 989616/10000000)\n",
      "Epsilon: 0.12, Episode reward: -18.0, Mean reward: -17.89\n",
      "***************************\n",
      "Episode 941 finished (timesteps: 991509/10000000)\n",
      "Epsilon: 0.12, Episode reward: -13.0, Mean reward: -17.83\n",
      "***************************\n",
      "Episode 942 finished (timesteps: 993208/10000000)\n",
      "Epsilon: 0.12, Episode reward: -17.0, Mean reward: -17.82\n",
      "***************************\n",
      "Episode 943 finished (timesteps: 995027/10000000)\n",
      "Epsilon: 0.11, Episode reward: -14.0, Mean reward: -17.78\n",
      "***************************\n",
      "Episode 944 finished (timesteps: 996247/10000000)\n",
      "Epsilon: 0.11, Episode reward: -20.0, Mean reward: -17.80\n",
      "***************************\n",
      "Episode 945 finished (timesteps: 998195/10000000)\n",
      "Epsilon: 0.11, Episode reward: -16.0, Mean reward: -17.76\n",
      "***************************\n",
      "Episode 946 finished (timesteps: 999839/10000000)\n",
      "Epsilon: 0.11, Episode reward: -19.0, Mean reward: -17.75\n",
      "***************************\n",
      "Episode 947 finished (timesteps: 1001468/10000000)\n",
      "Epsilon: 0.11, Episode reward: -19.0, Mean reward: -17.79\n",
      "***************************\n",
      "Episode 948 finished (timesteps: 1002769/10000000)\n",
      "Epsilon: 0.11, Episode reward: -20.0, Mean reward: -17.81\n",
      "***************************\n",
      "Episode 949 finished (timesteps: 1004801/10000000)\n",
      "Epsilon: 0.10, Episode reward: -17.0, Mean reward: -17.83\n",
      "***************************\n",
      "Episode 950 finished (timesteps: 1006654/10000000)\n",
      "Epsilon: 0.10, Episode reward: -17.0, Mean reward: -17.83\n",
      "***************************\n",
      "Episode 951 finished (timesteps: 1008549/10000000)\n",
      "Epsilon: 0.10, Episode reward: -16.0, Mean reward: -17.83\n",
      "***************************\n",
      "Episode 952 finished (timesteps: 1010230/10000000)\n",
      "Epsilon: 0.10, Episode reward: -20.0, Mean reward: -17.85\n",
      "***************************\n",
      "Episode 953 finished (timesteps: 1012096/10000000)\n",
      "Epsilon: 0.10, Episode reward: -19.0, Mean reward: -17.85\n",
      "***************************\n",
      "Episode 954 finished (timesteps: 1013810/10000000)\n",
      "Epsilon: 0.10, Episode reward: -21.0, Mean reward: -17.87\n",
      "***************************\n",
      "Episode 955 finished (timesteps: 1015571/10000000)\n",
      "Epsilon: 0.09, Episode reward: -14.0, Mean reward: -17.84\n",
      "***************************\n",
      "Episode 956 finished (timesteps: 1017246/10000000)\n",
      "Epsilon: 0.09, Episode reward: -18.0, Mean reward: -17.82\n",
      "***************************\n",
      "Episode 957 finished (timesteps: 1019076/10000000)\n",
      "Epsilon: 0.09, Episode reward: -19.0, Mean reward: -17.86\n",
      "***************************\n",
      "Episode 958 finished (timesteps: 1021215/10000000)\n",
      "Epsilon: 0.09, Episode reward: -18.0, Mean reward: -17.85\n",
      "***************************\n",
      "Episode 959 finished (timesteps: 1022636/10000000)\n",
      "Epsilon: 0.09, Episode reward: -19.0, Mean reward: -17.89\n",
      "***************************\n",
      "Episode 960 finished (timesteps: 1024641/10000000)\n",
      "Epsilon: 0.08, Episode reward: -19.0, Mean reward: -17.91\n",
      "***************************\n",
      "Episode 961 finished (timesteps: 1026548/10000000)\n",
      "Epsilon: 0.08, Episode reward: -16.0, Mean reward: -17.90\n",
      "***************************\n",
      "Episode 962 finished (timesteps: 1028472/10000000)\n",
      "Epsilon: 0.08, Episode reward: -17.0, Mean reward: -17.88\n",
      "***************************\n",
      "Episode 963 finished (timesteps: 1030750/10000000)\n",
      "Epsilon: 0.08, Episode reward: -15.0, Mean reward: -17.84\n",
      "***************************\n",
      "Episode 964 finished (timesteps: 1032239/10000000)\n",
      "Epsilon: 0.08, Episode reward: -18.0, Mean reward: -17.84\n",
      "***************************\n",
      "Episode 965 finished (timesteps: 1033604/10000000)\n",
      "Epsilon: 0.08, Episode reward: -19.0, Mean reward: -17.89\n",
      "***************************\n",
      "Episode 966 finished (timesteps: 1035536/10000000)\n",
      "Epsilon: 0.07, Episode reward: -19.0, Mean reward: -17.91\n",
      "***************************\n",
      "Episode 967 finished (timesteps: 1037122/10000000)\n",
      "Epsilon: 0.07, Episode reward: -18.0, Mean reward: -17.96\n",
      "***************************\n",
      "Episode 968 finished (timesteps: 1038611/10000000)\n",
      "Epsilon: 0.07, Episode reward: -20.0, Mean reward: -17.96\n",
      "***************************\n",
      "Episode 969 finished (timesteps: 1040458/10000000)\n",
      "Epsilon: 0.07, Episode reward: -18.0, Mean reward: -17.95\n",
      "***************************\n",
      "Episode 970 finished (timesteps: 1042937/10000000)\n",
      "Epsilon: 0.07, Episode reward: -13.0, Mean reward: -17.87\n",
      "***************************\n",
      "Episode 971 finished (timesteps: 1045026/10000000)\n",
      "Epsilon: 0.06, Episode reward: -15.0, Mean reward: -17.83\n",
      "***************************\n",
      "Episode 972 finished (timesteps: 1046542/10000000)\n",
      "Epsilon: 0.06, Episode reward: -21.0, Mean reward: -17.87\n",
      "***************************\n",
      "Episode 973 finished (timesteps: 1048403/10000000)\n",
      "Epsilon: 0.06, Episode reward: -18.0, Mean reward: -17.85\n",
      "***************************\n",
      "Episode 974 finished (timesteps: 1050252/10000000)\n",
      "Epsilon: 0.06, Episode reward: -21.0, Mean reward: -17.91\n",
      "***************************\n",
      "Episode 975 finished (timesteps: 1051963/10000000)\n",
      "Epsilon: 0.06, Episode reward: -18.0, Mean reward: -17.90\n",
      "***************************\n",
      "Episode 976 finished (timesteps: 1053773/10000000)\n",
      "Epsilon: 0.06, Episode reward: -17.0, Mean reward: -17.88\n",
      "***************************\n",
      "Episode 977 finished (timesteps: 1055540/10000000)\n",
      "Epsilon: 0.05, Episode reward: -17.0, Mean reward: -17.88\n",
      "***************************\n",
      "Episode 978 finished (timesteps: 1056769/10000000)\n",
      "Epsilon: 0.05, Episode reward: -19.0, Mean reward: -17.91\n",
      "***************************\n",
      "Episode 979 finished (timesteps: 1058394/10000000)\n",
      "Epsilon: 0.05, Episode reward: -18.0, Mean reward: -17.88\n",
      "***************************\n",
      "Episode 980 finished (timesteps: 1059645/10000000)\n",
      "Epsilon: 0.05, Episode reward: -19.0, Mean reward: -17.87\n",
      "***************************\n",
      "Episode 981 finished (timesteps: 1061790/10000000)\n",
      "Epsilon: 0.05, Episode reward: -15.0, Mean reward: -17.86\n",
      "***************************\n",
      "Episode 982 finished (timesteps: 1063396/10000000)\n",
      "Epsilon: 0.05, Episode reward: -16.0, Mean reward: -17.84\n",
      "***************************\n",
      "Episode 983 finished (timesteps: 1065338/10000000)\n",
      "Epsilon: 0.04, Episode reward: -13.0, Mean reward: -17.82\n",
      "***************************\n",
      "Episode 984 finished (timesteps: 1067195/10000000)\n",
      "Epsilon: 0.04, Episode reward: -12.0, Mean reward: -17.78\n",
      "***************************\n",
      "Episode 985 finished (timesteps: 1068557/10000000)\n",
      "Epsilon: 0.04, Episode reward: -17.0, Mean reward: -17.79\n",
      "***************************\n",
      "Episode 986 finished (timesteps: 1070007/10000000)\n",
      "Epsilon: 0.04, Episode reward: -18.0, Mean reward: -17.79\n",
      "***************************\n",
      "Episode 987 finished (timesteps: 1071214/10000000)\n",
      "Epsilon: 0.04, Episode reward: -20.0, Mean reward: -17.79\n",
      "***************************\n",
      "Episode 988 finished (timesteps: 1073426/10000000)\n",
      "Epsilon: 0.04, Episode reward: -15.0, Mean reward: -17.77\n",
      "***************************\n",
      "Episode 989 finished (timesteps: 1075010/10000000)\n",
      "Epsilon: 0.03, Episode reward: -19.0, Mean reward: -17.78\n",
      "***************************\n",
      "Episode 990 finished (timesteps: 1076815/10000000)\n",
      "Epsilon: 0.03, Episode reward: -17.0, Mean reward: -17.75\n",
      "***************************\n",
      "Episode 991 finished (timesteps: 1078778/10000000)\n",
      "Epsilon: 0.03, Episode reward: -18.0, Mean reward: -17.75\n",
      "***************************\n",
      "Episode 992 finished (timesteps: 1080754/10000000)\n",
      "Epsilon: 0.03, Episode reward: -18.0, Mean reward: -17.73\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 993 finished (timesteps: 1082302/10000000)\n",
      "Epsilon: 0.03, Episode reward: -17.0, Mean reward: -17.71\n",
      "***************************\n",
      "Episode 994 finished (timesteps: 1083392/10000000)\n",
      "Epsilon: 0.03, Episode reward: -20.0, Mean reward: -17.72\n",
      "***************************\n",
      "Episode 995 finished (timesteps: 1085047/10000000)\n",
      "Epsilon: 0.02, Episode reward: -18.0, Mean reward: -17.73\n",
      "***************************\n",
      "Episode 996 finished (timesteps: 1086830/10000000)\n",
      "Epsilon: 0.02, Episode reward: -17.0, Mean reward: -17.70\n",
      "***************************\n",
      "Episode 997 finished (timesteps: 1088564/10000000)\n",
      "Epsilon: 0.02, Episode reward: -17.0, Mean reward: -17.68\n",
      "***************************\n",
      "Episode 998 finished (timesteps: 1090284/10000000)\n",
      "Epsilon: 0.02, Episode reward: -16.0, Mean reward: -17.67\n",
      "***************************\n",
      "Episode 999 finished (timesteps: 1092491/10000000)\n",
      "Epsilon: 0.02, Episode reward: -15.0, Mean reward: -17.65\n",
      "***************************\n",
      "Episode 1000 finished (timesteps: 1096024/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -17.58\n",
      "Saving checkpoint...\n",
      "Checkpoint saved into runs/checkpoints/CNN_DDQN_Pong-v5_11-04-2023_16:28:30\n",
      "***************************\n",
      "Episode 1001 finished (timesteps: 1097854/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -17.59\n",
      "***************************\n",
      "Episode 1002 finished (timesteps: 1099936/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -17.55\n",
      "***************************\n",
      "Episode 1003 finished (timesteps: 1102062/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -17.52\n",
      "***************************\n",
      "Episode 1004 finished (timesteps: 1104804/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -17.50\n",
      "***************************\n",
      "Episode 1005 finished (timesteps: 1106864/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -17.49\n",
      "***************************\n",
      "Episode 1006 finished (timesteps: 1109244/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -17.46\n",
      "***************************\n",
      "Episode 1007 finished (timesteps: 1111540/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -17.44\n",
      "***************************\n",
      "Episode 1008 finished (timesteps: 1113684/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -17.44\n",
      "***************************\n",
      "Episode 1009 finished (timesteps: 1115810/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -17.48\n",
      "***************************\n",
      "Episode 1010 finished (timesteps: 1118147/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -17.49\n",
      "***************************\n",
      "Episode 1011 finished (timesteps: 1119979/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -17.51\n",
      "***************************\n",
      "Episode 1012 finished (timesteps: 1122275/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -17.46\n",
      "***************************\n",
      "Episode 1013 finished (timesteps: 1125096/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -17.46\n",
      "***************************\n",
      "Episode 1014 finished (timesteps: 1126786/10000000)\n",
      "Epsilon: 0.01, Episode reward: -21.0, Mean reward: -17.46\n",
      "***************************\n",
      "Episode 1015 finished (timesteps: 1129574/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -17.44\n",
      "***************************\n",
      "Episode 1016 finished (timesteps: 1131988/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -17.40\n",
      "***************************\n",
      "Episode 1017 finished (timesteps: 1133908/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -17.40\n",
      "***************************\n",
      "Episode 1018 finished (timesteps: 1135708/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -17.38\n",
      "***************************\n",
      "Episode 1019 finished (timesteps: 1137524/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -17.41\n",
      "***************************\n",
      "Episode 1020 finished (timesteps: 1139612/10000000)\n",
      "Epsilon: 0.01, Episode reward: -19.0, Mean reward: -17.41\n",
      "***************************\n",
      "Episode 1021 finished (timesteps: 1141604/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -17.35\n",
      "***************************\n",
      "Episode 1022 finished (timesteps: 1144153/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -17.39\n",
      "***************************\n",
      "Episode 1023 finished (timesteps: 1145387/10000000)\n",
      "Epsilon: 0.01, Episode reward: -20.0, Mean reward: -17.42\n",
      "***************************\n",
      "Episode 1024 finished (timesteps: 1147384/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -17.38\n",
      "***************************\n",
      "Episode 1025 finished (timesteps: 1148904/10000000)\n",
      "Epsilon: 0.01, Episode reward: -19.0, Mean reward: -17.40\n",
      "***************************\n",
      "Episode 1026 finished (timesteps: 1151653/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -17.33\n",
      "***************************\n",
      "Episode 1027 finished (timesteps: 1154042/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -17.34\n",
      "***************************\n",
      "Episode 1028 finished (timesteps: 1156124/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -17.33\n",
      "***************************\n",
      "Episode 1029 finished (timesteps: 1158283/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -17.30\n",
      "***************************\n",
      "Episode 1030 finished (timesteps: 1160344/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -17.28\n",
      "***************************\n",
      "Episode 1031 finished (timesteps: 1161592/10000000)\n",
      "Epsilon: 0.01, Episode reward: -19.0, Mean reward: -17.30\n",
      "***************************\n",
      "Episode 1032 finished (timesteps: 1164346/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -17.26\n",
      "***************************\n",
      "Episode 1033 finished (timesteps: 1166069/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -17.26\n",
      "***************************\n",
      "Episode 1034 finished (timesteps: 1167743/10000000)\n",
      "Epsilon: 0.01, Episode reward: -19.0, Mean reward: -17.26\n",
      "***************************\n",
      "Episode 1035 finished (timesteps: 1169254/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -17.24\n",
      "***************************\n",
      "Episode 1036 finished (timesteps: 1171450/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -17.22\n",
      "***************************\n",
      "Episode 1037 finished (timesteps: 1173297/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -17.14\n",
      "***************************\n",
      "Episode 1038 finished (timesteps: 1174567/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -17.13\n",
      "***************************\n",
      "Episode 1039 finished (timesteps: 1176020/10000000)\n",
      "Epsilon: 0.01, Episode reward: -20.0, Mean reward: -17.12\n",
      "***************************\n",
      "Episode 1040 finished (timesteps: 1177346/10000000)\n",
      "Epsilon: 0.01, Episode reward: -19.0, Mean reward: -17.13\n",
      "***************************\n",
      "Episode 1041 finished (timesteps: 1178868/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -17.18\n",
      "***************************\n",
      "Episode 1042 finished (timesteps: 1180744/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -17.18\n",
      "***************************\n",
      "Episode 1043 finished (timesteps: 1182018/10000000)\n",
      "Epsilon: 0.01, Episode reward: -19.0, Mean reward: -17.23\n",
      "***************************\n",
      "Episode 1044 finished (timesteps: 1183820/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -17.17\n",
      "***************************\n",
      "Episode 1045 finished (timesteps: 1185829/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -17.13\n",
      "***************************\n",
      "Episode 1046 finished (timesteps: 1187622/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -17.08\n",
      "***************************\n",
      "Episode 1047 finished (timesteps: 1190139/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -17.02\n",
      "***************************\n",
      "Episode 1048 finished (timesteps: 1191681/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -16.99\n",
      "***************************\n",
      "Episode 1049 finished (timesteps: 1193364/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -16.99\n",
      "***************************\n",
      "Episode 1050 finished (timesteps: 1195475/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -16.93\n",
      "***************************\n",
      "Episode 1051 finished (timesteps: 1197554/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -16.92\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1052 finished (timesteps: 1199679/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -16.89\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-1200000.mp4.\n",
      "Moviepy - Writing video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-1200000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-1200000.mp4\n",
      "Episode 1053 finished (timesteps: 1201843/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -16.84\n",
      "***************************\n",
      "Episode 1054 finished (timesteps: 1203557/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -16.80\n",
      "***************************\n",
      "Episode 1055 finished (timesteps: 1205041/10000000)\n",
      "Epsilon: 0.01, Episode reward: -20.0, Mean reward: -16.86\n",
      "***************************\n",
      "Episode 1056 finished (timesteps: 1206843/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -16.86\n",
      "***************************\n",
      "Episode 1057 finished (timesteps: 1208521/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -16.80\n",
      "***************************\n",
      "Episode 1058 finished (timesteps: 1210054/10000000)\n",
      "Epsilon: 0.01, Episode reward: -19.0, Mean reward: -16.81\n",
      "***************************\n",
      "Episode 1059 finished (timesteps: 1211535/10000000)\n",
      "Epsilon: 0.01, Episode reward: -19.0, Mean reward: -16.81\n",
      "***************************\n",
      "Episode 1060 finished (timesteps: 1212963/10000000)\n",
      "Epsilon: 0.01, Episode reward: -21.0, Mean reward: -16.83\n",
      "***************************\n",
      "Episode 1061 finished (timesteps: 1215435/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -16.75\n",
      "***************************\n",
      "Episode 1062 finished (timesteps: 1217495/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -16.71\n",
      "***************************\n",
      "Episode 1063 finished (timesteps: 1219055/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -16.72\n",
      "***************************\n",
      "Episode 1064 finished (timesteps: 1220909/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -16.67\n",
      "***************************\n",
      "Episode 1065 finished (timesteps: 1223038/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -16.63\n",
      "***************************\n",
      "Episode 1066 finished (timesteps: 1224976/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -16.58\n",
      "***************************\n",
      "Episode 1067 finished (timesteps: 1227395/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -16.46\n",
      "***************************\n",
      "Episode 1068 finished (timesteps: 1228874/10000000)\n",
      "Epsilon: 0.01, Episode reward: -19.0, Mean reward: -16.45\n",
      "***************************\n",
      "Episode 1069 finished (timesteps: 1230347/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -16.45\n",
      "***************************\n",
      "Episode 1070 finished (timesteps: 1232479/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -16.43\n",
      "***************************\n",
      "Episode 1071 finished (timesteps: 1233807/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -16.46\n",
      "***************************\n",
      "Episode 1072 finished (timesteps: 1236489/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -16.30\n",
      "***************************\n",
      "Episode 1073 finished (timesteps: 1238589/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -16.23\n",
      "***************************\n",
      "Episode 1074 finished (timesteps: 1240910/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -16.16\n",
      "***************************\n",
      "Episode 1075 finished (timesteps: 1242863/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -16.11\n",
      "***************************\n",
      "Episode 1076 finished (timesteps: 1244805/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -16.07\n",
      "***************************\n",
      "Episode 1077 finished (timesteps: 1246281/10000000)\n",
      "Epsilon: 0.01, Episode reward: -21.0, Mean reward: -16.11\n",
      "***************************\n",
      "Episode 1078 finished (timesteps: 1248215/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -16.03\n",
      "***************************\n",
      "Episode 1079 finished (timesteps: 1250047/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -15.98\n",
      "***************************\n",
      "Episode 1080 finished (timesteps: 1251652/10000000)\n",
      "Epsilon: 0.01, Episode reward: -19.0, Mean reward: -15.98\n",
      "***************************\n",
      "Episode 1081 finished (timesteps: 1253466/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -15.97\n",
      "***************************\n",
      "Episode 1082 finished (timesteps: 1255623/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -15.91\n",
      "***************************\n",
      "Episode 1083 finished (timesteps: 1257362/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -15.94\n",
      "***************************\n",
      "Episode 1084 finished (timesteps: 1259713/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -15.92\n",
      "***************************\n",
      "Episode 1085 finished (timesteps: 1261721/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -15.88\n",
      "***************************\n",
      "Episode 1086 finished (timesteps: 1263432/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -15.86\n",
      "***************************\n",
      "Episode 1087 finished (timesteps: 1265334/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -15.77\n",
      "***************************\n",
      "Episode 1088 finished (timesteps: 1266919/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -15.76\n",
      "***************************\n",
      "Episode 1089 finished (timesteps: 1268959/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -15.71\n",
      "***************************\n",
      "Episode 1090 finished (timesteps: 1270603/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -15.70\n",
      "***************************\n",
      "Episode 1091 finished (timesteps: 1272509/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -15.65\n",
      "***************************\n",
      "Episode 1092 finished (timesteps: 1274268/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -15.63\n",
      "***************************\n",
      "Episode 1093 finished (timesteps: 1275669/10000000)\n",
      "Epsilon: 0.01, Episode reward: -20.0, Mean reward: -15.66\n",
      "***************************\n",
      "Episode 1094 finished (timesteps: 1277719/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -15.56\n",
      "***************************\n",
      "Episode 1095 finished (timesteps: 1279518/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -15.51\n",
      "***************************\n",
      "Episode 1096 finished (timesteps: 1281493/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -15.44\n",
      "***************************\n",
      "Episode 1097 finished (timesteps: 1283380/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -15.37\n",
      "***************************\n",
      "Episode 1098 finished (timesteps: 1285500/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -15.36\n",
      "***************************\n",
      "Episode 1099 finished (timesteps: 1287427/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -15.36\n",
      "***************************\n",
      "Episode 1100 finished (timesteps: 1289665/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -15.32\n",
      "***************************\n",
      "Episode 1101 finished (timesteps: 1291528/10000000)\n",
      "Epsilon: 0.01, Episode reward: -19.0, Mean reward: -15.33\n",
      "***************************\n",
      "Episode 1102 finished (timesteps: 1293309/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -15.37\n",
      "***************************\n",
      "Episode 1103 finished (timesteps: 1295303/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -15.39\n",
      "***************************\n",
      "Episode 1104 finished (timesteps: 1297761/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -15.34\n",
      "***************************\n",
      "Episode 1105 finished (timesteps: 1300250/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -15.26\n",
      "***************************\n",
      "Episode 1106 finished (timesteps: 1303345/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -15.15\n",
      "***************************\n",
      "Episode 1107 finished (timesteps: 1305174/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -15.16\n",
      "***************************\n",
      "Episode 1108 finished (timesteps: 1307287/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -15.18\n",
      "***************************\n",
      "Episode 1109 finished (timesteps: 1309008/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -15.15\n",
      "***************************\n",
      "Episode 1110 finished (timesteps: 1310808/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -15.14\n",
      "***************************\n",
      "Episode 1111 finished (timesteps: 1312966/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -15.05\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1112 finished (timesteps: 1315282/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -15.01\n",
      "***************************\n",
      "Episode 1113 finished (timesteps: 1316918/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -15.03\n",
      "***************************\n",
      "Episode 1114 finished (timesteps: 1319168/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -14.96\n",
      "***************************\n",
      "Episode 1115 finished (timesteps: 1321017/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -14.90\n",
      "***************************\n",
      "Episode 1116 finished (timesteps: 1322651/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -14.94\n",
      "***************************\n",
      "Episode 1117 finished (timesteps: 1324227/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -14.95\n",
      "***************************\n",
      "Episode 1118 finished (timesteps: 1325950/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -14.92\n",
      "***************************\n",
      "Episode 1119 finished (timesteps: 1328368/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -14.83\n",
      "***************************\n",
      "Episode 1120 finished (timesteps: 1330413/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -14.79\n",
      "***************************\n",
      "Episode 1121 finished (timesteps: 1332080/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -14.80\n",
      "***************************\n",
      "Episode 1122 finished (timesteps: 1334787/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -14.73\n",
      "***************************\n",
      "Episode 1123 finished (timesteps: 1337318/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -14.64\n",
      "***************************\n",
      "Episode 1124 finished (timesteps: 1339261/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -14.63\n",
      "***************************\n",
      "Episode 1125 finished (timesteps: 1341038/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -14.58\n",
      "***************************\n",
      "Episode 1126 finished (timesteps: 1343227/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -14.58\n",
      "***************************\n",
      "Episode 1127 finished (timesteps: 1345355/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -14.56\n",
      "***************************\n",
      "Episode 1128 finished (timesteps: 1346866/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -14.56\n",
      "***************************\n",
      "Episode 1129 finished (timesteps: 1348902/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -14.50\n",
      "***************************\n",
      "Episode 1130 finished (timesteps: 1350871/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -14.44\n",
      "***************************\n",
      "Episode 1131 finished (timesteps: 1353612/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -14.32\n",
      "***************************\n",
      "Episode 1132 finished (timesteps: 1356157/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -14.28\n",
      "***************************\n",
      "Episode 1133 finished (timesteps: 1358919/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -14.22\n",
      "***************************\n",
      "Episode 1134 finished (timesteps: 1360936/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -14.17\n",
      "***************************\n",
      "Episode 1135 finished (timesteps: 1362473/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -14.16\n",
      "***************************\n",
      "Episode 1136 finished (timesteps: 1364612/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -14.18\n",
      "***************************\n",
      "Episode 1137 finished (timesteps: 1367331/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -14.16\n",
      "***************************\n",
      "Episode 1138 finished (timesteps: 1370228/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -14.06\n",
      "***************************\n",
      "Episode 1139 finished (timesteps: 1373282/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -13.93\n",
      "***************************\n",
      "Episode 1140 finished (timesteps: 1375346/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -13.91\n",
      "***************************\n",
      "Episode 1141 finished (timesteps: 1377431/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -13.83\n",
      "***************************\n",
      "Episode 1142 finished (timesteps: 1379107/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -13.84\n",
      "***************************\n",
      "Episode 1143 finished (timesteps: 1380664/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -13.82\n",
      "***************************\n",
      "Episode 1144 finished (timesteps: 1383103/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -13.78\n",
      "***************************\n",
      "Episode 1145 finished (timesteps: 1384653/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -13.83\n",
      "***************************\n",
      "Episode 1146 finished (timesteps: 1386847/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -13.83\n",
      "***************************\n",
      "Episode 1147 finished (timesteps: 1388989/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -13.86\n",
      "***************************\n",
      "Episode 1148 finished (timesteps: 1390898/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -13.85\n",
      "***************************\n",
      "Episode 1149 finished (timesteps: 1392298/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -13.86\n",
      "***************************\n",
      "Episode 1150 finished (timesteps: 1394584/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -13.92\n",
      "***************************\n",
      "Episode 1151 finished (timesteps: 1397431/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -13.87\n",
      "***************************\n",
      "Episode 1152 finished (timesteps: 1399481/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -13.86\n",
      "***************************\n",
      "Episode 1153 finished (timesteps: 1401809/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -13.85\n",
      "***************************\n",
      "Episode 1154 finished (timesteps: 1403623/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -13.80\n",
      "***************************\n",
      "Episode 1155 finished (timesteps: 1405420/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -13.75\n",
      "***************************\n",
      "Episode 1156 finished (timesteps: 1407219/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -13.73\n",
      "***************************\n",
      "Episode 1157 finished (timesteps: 1409687/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -13.68\n",
      "***************************\n",
      "Episode 1158 finished (timesteps: 1411678/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -13.65\n",
      "***************************\n",
      "Episode 1159 finished (timesteps: 1414785/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -13.50\n",
      "***************************\n",
      "Episode 1160 finished (timesteps: 1416683/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -13.45\n",
      "***************************\n",
      "Episode 1161 finished (timesteps: 1418516/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -13.49\n",
      "***************************\n",
      "Episode 1162 finished (timesteps: 1421271/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -13.45\n",
      "***************************\n",
      "Episode 1163 finished (timesteps: 1423298/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -13.42\n",
      "***************************\n",
      "Episode 1164 finished (timesteps: 1425987/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -13.33\n",
      "***************************\n",
      "Episode 1165 finished (timesteps: 1428042/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -13.31\n",
      "***************************\n",
      "Episode 1166 finished (timesteps: 1430093/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -13.30\n",
      "***************************\n",
      "Episode 1167 finished (timesteps: 1432071/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -13.34\n",
      "***************************\n",
      "Episode 1168 finished (timesteps: 1433157/10000000)\n",
      "Epsilon: 0.01, Episode reward: -20.0, Mean reward: -13.35\n",
      "***************************\n",
      "Episode 1169 finished (timesteps: 1435135/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -13.31\n",
      "***************************\n",
      "Episode 1170 finished (timesteps: 1437089/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -13.32\n",
      "***************************\n",
      "Episode 1171 finished (timesteps: 1438542/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -13.31\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1172 finished (timesteps: 1440219/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -13.40\n",
      "***************************\n",
      "Episode 1173 finished (timesteps: 1442228/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -13.44\n",
      "***************************\n",
      "Episode 1174 finished (timesteps: 1445141/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -13.35\n",
      "***************************\n",
      "Episode 1175 finished (timesteps: 1447278/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -13.36\n",
      "***************************\n",
      "Episode 1176 finished (timesteps: 1450118/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -13.30\n",
      "***************************\n",
      "Episode 1177 finished (timesteps: 1452860/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -13.18\n",
      "***************************\n",
      "Episode 1178 finished (timesteps: 1455039/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -13.20\n",
      "***************************\n",
      "Episode 1179 finished (timesteps: 1457366/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -13.19\n",
      "***************************\n",
      "Episode 1180 finished (timesteps: 1459784/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -13.13\n",
      "***************************\n",
      "Episode 1181 finished (timesteps: 1462085/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -13.13\n",
      "***************************\n",
      "Episode 1182 finished (timesteps: 1464612/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -13.15\n",
      "***************************\n",
      "Episode 1183 finished (timesteps: 1466248/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -13.16\n",
      "***************************\n",
      "Episode 1184 finished (timesteps: 1469278/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -13.18\n",
      "***************************\n",
      "Episode 1185 finished (timesteps: 1471091/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -13.20\n",
      "***************************\n",
      "Episode 1186 finished (timesteps: 1473264/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -13.15\n",
      "***************************\n",
      "Episode 1187 finished (timesteps: 1476003/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -13.09\n",
      "***************************\n",
      "Episode 1188 finished (timesteps: 1478328/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -13.04\n",
      "***************************\n",
      "Episode 1189 finished (timesteps: 1479822/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -13.06\n",
      "***************************\n",
      "Episode 1190 finished (timesteps: 1481566/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -13.05\n",
      "***************************\n",
      "Episode 1191 finished (timesteps: 1483692/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -13.03\n",
      "***************************\n",
      "Episode 1192 finished (timesteps: 1485853/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -13.00\n",
      "***************************\n",
      "Episode 1193 finished (timesteps: 1487952/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -12.90\n",
      "***************************\n",
      "Episode 1194 finished (timesteps: 1490379/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -12.89\n",
      "***************************\n",
      "Episode 1195 finished (timesteps: 1492313/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -12.92\n",
      "***************************\n",
      "Episode 1196 finished (timesteps: 1494228/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -12.95\n",
      "***************************\n",
      "Episode 1197 finished (timesteps: 1496431/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -12.95\n",
      "***************************\n",
      "Episode 1198 finished (timesteps: 1498584/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -12.92\n",
      "***************************\n",
      "Episode 1199 finished (timesteps: 1500877/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -12.87\n",
      "***************************\n",
      "Episode 1200 finished (timesteps: 1502292/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -12.94\n",
      "***************************\n",
      "Episode 1201 finished (timesteps: 1503636/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -12.92\n",
      "***************************\n",
      "Episode 1202 finished (timesteps: 1505674/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -12.85\n",
      "***************************\n",
      "Episode 1203 finished (timesteps: 1508531/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -12.71\n",
      "***************************\n",
      "Episode 1204 finished (timesteps: 1510006/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -12.78\n",
      "***************************\n",
      "Episode 1205 finished (timesteps: 1512159/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -12.79\n",
      "***************************\n",
      "Episode 1206 finished (timesteps: 1513974/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -12.85\n",
      "***************************\n",
      "Episode 1207 finished (timesteps: 1516139/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -12.82\n",
      "***************************\n",
      "Episode 1208 finished (timesteps: 1517982/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -12.78\n",
      "***************************\n",
      "Episode 1209 finished (timesteps: 1519813/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -12.77\n",
      "***************************\n",
      "Episode 1210 finished (timesteps: 1521838/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -12.72\n",
      "***************************\n",
      "Episode 1211 finished (timesteps: 1524382/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -12.74\n",
      "***************************\n",
      "Episode 1212 finished (timesteps: 1526325/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -12.75\n",
      "***************************\n",
      "Episode 1213 finished (timesteps: 1528656/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -12.65\n",
      "***************************\n",
      "Episode 1214 finished (timesteps: 1531015/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -12.60\n",
      "***************************\n",
      "Episode 1215 finished (timesteps: 1532763/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -12.65\n",
      "***************************\n",
      "Episode 1216 finished (timesteps: 1534758/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -12.58\n",
      "***************************\n",
      "Episode 1217 finished (timesteps: 1536476/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -12.56\n",
      "***************************\n",
      "Episode 1218 finished (timesteps: 1538523/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -12.52\n",
      "***************************\n",
      "Episode 1219 finished (timesteps: 1540402/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -12.57\n",
      "***************************\n",
      "Episode 1220 finished (timesteps: 1542267/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -12.54\n",
      "***************************\n",
      "Episode 1221 finished (timesteps: 1544460/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -12.53\n",
      "***************************\n",
      "Episode 1222 finished (timesteps: 1545943/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -12.59\n",
      "***************************\n",
      "Episode 1223 finished (timesteps: 1547953/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -12.60\n",
      "***************************\n",
      "Episode 1224 finished (timesteps: 1550120/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -12.56\n",
      "***************************\n",
      "Episode 1225 finished (timesteps: 1552864/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -12.49\n",
      "***************************\n",
      "Episode 1226 finished (timesteps: 1555372/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -12.43\n",
      "***************************\n",
      "Episode 1227 finished (timesteps: 1557443/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -12.40\n",
      "***************************\n",
      "Episode 1228 finished (timesteps: 1560013/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -12.31\n",
      "***************************\n",
      "Episode 1229 finished (timesteps: 1562891/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: -12.20\n",
      "***************************\n",
      "Episode 1230 finished (timesteps: 1565326/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -12.14\n",
      "***************************\n",
      "Episode 1231 finished (timesteps: 1567628/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -12.20\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1232 finished (timesteps: 1569752/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -12.21\n",
      "***************************\n",
      "Episode 1233 finished (timesteps: 1571915/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -12.21\n",
      "***************************\n",
      "Episode 1234 finished (timesteps: 1573879/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -12.18\n",
      "***************************\n",
      "Episode 1235 finished (timesteps: 1575844/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -12.13\n",
      "***************************\n",
      "Episode 1236 finished (timesteps: 1578195/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -12.02\n",
      "***************************\n",
      "Episode 1237 finished (timesteps: 1580205/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -12.03\n",
      "***************************\n",
      "Episode 1238 finished (timesteps: 1582536/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -12.01\n",
      "***************************\n",
      "Episode 1239 finished (timesteps: 1584107/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -12.11\n",
      "***************************\n",
      "Episode 1240 finished (timesteps: 1586909/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -11.99\n",
      "***************************\n",
      "Episode 1241 finished (timesteps: 1589096/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -12.01\n",
      "***************************\n",
      "Episode 1242 finished (timesteps: 1591731/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -11.89\n",
      "***************************\n",
      "Episode 1243 finished (timesteps: 1594216/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -11.82\n",
      "***************************\n",
      "Episode 1244 finished (timesteps: 1596517/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -11.82\n",
      "***************************\n",
      "Episode 1245 finished (timesteps: 1598231/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -11.80\n",
      "***************************\n",
      "Episode 1246 finished (timesteps: 1600196/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -11.80\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-1600000.mp4.\n",
      "Moviepy - Writing video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-1600000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-1600000.mp4\n",
      "Episode 1247 finished (timesteps: 1602663/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -11.70\n",
      "***************************\n",
      "Episode 1248 finished (timesteps: 1604704/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -11.68\n",
      "***************************\n",
      "Episode 1249 finished (timesteps: 1606700/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -11.60\n",
      "***************************\n",
      "Episode 1250 finished (timesteps: 1608148/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -11.60\n",
      "***************************\n",
      "Episode 1251 finished (timesteps: 1609496/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -11.67\n",
      "***************************\n",
      "Episode 1252 finished (timesteps: 1611376/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -11.64\n",
      "***************************\n",
      "Episode 1253 finished (timesteps: 1613922/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: -11.53\n",
      "***************************\n",
      "Episode 1254 finished (timesteps: 1615960/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -11.52\n",
      "***************************\n",
      "Episode 1255 finished (timesteps: 1617738/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -11.51\n",
      "***************************\n",
      "Episode 1256 finished (timesteps: 1619663/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -11.48\n",
      "***************************\n",
      "Episode 1257 finished (timesteps: 1621610/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -11.49\n",
      "***************************\n",
      "Episode 1258 finished (timesteps: 1623579/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: -11.21\n",
      "***************************\n",
      "Episode 1259 finished (timesteps: 1625163/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -11.30\n",
      "***************************\n",
      "Episode 1260 finished (timesteps: 1628078/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: -11.13\n",
      "***************************\n",
      "Episode 1261 finished (timesteps: 1630149/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -11.14\n",
      "***************************\n",
      "Episode 1262 finished (timesteps: 1632695/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: -10.96\n",
      "***************************\n",
      "Episode 1263 finished (timesteps: 1634627/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -10.95\n",
      "***************************\n",
      "Episode 1264 finished (timesteps: 1637160/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -10.97\n",
      "***************************\n",
      "Episode 1265 finished (timesteps: 1639051/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -10.99\n",
      "***************************\n",
      "Episode 1266 finished (timesteps: 1641494/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -10.94\n",
      "***************************\n",
      "Episode 1267 finished (timesteps: 1644273/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: -10.82\n",
      "***************************\n",
      "Episode 1268 finished (timesteps: 1646249/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -10.74\n",
      "***************************\n",
      "Episode 1269 finished (timesteps: 1647975/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -10.71\n",
      "***************************\n",
      "Episode 1270 finished (timesteps: 1650637/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -10.65\n",
      "***************************\n",
      "Episode 1271 finished (timesteps: 1652076/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -10.66\n",
      "***************************\n",
      "Episode 1272 finished (timesteps: 1653642/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -10.69\n",
      "***************************\n",
      "Episode 1273 finished (timesteps: 1656244/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -10.58\n",
      "***************************\n",
      "Episode 1274 finished (timesteps: 1658422/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -10.65\n",
      "***************************\n",
      "Episode 1275 finished (timesteps: 1660338/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -10.65\n",
      "***************************\n",
      "Episode 1276 finished (timesteps: 1662311/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -10.69\n",
      "***************************\n",
      "Episode 1277 finished (timesteps: 1664461/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -10.73\n",
      "***************************\n",
      "Episode 1278 finished (timesteps: 1667106/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: -10.61\n",
      "***************************\n",
      "Episode 1279 finished (timesteps: 1670008/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -10.52\n",
      "***************************\n",
      "Episode 1280 finished (timesteps: 1671881/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -10.54\n",
      "***************************\n",
      "Episode 1281 finished (timesteps: 1674715/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: -10.42\n",
      "***************************\n",
      "Episode 1282 finished (timesteps: 1676846/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -10.40\n",
      "***************************\n",
      "Episode 1283 finished (timesteps: 1679239/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: -10.15\n",
      "***************************\n",
      "Episode 1284 finished (timesteps: 1681487/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -10.13\n",
      "***************************\n",
      "Episode 1285 finished (timesteps: 1684176/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -10.05\n",
      "***************************\n",
      "Episode 1286 finished (timesteps: 1686468/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: -9.84\n",
      "***************************\n",
      "Episode 1287 finished (timesteps: 1688820/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -9.88\n",
      "***************************\n",
      "Episode 1288 finished (timesteps: 1690185/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -9.95\n",
      "***************************\n",
      "Episode 1289 finished (timesteps: 1692470/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -9.84\n",
      "***************************\n",
      "Episode 1290 finished (timesteps: 1694758/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -9.77\n",
      "***************************\n",
      "Episode 1291 finished (timesteps: 1696481/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -9.79\n",
      "***************************\n",
      "Episode 1292 finished (timesteps: 1698263/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -9.78\n",
      "***************************\n",
      "Episode 1293 finished (timesteps: 1700153/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -9.82\n",
      "***************************\n",
      "Episode 1294 finished (timesteps: 1702441/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -9.81\n",
      "***************************\n",
      "Episode 1295 finished (timesteps: 1704165/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -9.80\n",
      "***************************\n",
      "Episode 1296 finished (timesteps: 1707125/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: -9.68\n",
      "***************************\n",
      "Episode 1297 finished (timesteps: 1709199/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -9.68\n",
      "***************************\n",
      "Episode 1298 finished (timesteps: 1710979/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -9.71\n",
      "***************************\n",
      "Episode 1299 finished (timesteps: 1713355/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -9.67\n",
      "***************************\n",
      "Episode 1300 finished (timesteps: 1715781/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -9.55\n",
      "***************************\n",
      "Episode 1301 finished (timesteps: 1717552/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -9.51\n",
      "***************************\n",
      "Episode 1302 finished (timesteps: 1719408/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -9.52\n",
      "***************************\n",
      "Episode 1303 finished (timesteps: 1721513/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -9.58\n",
      "***************************\n",
      "Episode 1304 finished (timesteps: 1723356/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -9.55\n",
      "***************************\n",
      "Episode 1305 finished (timesteps: 1726085/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -9.50\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1306 finished (timesteps: 1728578/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -9.43\n",
      "***************************\n",
      "Episode 1307 finished (timesteps: 1730975/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -9.38\n",
      "***************************\n",
      "Episode 1308 finished (timesteps: 1733713/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -9.35\n",
      "***************************\n",
      "Episode 1309 finished (timesteps: 1735541/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -9.37\n",
      "***************************\n",
      "Episode 1310 finished (timesteps: 1738277/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -9.30\n",
      "***************************\n",
      "Episode 1311 finished (timesteps: 1740227/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -9.33\n",
      "***************************\n",
      "Episode 1312 finished (timesteps: 1742781/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -9.28\n",
      "***************************\n",
      "Episode 1313 finished (timesteps: 1744600/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -9.38\n",
      "***************************\n",
      "Episode 1314 finished (timesteps: 1747325/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -9.38\n",
      "***************************\n",
      "Episode 1315 finished (timesteps: 1749866/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: -9.21\n",
      "***************************\n",
      "Episode 1316 finished (timesteps: 1752158/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: -9.03\n",
      "***************************\n",
      "Episode 1317 finished (timesteps: 1754364/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -8.98\n",
      "***************************\n",
      "Episode 1318 finished (timesteps: 1757112/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -8.90\n",
      "***************************\n",
      "Episode 1319 finished (timesteps: 1759536/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -8.79\n",
      "***************************\n",
      "Episode 1320 finished (timesteps: 1761474/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -8.79\n",
      "***************************\n",
      "Episode 1321 finished (timesteps: 1763894/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -8.75\n",
      "***************************\n",
      "Episode 1322 finished (timesteps: 1766240/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -8.54\n",
      "***************************\n",
      "Episode 1323 finished (timesteps: 1768309/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -8.53\n",
      "***************************\n",
      "Episode 1324 finished (timesteps: 1770941/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -8.46\n",
      "***************************\n",
      "Episode 1325 finished (timesteps: 1773199/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -8.48\n",
      "***************************\n",
      "Episode 1326 finished (timesteps: 1775779/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -8.49\n",
      "***************************\n",
      "Episode 1327 finished (timesteps: 1778107/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -8.48\n",
      "***************************\n",
      "Episode 1328 finished (timesteps: 1780338/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -8.49\n",
      "***************************\n",
      "Episode 1329 finished (timesteps: 1782069/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -8.63\n",
      "***************************\n",
      "Episode 1330 finished (timesteps: 1783899/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -8.74\n",
      "***************************\n",
      "Episode 1331 finished (timesteps: 1786785/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -8.69\n",
      "***************************\n",
      "Episode 1332 finished (timesteps: 1788792/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -8.69\n",
      "***************************\n",
      "Episode 1333 finished (timesteps: 1791260/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -8.70\n",
      "***************************\n",
      "Episode 1334 finished (timesteps: 1792671/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -8.76\n",
      "***************************\n",
      "Episode 1335 finished (timesteps: 1795010/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -8.78\n",
      "***************************\n",
      "Episode 1336 finished (timesteps: 1797209/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -8.86\n",
      "***************************\n",
      "Episode 1337 finished (timesteps: 1799412/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -8.85\n",
      "***************************\n",
      "Episode 1338 finished (timesteps: 1801494/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: -8.64\n",
      "***************************\n",
      "Episode 1339 finished (timesteps: 1804049/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -8.57\n",
      "***************************\n",
      "Episode 1340 finished (timesteps: 1806007/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -8.65\n",
      "***************************\n",
      "Episode 1341 finished (timesteps: 1808748/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: -8.51\n",
      "***************************\n",
      "Episode 1342 finished (timesteps: 1811420/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: -8.47\n",
      "***************************\n",
      "Episode 1343 finished (timesteps: 1813586/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -8.48\n",
      "***************************\n",
      "Episode 1344 finished (timesteps: 1815423/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: -8.24\n",
      "***************************\n",
      "Episode 1345 finished (timesteps: 1817252/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -8.21\n",
      "***************************\n",
      "Episode 1346 finished (timesteps: 1819999/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: -8.08\n",
      "***************************\n",
      "Episode 1347 finished (timesteps: 1822225/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: -7.90\n",
      "***************************\n",
      "Episode 1348 finished (timesteps: 1824532/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -7.84\n",
      "***************************\n",
      "Episode 1349 finished (timesteps: 1826670/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -7.79\n",
      "***************************\n",
      "Episode 1350 finished (timesteps: 1828460/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -7.76\n",
      "***************************\n",
      "Episode 1351 finished (timesteps: 1831093/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: -7.57\n",
      "***************************\n",
      "Episode 1352 finished (timesteps: 1833559/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -7.51\n",
      "***************************\n",
      "Episode 1353 finished (timesteps: 1835624/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -7.61\n",
      "***************************\n",
      "Episode 1354 finished (timesteps: 1837956/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -7.57\n",
      "***************************\n",
      "Episode 1355 finished (timesteps: 1839835/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -7.54\n",
      "***************************\n",
      "Episode 1356 finished (timesteps: 1842356/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: -7.34\n",
      "***************************\n",
      "Episode 1357 finished (timesteps: 1844560/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -7.35\n",
      "***************************\n",
      "Episode 1358 finished (timesteps: 1846777/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -7.56\n",
      "***************************\n",
      "Episode 1359 finished (timesteps: 1848756/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -7.54\n",
      "***************************\n",
      "Episode 1360 finished (timesteps: 1851467/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: -7.53\n",
      "***************************\n",
      "Episode 1361 finished (timesteps: 1854223/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -7.37\n",
      "***************************\n",
      "Episode 1362 finished (timesteps: 1856334/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -7.57\n",
      "***************************\n",
      "Episode 1363 finished (timesteps: 1858133/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -7.55\n",
      "***************************\n",
      "Episode 1364 finished (timesteps: 1859859/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -7.61\n",
      "***************************\n",
      "Episode 1365 finished (timesteps: 1862002/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -7.54\n",
      "***************************\n",
      "Episode 1366 finished (timesteps: 1863956/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -7.64\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1367 finished (timesteps: 1866030/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: -7.51\n",
      "***************************\n",
      "Episode 1368 finished (timesteps: 1867872/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -7.55\n",
      "***************************\n",
      "Episode 1369 finished (timesteps: 1869952/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: -7.33\n",
      "***************************\n",
      "Episode 1370 finished (timesteps: 1872156/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: -7.19\n",
      "***************************\n",
      "Episode 1371 finished (timesteps: 1874258/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -7.14\n",
      "***************************\n",
      "Episode 1372 finished (timesteps: 1875992/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -7.10\n",
      "***************************\n",
      "Episode 1373 finished (timesteps: 1878421/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -7.11\n",
      "***************************\n",
      "Episode 1374 finished (timesteps: 1880668/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: -6.88\n",
      "***************************\n",
      "Episode 1375 finished (timesteps: 1882716/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -6.84\n",
      "***************************\n",
      "Episode 1376 finished (timesteps: 1885256/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -6.70\n",
      "***************************\n",
      "Episode 1377 finished (timesteps: 1888028/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: -6.56\n",
      "***************************\n",
      "Episode 1378 finished (timesteps: 1889665/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -6.67\n",
      "***************************\n",
      "Episode 1379 finished (timesteps: 1891963/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -6.73\n",
      "***************************\n",
      "Episode 1380 finished (timesteps: 1894692/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -6.52\n",
      "***************************\n",
      "Episode 1381 finished (timesteps: 1896629/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: -6.37\n",
      "***************************\n",
      "Episode 1382 finished (timesteps: 1898050/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -6.42\n",
      "***************************\n",
      "Episode 1383 finished (timesteps: 1899242/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -6.68\n",
      "***************************\n",
      "Episode 1384 finished (timesteps: 1901730/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -6.62\n",
      "***************************\n",
      "Episode 1385 finished (timesteps: 1904230/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: -6.51\n",
      "***************************\n",
      "Episode 1386 finished (timesteps: 1906506/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -6.71\n",
      "***************************\n",
      "Episode 1387 finished (timesteps: 1908652/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -6.75\n",
      "***************************\n",
      "Episode 1388 finished (timesteps: 1910297/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -6.74\n",
      "***************************\n",
      "Episode 1389 finished (timesteps: 1912531/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: -6.59\n",
      "***************************\n",
      "Episode 1390 finished (timesteps: 1914478/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -6.65\n",
      "***************************\n",
      "Episode 1391 finished (timesteps: 1916604/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -6.63\n",
      "***************************\n",
      "Episode 1392 finished (timesteps: 1918923/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -6.45\n",
      "***************************\n",
      "Episode 1393 finished (timesteps: 1920749/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -6.44\n",
      "***************************\n",
      "Episode 1394 finished (timesteps: 1922766/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: -6.22\n",
      "***************************\n",
      "Episode 1395 finished (timesteps: 1923979/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -6.25\n",
      "***************************\n",
      "Episode 1396 finished (timesteps: 1925660/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -6.38\n",
      "***************************\n",
      "Episode 1397 finished (timesteps: 1927465/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -6.41\n",
      "***************************\n",
      "Episode 1398 finished (timesteps: 1929385/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -6.37\n",
      "***************************\n",
      "Episode 1399 finished (timesteps: 1932091/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: -6.33\n",
      "***************************\n",
      "Episode 1400 finished (timesteps: 1934254/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -6.36\n",
      "***************************\n",
      "Episode 1401 finished (timesteps: 1936363/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -6.30\n",
      "***************************\n",
      "Episode 1402 finished (timesteps: 1938337/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -6.31\n",
      "***************************\n",
      "Episode 1403 finished (timesteps: 1940147/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -6.35\n",
      "***************************\n",
      "Episode 1404 finished (timesteps: 1941518/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -6.39\n",
      "***************************\n",
      "Episode 1405 finished (timesteps: 1943570/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -6.44\n",
      "***************************\n",
      "Episode 1406 finished (timesteps: 1945545/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -6.49\n",
      "***************************\n",
      "Episode 1407 finished (timesteps: 1947823/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: -6.32\n",
      "***************************\n",
      "Episode 1408 finished (timesteps: 1949675/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -6.36\n",
      "***************************\n",
      "Episode 1409 finished (timesteps: 1952737/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: -6.19\n",
      "***************************\n",
      "Episode 1410 finished (timesteps: 1953951/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -6.32\n",
      "***************************\n",
      "Episode 1411 finished (timesteps: 1956207/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -6.22\n",
      "***************************\n",
      "Episode 1412 finished (timesteps: 1959059/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -6.18\n",
      "***************************\n",
      "Episode 1413 finished (timesteps: 1961029/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -6.17\n",
      "***************************\n",
      "Episode 1414 finished (timesteps: 1962745/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: -5.90\n",
      "***************************\n",
      "Episode 1415 finished (timesteps: 1965451/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: -5.94\n",
      "***************************\n",
      "Episode 1416 finished (timesteps: 1967670/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -6.05\n",
      "***************************\n",
      "Episode 1417 finished (timesteps: 1969798/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -6.08\n",
      "***************************\n",
      "Episode 1418 finished (timesteps: 1971697/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -6.17\n",
      "***************************\n",
      "Episode 1419 finished (timesteps: 1974420/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -6.18\n",
      "***************************\n",
      "Episode 1420 finished (timesteps: 1975743/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -6.23\n",
      "***************************\n",
      "Episode 1421 finished (timesteps: 1977502/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -6.26\n",
      "***************************\n",
      "Episode 1422 finished (timesteps: 1979767/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -6.41\n",
      "***************************\n",
      "Episode 1423 finished (timesteps: 1981742/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: -6.19\n",
      "***************************\n",
      "Episode 1424 finished (timesteps: 1984067/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -6.26\n",
      "***************************\n",
      "Episode 1425 finished (timesteps: 1986439/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -6.25\n",
      "***************************\n",
      "Episode 1426 finished (timesteps: 1988497/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -6.27\n",
      "***************************\n",
      "Episode 1427 finished (timesteps: 1991155/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -6.13\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1428 finished (timesteps: 1993283/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -6.12\n",
      "***************************\n",
      "Episode 1429 finished (timesteps: 1995661/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: -5.87\n",
      "***************************\n",
      "Episode 1430 finished (timesteps: 1998044/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -5.77\n",
      "***************************\n",
      "Episode 1431 finished (timesteps: 2000728/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -5.77\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-2000000.mp4.\n",
      "Moviepy - Writing video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-2000000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-2000000.mp4\n",
      "Episode 1432 finished (timesteps: 2002808/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -5.76\n",
      "***************************\n",
      "Episode 1433 finished (timesteps: 2005217/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -5.58\n",
      "***************************\n",
      "Episode 1434 finished (timesteps: 2007777/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -5.49\n",
      "***************************\n",
      "Episode 1435 finished (timesteps: 2010046/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -5.43\n",
      "***************************\n",
      "Episode 1436 finished (timesteps: 2012155/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -5.42\n",
      "***************************\n",
      "Episode 1437 finished (timesteps: 2014639/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: -5.27\n",
      "***************************\n",
      "Episode 1438 finished (timesteps: 2016190/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -5.56\n",
      "***************************\n",
      "Episode 1439 finished (timesteps: 2019038/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: -5.45\n",
      "***************************\n",
      "Episode 1440 finished (timesteps: 2021096/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: -5.18\n",
      "***************************\n",
      "Episode 1441 finished (timesteps: 2022740/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -5.34\n",
      "***************************\n",
      "Episode 1442 finished (timesteps: 2025413/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: -5.30\n",
      "***************************\n",
      "Episode 1443 finished (timesteps: 2027804/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -5.26\n",
      "***************************\n",
      "Episode 1444 finished (timesteps: 2029822/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -5.51\n",
      "***************************\n",
      "Episode 1445 finished (timesteps: 2031632/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -5.49\n",
      "***************************\n",
      "Episode 1446 finished (timesteps: 2034144/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -5.51\n",
      "***************************\n",
      "Episode 1447 finished (timesteps: 2035952/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -5.74\n",
      "***************************\n",
      "Episode 1448 finished (timesteps: 2038302/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -5.72\n",
      "***************************\n",
      "Episode 1449 finished (timesteps: 2040647/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -5.75\n",
      "***************************\n",
      "Episode 1450 finished (timesteps: 2043052/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: -5.60\n",
      "***************************\n",
      "Episode 1451 finished (timesteps: 2045403/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: -5.54\n",
      "***************************\n",
      "Episode 1452 finished (timesteps: 2047130/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -5.59\n",
      "***************************\n",
      "Episode 1453 finished (timesteps: 2048730/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -5.62\n",
      "***************************\n",
      "Episode 1454 finished (timesteps: 2050674/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: -5.42\n",
      "***************************\n",
      "Episode 1455 finished (timesteps: 2052799/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -5.41\n",
      "***************************\n",
      "Episode 1456 finished (timesteps: 2054417/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -5.62\n",
      "***************************\n",
      "Episode 1457 finished (timesteps: 2056054/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -5.67\n",
      "***************************\n",
      "Episode 1458 finished (timesteps: 2057983/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -5.69\n",
      "***************************\n",
      "Episode 1459 finished (timesteps: 2059208/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -5.76\n",
      "***************************\n",
      "Episode 1460 finished (timesteps: 2061661/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: -5.79\n",
      "***************************\n",
      "Episode 1461 finished (timesteps: 2064277/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: -5.78\n",
      "***************************\n",
      "Episode 1462 finished (timesteps: 2066879/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -5.64\n",
      "***************************\n",
      "Episode 1463 finished (timesteps: 2068810/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -5.66\n",
      "***************************\n",
      "Episode 1464 finished (timesteps: 2070848/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -5.66\n",
      "***************************\n",
      "Episode 1465 finished (timesteps: 2073223/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -5.62\n",
      "***************************\n",
      "Episode 1466 finished (timesteps: 2075231/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -5.59\n",
      "***************************\n",
      "Episode 1467 finished (timesteps: 2077465/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -5.84\n",
      "***************************\n",
      "Episode 1468 finished (timesteps: 2080276/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: -5.66\n",
      "***************************\n",
      "Episode 1469 finished (timesteps: 2081974/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -5.93\n",
      "***************************\n",
      "Episode 1470 finished (timesteps: 2083760/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -6.16\n",
      "***************************\n",
      "Episode 1471 finished (timesteps: 2086154/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -6.07\n",
      "***************************\n",
      "Episode 1472 finished (timesteps: 2088512/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -6.00\n",
      "***************************\n",
      "Episode 1473 finished (timesteps: 2091083/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: -5.96\n",
      "***************************\n",
      "Episode 1474 finished (timesteps: 2093428/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -6.12\n",
      "***************************\n",
      "Episode 1475 finished (timesteps: 2094698/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -6.20\n",
      "***************************\n",
      "Episode 1476 finished (timesteps: 2097144/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -6.29\n",
      "***************************\n",
      "Episode 1477 finished (timesteps: 2099313/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -6.40\n",
      "***************************\n",
      "Episode 1478 finished (timesteps: 2101206/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -6.38\n",
      "***************************\n",
      "Episode 1479 finished (timesteps: 2102772/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -6.42\n",
      "***************************\n",
      "Episode 1480 finished (timesteps: 2105199/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -6.54\n",
      "***************************\n",
      "Episode 1481 finished (timesteps: 2107494/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -6.75\n",
      "***************************\n",
      "Episode 1482 finished (timesteps: 2109094/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -6.75\n",
      "***************************\n",
      "Episode 1483 finished (timesteps: 2111715/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: -6.50\n",
      "***************************\n",
      "Episode 1484 finished (timesteps: 2114219/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -6.51\n",
      "***************************\n",
      "Episode 1485 finished (timesteps: 2116925/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -6.60\n",
      "***************************\n",
      "Episode 1486 finished (timesteps: 2118920/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: -6.37\n",
      "***************************\n",
      "Episode 1487 finished (timesteps: 2120610/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -6.37\n",
      "***************************\n",
      "Episode 1488 finished (timesteps: 2122842/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -6.26\n",
      "***************************\n",
      "Episode 1489 finished (timesteps: 2124339/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -6.50\n",
      "***************************\n",
      "Episode 1490 finished (timesteps: 2125960/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -6.50\n",
      "***************************\n",
      "Episode 1491 finished (timesteps: 2128157/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -6.49\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1492 finished (timesteps: 2130232/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -6.67\n",
      "***************************\n",
      "Episode 1493 finished (timesteps: 2133117/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: -6.53\n",
      "***************************\n",
      "Episode 1494 finished (timesteps: 2136111/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: -6.66\n",
      "***************************\n",
      "Episode 1495 finished (timesteps: 2138162/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -6.62\n",
      "***************************\n",
      "Episode 1496 finished (timesteps: 2139924/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -6.64\n",
      "***************************\n",
      "Episode 1497 finished (timesteps: 2141956/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: -6.38\n",
      "***************************\n",
      "Episode 1498 finished (timesteps: 2144496/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -6.34\n",
      "***************************\n",
      "Episode 1499 finished (timesteps: 2146744/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -6.42\n",
      "***************************\n",
      "Episode 1500 finished (timesteps: 2148751/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -6.46\n",
      "Saving checkpoint...\n",
      "Checkpoint saved into runs/checkpoints/CNN_DDQN_Pong-v5_11-04-2023_16:28:30\n",
      "***************************\n",
      "Episode 1501 finished (timesteps: 2150655/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -6.51\n",
      "***************************\n",
      "Episode 1502 finished (timesteps: 2152691/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -6.50\n",
      "***************************\n",
      "Episode 1503 finished (timesteps: 2154860/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -6.44\n",
      "***************************\n",
      "Episode 1504 finished (timesteps: 2156847/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -6.38\n",
      "***************************\n",
      "Episode 1505 finished (timesteps: 2158773/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -6.38\n",
      "***************************\n",
      "Episode 1506 finished (timesteps: 2160518/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -6.41\n",
      "***************************\n",
      "Episode 1507 finished (timesteps: 2162749/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -6.60\n",
      "***************************\n",
      "Episode 1508 finished (timesteps: 2164772/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -6.54\n",
      "***************************\n",
      "Episode 1509 finished (timesteps: 2166348/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -6.71\n",
      "***************************\n",
      "Episode 1510 finished (timesteps: 2168099/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -6.66\n",
      "***************************\n",
      "Episode 1511 finished (timesteps: 2169987/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -6.76\n",
      "***************************\n",
      "Episode 1512 finished (timesteps: 2172704/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: -6.75\n",
      "***************************\n",
      "Episode 1513 finished (timesteps: 2174439/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -6.75\n",
      "***************************\n",
      "Episode 1514 finished (timesteps: 2177011/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -7.04\n",
      "***************************\n",
      "Episode 1515 finished (timesteps: 2178824/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -7.19\n",
      "***************************\n",
      "Episode 1516 finished (timesteps: 2180842/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -7.24\n",
      "***************************\n",
      "Episode 1517 finished (timesteps: 2182735/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -7.25\n",
      "***************************\n",
      "Episode 1518 finished (timesteps: 2184584/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -7.26\n",
      "***************************\n",
      "Episode 1519 finished (timesteps: 2185945/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -7.40\n",
      "***************************\n",
      "Episode 1520 finished (timesteps: 2187994/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -7.34\n",
      "***************************\n",
      "Episode 1521 finished (timesteps: 2190692/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: -7.17\n",
      "***************************\n",
      "Episode 1522 finished (timesteps: 2193200/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: -7.04\n",
      "***************************\n",
      "Episode 1523 finished (timesteps: 2195444/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -7.26\n",
      "***************************\n",
      "Episode 1524 finished (timesteps: 2197717/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -7.27\n",
      "***************************\n",
      "Episode 1525 finished (timesteps: 2200605/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: -7.21\n",
      "***************************\n",
      "Episode 1526 finished (timesteps: 2202857/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: -7.01\n",
      "***************************\n",
      "Episode 1527 finished (timesteps: 2205515/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -7.07\n",
      "***************************\n",
      "Episode 1528 finished (timesteps: 2207735/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -7.06\n",
      "***************************\n",
      "Episode 1529 finished (timesteps: 2208790/10000000)\n",
      "Epsilon: 0.01, Episode reward: -19.0, Mean reward: -7.35\n",
      "***************************\n",
      "Episode 1530 finished (timesteps: 2211469/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -7.39\n",
      "***************************\n",
      "Episode 1531 finished (timesteps: 2213246/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -7.47\n",
      "***************************\n",
      "Episode 1532 finished (timesteps: 2215894/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -7.40\n",
      "***************************\n",
      "Episode 1533 finished (timesteps: 2218483/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: -7.44\n",
      "***************************\n",
      "Episode 1534 finished (timesteps: 2220803/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -7.42\n",
      "***************************\n",
      "Episode 1535 finished (timesteps: 2223000/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -7.42\n",
      "***************************\n",
      "Episode 1536 finished (timesteps: 2224859/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -7.43\n",
      "***************************\n",
      "Episode 1537 finished (timesteps: 2227525/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: -7.46\n",
      "***************************\n",
      "Episode 1538 finished (timesteps: 2230291/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: -7.33\n",
      "***************************\n",
      "Episode 1539 finished (timesteps: 2232197/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -7.45\n",
      "***************************\n",
      "Episode 1540 finished (timesteps: 2234312/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -7.70\n",
      "***************************\n",
      "Episode 1541 finished (timesteps: 2236025/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -7.67\n",
      "***************************\n",
      "Episode 1542 finished (timesteps: 2238355/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -7.73\n",
      "***************************\n",
      "Episode 1543 finished (timesteps: 2240359/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -7.77\n",
      "***************************\n",
      "Episode 1544 finished (timesteps: 2242386/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: -7.55\n",
      "***************************\n",
      "Episode 1545 finished (timesteps: 2245194/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: -7.44\n",
      "***************************\n",
      "Episode 1546 finished (timesteps: 2247403/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -7.51\n",
      "***************************\n",
      "Episode 1547 finished (timesteps: 2249700/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -7.46\n",
      "***************************\n",
      "Episode 1548 finished (timesteps: 2252298/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: -7.35\n",
      "***************************\n",
      "Episode 1549 finished (timesteps: 2254414/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -7.35\n",
      "***************************\n",
      "Episode 1550 finished (timesteps: 2257068/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: -7.38\n",
      "***************************\n",
      "Episode 1551 finished (timesteps: 2259041/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: -7.34\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1552 finished (timesteps: 2260820/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -7.34\n",
      "***************************\n",
      "Episode 1553 finished (timesteps: 2263280/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -7.13\n",
      "***************************\n",
      "Episode 1554 finished (timesteps: 2265271/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -7.38\n",
      "***************************\n",
      "Episode 1555 finished (timesteps: 2267165/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -7.40\n",
      "***************************\n",
      "Episode 1556 finished (timesteps: 2268933/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -7.40\n",
      "***************************\n",
      "Episode 1557 finished (timesteps: 2271533/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -7.19\n",
      "***************************\n",
      "Episode 1558 finished (timesteps: 2273878/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -7.15\n",
      "***************************\n",
      "Episode 1559 finished (timesteps: 2276190/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -7.06\n",
      "***************************\n",
      "Episode 1560 finished (timesteps: 2277689/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -7.22\n",
      "***************************\n",
      "Episode 1561 finished (timesteps: 2280348/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -7.23\n",
      "***************************\n",
      "Episode 1562 finished (timesteps: 2282621/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -7.32\n",
      "***************************\n",
      "Episode 1563 finished (timesteps: 2284587/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -7.28\n",
      "***************************\n",
      "Episode 1564 finished (timesteps: 2286903/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -7.26\n",
      "***************************\n",
      "Episode 1565 finished (timesteps: 2288944/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: -7.10\n",
      "***************************\n",
      "Episode 1566 finished (timesteps: 2290976/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: -6.84\n",
      "***************************\n",
      "Episode 1567 finished (timesteps: 2293468/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -6.77\n",
      "***************************\n",
      "Episode 1568 finished (timesteps: 2295459/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -6.94\n",
      "***************************\n",
      "Episode 1569 finished (timesteps: 2297837/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -6.72\n",
      "***************************\n",
      "Episode 1570 finished (timesteps: 2300743/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -6.60\n",
      "***************************\n",
      "Episode 1571 finished (timesteps: 2302867/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -6.66\n",
      "***************************\n",
      "Episode 1572 finished (timesteps: 2305191/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -6.69\n",
      "***************************\n",
      "Episode 1573 finished (timesteps: 2306840/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -6.83\n",
      "***************************\n",
      "Episode 1574 finished (timesteps: 2308889/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: -6.67\n",
      "***************************\n",
      "Episode 1575 finished (timesteps: 2311031/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: -6.39\n",
      "***************************\n",
      "Episode 1576 finished (timesteps: 2312674/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -6.46\n",
      "***************************\n",
      "Episode 1577 finished (timesteps: 2314391/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -6.50\n",
      "***************************\n",
      "Episode 1578 finished (timesteps: 2316262/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -6.51\n",
      "***************************\n",
      "Episode 1579 finished (timesteps: 2318427/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -6.47\n",
      "***************************\n",
      "Episode 1580 finished (timesteps: 2319984/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -6.55\n",
      "***************************\n",
      "Episode 1581 finished (timesteps: 2321826/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -6.60\n",
      "***************************\n",
      "Episode 1582 finished (timesteps: 2323527/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -6.58\n",
      "***************************\n",
      "Episode 1583 finished (timesteps: 2326008/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -6.72\n",
      "***************************\n",
      "Episode 1584 finished (timesteps: 2328040/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -6.79\n",
      "***************************\n",
      "Episode 1585 finished (timesteps: 2330232/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -6.83\n",
      "***************************\n",
      "Episode 1586 finished (timesteps: 2332229/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -7.09\n",
      "***************************\n",
      "Episode 1587 finished (timesteps: 2334820/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -7.02\n",
      "***************************\n",
      "Episode 1588 finished (timesteps: 2337747/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -7.01\n",
      "***************************\n",
      "Episode 1589 finished (timesteps: 2339856/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -6.99\n",
      "***************************\n",
      "Episode 1590 finished (timesteps: 2342260/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -6.92\n",
      "***************************\n",
      "Episode 1591 finished (timesteps: 2344239/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: -6.69\n",
      "***************************\n",
      "Episode 1592 finished (timesteps: 2345837/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -6.72\n",
      "***************************\n",
      "Episode 1593 finished (timesteps: 2348277/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -6.82\n",
      "***************************\n",
      "Episode 1594 finished (timesteps: 2351259/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -6.86\n",
      "***************************\n",
      "Episode 1595 finished (timesteps: 2354283/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: -6.71\n",
      "***************************\n",
      "Episode 1596 finished (timesteps: 2355953/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -6.69\n",
      "***************************\n",
      "Episode 1597 finished (timesteps: 2359342/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: -6.80\n",
      "***************************\n",
      "Episode 1598 finished (timesteps: 2360722/10000000)\n",
      "Epsilon: 0.01, Episode reward: -19.0, Mean reward: -6.92\n",
      "***************************\n",
      "Episode 1599 finished (timesteps: 2361988/10000000)\n",
      "Epsilon: 0.01, Episode reward: -20.0, Mean reward: -7.02\n",
      "***************************\n",
      "Episode 1600 finished (timesteps: 2365104/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: -6.87\n",
      "***************************\n",
      "Episode 1601 finished (timesteps: 2367481/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -6.86\n",
      "***************************\n",
      "Episode 1602 finished (timesteps: 2371061/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: -6.77\n",
      "***************************\n",
      "Episode 1603 finished (timesteps: 2373004/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -6.84\n",
      "***************************\n",
      "Episode 1604 finished (timesteps: 2375688/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -6.80\n",
      "***************************\n",
      "Episode 1605 finished (timesteps: 2378349/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -6.78\n",
      "***************************\n",
      "Episode 1606 finished (timesteps: 2381203/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: -6.60\n",
      "***************************\n",
      "Episode 1607 finished (timesteps: 2383908/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -6.54\n",
      "***************************\n",
      "Episode 1608 finished (timesteps: 2385688/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: -6.29\n",
      "***************************\n",
      "Episode 1609 finished (timesteps: 2387217/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -6.29\n",
      "***************************\n",
      "Episode 1610 finished (timesteps: 2389812/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -6.24\n",
      "***************************\n",
      "Episode 1611 finished (timesteps: 2392263/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -6.15\n",
      "***************************\n",
      "Episode 1612 finished (timesteps: 2394262/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: -5.99\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1613 finished (timesteps: 2396336/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -5.95\n",
      "***************************\n",
      "Episode 1614 finished (timesteps: 2398235/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -5.97\n",
      "***************************\n",
      "Episode 1615 finished (timesteps: 2400391/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: -5.69\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-2400000.mp4.\n",
      "Moviepy - Writing video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-2400000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-2400000.mp4\n",
      "Episode 1616 finished (timesteps: 2402610/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: -5.51\n",
      "***************************\n",
      "Episode 1617 finished (timesteps: 2404787/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: -5.29\n",
      "***************************\n",
      "Episode 1618 finished (timesteps: 2407243/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -5.24\n",
      "***************************\n",
      "Episode 1619 finished (timesteps: 2409670/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -5.11\n",
      "***************************\n",
      "Episode 1620 finished (timesteps: 2411733/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -5.14\n",
      "***************************\n",
      "Episode 1621 finished (timesteps: 2414755/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: -5.17\n",
      "***************************\n",
      "Episode 1622 finished (timesteps: 2416797/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -5.34\n",
      "***************************\n",
      "Episode 1623 finished (timesteps: 2419953/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -5.27\n",
      "***************************\n",
      "Episode 1624 finished (timesteps: 2421919/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -5.31\n",
      "***************************\n",
      "Episode 1625 finished (timesteps: 2424497/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -5.35\n",
      "***************************\n",
      "Episode 1626 finished (timesteps: 2427386/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -5.50\n",
      "***************************\n",
      "Episode 1627 finished (timesteps: 2430263/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -5.41\n",
      "***************************\n",
      "Episode 1628 finished (timesteps: 2432557/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -5.40\n",
      "***************************\n",
      "Episode 1629 finished (timesteps: 2436086/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -5.24\n",
      "***************************\n",
      "Episode 1630 finished (timesteps: 2437780/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -5.29\n",
      "***************************\n",
      "Episode 1631 finished (timesteps: 2439973/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -5.30\n",
      "***************************\n",
      "Episode 1632 finished (timesteps: 2442725/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: -5.17\n",
      "***************************\n",
      "Episode 1633 finished (timesteps: 2445205/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -5.27\n",
      "***************************\n",
      "Episode 1634 finished (timesteps: 2447229/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -5.34\n",
      "***************************\n",
      "Episode 1635 finished (timesteps: 2449475/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: -5.16\n",
      "***************************\n",
      "Episode 1636 finished (timesteps: 2451465/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -5.15\n",
      "***************************\n",
      "Episode 1637 finished (timesteps: 2453273/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -5.30\n",
      "***************************\n",
      "Episode 1638 finished (timesteps: 2455238/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -5.38\n",
      "***************************\n",
      "Episode 1639 finished (timesteps: 2457337/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -5.36\n",
      "***************************\n",
      "Episode 1640 finished (timesteps: 2459013/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -5.39\n",
      "***************************\n",
      "Episode 1641 finished (timesteps: 2461674/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -5.36\n",
      "***************************\n",
      "Episode 1642 finished (timesteps: 2464200/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -5.41\n",
      "***************************\n",
      "Episode 1643 finished (timesteps: 2465914/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -5.44\n",
      "***************************\n",
      "Episode 1644 finished (timesteps: 2468146/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -5.66\n",
      "***************************\n",
      "Episode 1645 finished (timesteps: 2471040/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -5.76\n",
      "***************************\n",
      "Episode 1646 finished (timesteps: 2473355/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: -5.54\n",
      "***************************\n",
      "Episode 1647 finished (timesteps: 2475886/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -5.58\n",
      "***************************\n",
      "Episode 1648 finished (timesteps: 2477964/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -5.76\n",
      "***************************\n",
      "Episode 1649 finished (timesteps: 2480354/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -5.78\n",
      "***************************\n",
      "Episode 1650 finished (timesteps: 2482616/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -5.86\n",
      "***************************\n",
      "Episode 1651 finished (timesteps: 2484821/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -6.11\n",
      "***************************\n",
      "Episode 1652 finished (timesteps: 2486947/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -6.09\n",
      "***************************\n",
      "Episode 1653 finished (timesteps: 2489066/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -6.24\n",
      "***************************\n",
      "Episode 1654 finished (timesteps: 2491154/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -6.23\n",
      "***************************\n",
      "Episode 1655 finished (timesteps: 2493428/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -6.18\n",
      "***************************\n",
      "Episode 1656 finished (timesteps: 2495638/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -6.15\n",
      "***************************\n",
      "Episode 1657 finished (timesteps: 2497822/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: -6.12\n",
      "***************************\n",
      "Episode 1658 finished (timesteps: 2499904/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -6.16\n",
      "***************************\n",
      "Episode 1659 finished (timesteps: 2501918/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -6.18\n",
      "***************************\n",
      "Episode 1660 finished (timesteps: 2504848/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -6.05\n",
      "***************************\n",
      "Episode 1661 finished (timesteps: 2507438/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -6.17\n",
      "***************************\n",
      "Episode 1662 finished (timesteps: 2509543/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -6.26\n",
      "***************************\n",
      "Episode 1663 finished (timesteps: 2511983/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -6.26\n",
      "***************************\n",
      "Episode 1664 finished (timesteps: 2514884/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -6.13\n",
      "***************************\n",
      "Episode 1665 finished (timesteps: 2517173/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -6.37\n",
      "***************************\n",
      "Episode 1666 finished (timesteps: 2519658/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -6.52\n",
      "***************************\n",
      "Episode 1667 finished (timesteps: 2521864/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: -6.39\n",
      "***************************\n",
      "Episode 1668 finished (timesteps: 2523961/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -6.35\n",
      "***************************\n",
      "Episode 1669 finished (timesteps: 2526251/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: -6.31\n",
      "***************************\n",
      "Episode 1670 finished (timesteps: 2528611/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -6.22\n",
      "***************************\n",
      "Episode 1671 finished (timesteps: 2531376/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -6.06\n",
      "***************************\n",
      "Episode 1672 finished (timesteps: 2534214/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -6.03\n",
      "***************************\n",
      "Episode 1673 finished (timesteps: 2536818/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -5.94\n",
      "***************************\n",
      "Episode 1674 finished (timesteps: 2539307/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: -6.00\n",
      "***************************\n",
      "Episode 1675 finished (timesteps: 2541924/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -6.16\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1676 finished (timesteps: 2544298/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -6.12\n",
      "***************************\n",
      "Episode 1677 finished (timesteps: 2546763/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -6.06\n",
      "***************************\n",
      "Episode 1678 finished (timesteps: 2549154/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -6.07\n",
      "***************************\n",
      "Episode 1679 finished (timesteps: 2552386/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -6.01\n",
      "***************************\n",
      "Episode 1680 finished (timesteps: 2554428/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -5.98\n",
      "***************************\n",
      "Episode 1681 finished (timesteps: 2558329/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: -5.84\n",
      "***************************\n",
      "Episode 1682 finished (timesteps: 2561086/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: -5.73\n",
      "***************************\n",
      "Episode 1683 finished (timesteps: 2563107/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -5.78\n",
      "***************************\n",
      "Episode 1684 finished (timesteps: 2566978/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: -5.67\n",
      "***************************\n",
      "Episode 1685 finished (timesteps: 2570066/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: -5.60\n",
      "***************************\n",
      "Episode 1686 finished (timesteps: 2573783/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: -5.48\n",
      "***************************\n",
      "Episode 1687 finished (timesteps: 2576030/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -5.54\n",
      "***************************\n",
      "Episode 1688 finished (timesteps: 2578576/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -5.59\n",
      "***************************\n",
      "Episode 1689 finished (timesteps: 2581124/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -5.52\n",
      "***************************\n",
      "Episode 1690 finished (timesteps: 2583980/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -5.42\n",
      "***************************\n",
      "Episode 1691 finished (timesteps: 2586059/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -5.66\n",
      "***************************\n",
      "Episode 1692 finished (timesteps: 2588636/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -5.61\n",
      "***************************\n",
      "Episode 1693 finished (timesteps: 2590872/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: -5.43\n",
      "***************************\n",
      "Episode 1694 finished (timesteps: 2593327/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: -5.28\n",
      "***************************\n",
      "Episode 1695 finished (timesteps: 2595389/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -5.40\n",
      "***************************\n",
      "Episode 1696 finished (timesteps: 2598175/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -5.33\n",
      "***************************\n",
      "Episode 1697 finished (timesteps: 2600531/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -5.45\n",
      "***************************\n",
      "Episode 1698 finished (timesteps: 2602592/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: -5.13\n",
      "***************************\n",
      "Episode 1699 finished (timesteps: 2604989/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: -4.83\n",
      "***************************\n",
      "Episode 1700 finished (timesteps: 2608017/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: -4.89\n",
      "***************************\n",
      "Episode 1701 finished (timesteps: 2610181/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -4.88\n",
      "***************************\n",
      "Episode 1702 finished (timesteps: 2612656/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: -4.81\n",
      "***************************\n",
      "Episode 1703 finished (timesteps: 2614844/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -4.74\n",
      "***************************\n",
      "Episode 1704 finished (timesteps: 2617226/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -4.78\n",
      "***************************\n",
      "Episode 1705 finished (timesteps: 2619390/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -4.81\n",
      "***************************\n",
      "Episode 1706 finished (timesteps: 2621856/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: -4.80\n",
      "***************************\n",
      "Episode 1707 finished (timesteps: 2624642/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: -4.77\n",
      "***************************\n",
      "Episode 1708 finished (timesteps: 2627507/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: -4.96\n",
      "***************************\n",
      "Episode 1709 finished (timesteps: 2629766/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -4.86\n",
      "***************************\n",
      "Episode 1710 finished (timesteps: 2632155/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: -4.70\n",
      "***************************\n",
      "Episode 1711 finished (timesteps: 2634681/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -4.72\n",
      "***************************\n",
      "Episode 1712 finished (timesteps: 2636917/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -4.97\n",
      "***************************\n",
      "Episode 1713 finished (timesteps: 2639688/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -4.92\n",
      "***************************\n",
      "Episode 1714 finished (timesteps: 2642789/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -4.76\n",
      "***************************\n",
      "Episode 1715 finished (timesteps: 2645080/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: -4.79\n",
      "***************************\n",
      "Episode 1716 finished (timesteps: 2647313/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: -4.77\n",
      "***************************\n",
      "Episode 1717 finished (timesteps: 2648994/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -5.02\n",
      "***************************\n",
      "Episode 1718 finished (timesteps: 2651019/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: -4.81\n",
      "***************************\n",
      "Episode 1719 finished (timesteps: 2653217/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: -4.64\n",
      "***************************\n",
      "Episode 1720 finished (timesteps: 2655763/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: -4.46\n",
      "***************************\n",
      "Episode 1721 finished (timesteps: 2657462/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: -4.30\n",
      "***************************\n",
      "Episode 1722 finished (timesteps: 2659048/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -4.32\n",
      "***************************\n",
      "Episode 1723 finished (timesteps: 2661422/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -4.35\n",
      "***************************\n",
      "Episode 1724 finished (timesteps: 2663385/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -4.34\n",
      "***************************\n",
      "Episode 1725 finished (timesteps: 2665827/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: -4.24\n",
      "***************************\n",
      "Episode 1726 finished (timesteps: 2668524/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: -4.17\n",
      "***************************\n",
      "Episode 1727 finished (timesteps: 2670194/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -4.37\n",
      "***************************\n",
      "Episode 1728 finished (timesteps: 2671297/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -4.49\n",
      "***************************\n",
      "Episode 1729 finished (timesteps: 2673714/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -4.54\n",
      "***************************\n",
      "Episode 1730 finished (timesteps: 2675728/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -4.51\n",
      "***************************\n",
      "Episode 1731 finished (timesteps: 2677690/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -4.44\n",
      "***************************\n",
      "Episode 1732 finished (timesteps: 2680037/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -4.60\n",
      "***************************\n",
      "Episode 1733 finished (timesteps: 2682409/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -4.62\n",
      "***************************\n",
      "Episode 1734 finished (timesteps: 2684724/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: -4.39\n",
      "***************************\n",
      "Episode 1735 finished (timesteps: 2687056/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: -4.41\n",
      "***************************\n",
      "Episode 1736 finished (timesteps: 2689904/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: -4.28\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1737 finished (timesteps: 2692165/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -4.24\n",
      "***************************\n",
      "Episode 1738 finished (timesteps: 2694432/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -4.23\n",
      "***************************\n",
      "Episode 1739 finished (timesteps: 2696424/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -4.27\n",
      "***************************\n",
      "Episode 1740 finished (timesteps: 2698979/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: -4.09\n",
      "***************************\n",
      "Episode 1741 finished (timesteps: 2701720/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: -4.02\n",
      "***************************\n",
      "Episode 1742 finished (timesteps: 2703944/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -4.04\n",
      "***************************\n",
      "Episode 1743 finished (timesteps: 2706750/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -3.93\n",
      "***************************\n",
      "Episode 1744 finished (timesteps: 2708725/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: -3.68\n",
      "***************************\n",
      "Episode 1745 finished (timesteps: 2711157/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -3.66\n",
      "***************************\n",
      "Episode 1746 finished (timesteps: 2713287/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: -3.66\n",
      "***************************\n",
      "Episode 1747 finished (timesteps: 2715548/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: -3.49\n",
      "***************************\n",
      "Episode 1748 finished (timesteps: 2717153/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -3.51\n",
      "***************************\n",
      "Episode 1749 finished (timesteps: 2719495/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: -3.36\n",
      "***************************\n",
      "Episode 1750 finished (timesteps: 2721791/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: -3.17\n",
      "***************************\n",
      "Episode 1751 finished (timesteps: 2723915/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -3.13\n",
      "***************************\n",
      "Episode 1752 finished (timesteps: 2726397/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -3.10\n",
      "***************************\n",
      "Episode 1753 finished (timesteps: 2728251/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -3.15\n",
      "***************************\n",
      "Episode 1754 finished (timesteps: 2731016/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: -3.00\n",
      "***************************\n",
      "Episode 1755 finished (timesteps: 2732835/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -3.05\n",
      "***************************\n",
      "Episode 1756 finished (timesteps: 2735247/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -3.01\n",
      "***************************\n",
      "Episode 1757 finished (timesteps: 2737400/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -3.20\n",
      "***************************\n",
      "Episode 1758 finished (timesteps: 2739821/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -3.03\n",
      "***************************\n",
      "Episode 1759 finished (timesteps: 2741113/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -3.09\n",
      "***************************\n",
      "Episode 1760 finished (timesteps: 2742882/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -3.16\n",
      "***************************\n",
      "Episode 1761 finished (timesteps: 2744222/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -3.24\n",
      "***************************\n",
      "Episode 1762 finished (timesteps: 2746173/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -3.19\n",
      "***************************\n",
      "Episode 1763 finished (timesteps: 2747844/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -3.23\n",
      "***************************\n",
      "Episode 1764 finished (timesteps: 2749409/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -3.40\n",
      "***************************\n",
      "Episode 1765 finished (timesteps: 2751585/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -3.37\n",
      "***************************\n",
      "Episode 1766 finished (timesteps: 2753771/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -3.41\n",
      "***************************\n",
      "Episode 1767 finished (timesteps: 2755363/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -3.63\n",
      "***************************\n",
      "Episode 1768 finished (timesteps: 2756983/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -3.66\n",
      "***************************\n",
      "Episode 1769 finished (timesteps: 2758713/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -3.91\n",
      "***************************\n",
      "Episode 1770 finished (timesteps: 2760214/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -4.13\n",
      "***************************\n",
      "Episode 1771 finished (timesteps: 2761728/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -4.36\n",
      "***************************\n",
      "Episode 1772 finished (timesteps: 2763417/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -4.42\n",
      "***************************\n",
      "Episode 1773 finished (timesteps: 2764867/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -4.52\n",
      "***************************\n",
      "Episode 1774 finished (timesteps: 2766807/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -4.68\n",
      "***************************\n",
      "Episode 1775 finished (timesteps: 2768733/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -4.71\n",
      "***************************\n",
      "Episode 1776 finished (timesteps: 2770518/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -4.73\n",
      "***************************\n",
      "Episode 1777 finished (timesteps: 2772862/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: -4.58\n",
      "***************************\n",
      "Episode 1778 finished (timesteps: 2774750/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -4.59\n",
      "***************************\n",
      "Episode 1779 finished (timesteps: 2777292/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: -4.51\n",
      "***************************\n",
      "Episode 1780 finished (timesteps: 2779293/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -4.53\n",
      "***************************\n",
      "Episode 1781 finished (timesteps: 2782108/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -4.60\n",
      "***************************\n",
      "Episode 1782 finished (timesteps: 2783787/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -4.73\n",
      "***************************\n",
      "Episode 1783 finished (timesteps: 2785692/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -4.74\n",
      "***************************\n",
      "Episode 1784 finished (timesteps: 2788039/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: -4.65\n",
      "***************************\n",
      "Episode 1785 finished (timesteps: 2790516/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -4.60\n",
      "***************************\n",
      "Episode 1786 finished (timesteps: 2792774/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: -4.48\n",
      "***************************\n",
      "Episode 1787 finished (timesteps: 2795114/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -4.30\n",
      "***************************\n",
      "Episode 1788 finished (timesteps: 2797655/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -4.19\n",
      "***************************\n",
      "Episode 1789 finished (timesteps: 2799797/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -4.26\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-2800000.mp4.\n",
      "Moviepy - Writing video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-2800000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-2800000.mp4\n",
      "Episode 1790 finished (timesteps: 2802779/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: -4.30\n",
      "***************************\n",
      "Episode 1791 finished (timesteps: 2805267/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -4.31\n",
      "***************************\n",
      "Episode 1792 finished (timesteps: 2807308/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -4.33\n",
      "***************************\n",
      "Episode 1793 finished (timesteps: 2809163/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -4.55\n",
      "***************************\n",
      "Episode 1794 finished (timesteps: 2811579/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -4.75\n",
      "***************************\n",
      "Episode 1795 finished (timesteps: 2814408/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -4.73\n",
      "***************************\n",
      "Episode 1796 finished (timesteps: 2816459/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -4.75\n",
      "***************************\n",
      "Episode 1797 finished (timesteps: 2819011/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -4.71\n",
      "***************************\n",
      "Episode 1798 finished (timesteps: 2821159/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -4.96\n",
      "***************************\n",
      "Episode 1799 finished (timesteps: 2823554/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -5.13\n",
      "***************************\n",
      "Episode 1800 finished (timesteps: 2826520/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: -5.10\n",
      "***************************\n",
      "Episode 1801 finished (timesteps: 2828893/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -5.06\n",
      "***************************\n",
      "Episode 1802 finished (timesteps: 2830938/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -5.22\n",
      "***************************\n",
      "Episode 1803 finished (timesteps: 2833570/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -5.08\n",
      "***************************\n",
      "Episode 1804 finished (timesteps: 2836162/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: -4.89\n",
      "***************************\n",
      "Episode 1805 finished (timesteps: 2838328/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: -4.66\n",
      "***************************\n",
      "Episode 1806 finished (timesteps: 2840459/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: -4.59\n",
      "***************************\n",
      "Episode 1807 finished (timesteps: 2842940/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -4.65\n",
      "***************************\n",
      "Episode 1808 finished (timesteps: 2845207/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -4.74\n",
      "***************************\n",
      "Episode 1809 finished (timesteps: 2846891/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -4.81\n",
      "***************************\n",
      "Episode 1810 finished (timesteps: 2848290/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -5.04\n",
      "***************************\n",
      "Episode 1811 finished (timesteps: 2850002/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -5.12\n",
      "***************************\n",
      "Episode 1812 finished (timesteps: 2852649/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -5.05\n",
      "***************************\n",
      "Episode 1813 finished (timesteps: 2855185/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: -5.00\n",
      "***************************\n",
      "Episode 1814 finished (timesteps: 2857094/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -5.15\n",
      "***************************\n",
      "Episode 1815 finished (timesteps: 2859994/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: -5.24\n",
      "***************************\n",
      "Episode 1816 finished (timesteps: 2862671/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: -5.37\n",
      "***************************\n",
      "Episode 1817 finished (timesteps: 2864455/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -5.33\n",
      "***************************\n",
      "Episode 1818 finished (timesteps: 2866264/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -5.58\n",
      "***************************\n",
      "Episode 1819 finished (timesteps: 2868338/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -5.81\n",
      "***************************\n",
      "Episode 1820 finished (timesteps: 2870861/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: -5.80\n",
      "***************************\n",
      "Episode 1821 finished (timesteps: 2873108/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -6.06\n",
      "***************************\n",
      "Episode 1822 finished (timesteps: 2875542/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -5.95\n",
      "***************************\n",
      "Episode 1823 finished (timesteps: 2878667/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: -5.86\n",
      "***************************\n",
      "Episode 1824 finished (timesteps: 2881382/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -5.78\n",
      "***************************\n",
      "Episode 1825 finished (timesteps: 2883154/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -6.00\n",
      "***************************\n",
      "Episode 1826 finished (timesteps: 2885253/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -6.16\n",
      "***************************\n",
      "Episode 1827 finished (timesteps: 2887170/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -6.15\n",
      "***************************\n",
      "Episode 1828 finished (timesteps: 2889056/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -6.10\n",
      "***************************\n",
      "Episode 1829 finished (timesteps: 2891284/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -6.10\n",
      "***************************\n",
      "Episode 1830 finished (timesteps: 2894141/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: -5.96\n",
      "***************************\n",
      "Episode 1831 finished (timesteps: 2896340/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -5.96\n",
      "***************************\n",
      "Episode 1832 finished (timesteps: 2899084/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: -5.89\n",
      "***************************\n",
      "Episode 1833 finished (timesteps: 2901028/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -5.92\n",
      "***************************\n",
      "Episode 1834 finished (timesteps: 2903525/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -6.09\n",
      "***************************\n",
      "Episode 1835 finished (timesteps: 2905960/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -6.26\n",
      "***************************\n",
      "Episode 1836 finished (timesteps: 2907951/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -6.39\n",
      "***************************\n",
      "Episode 1837 finished (timesteps: 2910581/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -6.38\n",
      "***************************\n",
      "Episode 1838 finished (timesteps: 2913392/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: -6.26\n",
      "***************************\n",
      "Episode 1839 finished (timesteps: 2915846/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -6.17\n",
      "***************************\n",
      "Episode 1840 finished (timesteps: 2918464/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: -6.16\n",
      "***************************\n",
      "Episode 1841 finished (timesteps: 2920749/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -6.24\n",
      "***************************\n",
      "Episode 1842 finished (timesteps: 2922665/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -6.26\n",
      "***************************\n",
      "Episode 1843 finished (timesteps: 2925335/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -6.20\n",
      "***************************\n",
      "Episode 1844 finished (timesteps: 2927407/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -6.48\n",
      "***************************\n",
      "Episode 1845 finished (timesteps: 2929814/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -6.50\n",
      "***************************\n",
      "Episode 1846 finished (timesteps: 2931836/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: -6.51\n",
      "***************************\n",
      "Episode 1847 finished (timesteps: 2934196/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -6.68\n",
      "***************************\n",
      "Episode 1848 finished (timesteps: 2936355/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -6.63\n",
      "***************************\n",
      "Episode 1849 finished (timesteps: 2939022/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: -6.70\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1850 finished (timesteps: 2941218/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -6.86\n",
      "***************************\n",
      "Episode 1851 finished (timesteps: 2942835/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -6.91\n",
      "***************************\n",
      "Episode 1852 finished (timesteps: 2945759/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: -6.82\n",
      "***************************\n",
      "Episode 1853 finished (timesteps: 2947794/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: -6.54\n",
      "***************************\n",
      "Episode 1854 finished (timesteps: 2949985/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -6.69\n",
      "***************************\n",
      "Episode 1855 finished (timesteps: 2952468/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -6.64\n",
      "***************************\n",
      "Episode 1856 finished (timesteps: 2955123/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -6.64\n",
      "***************************\n",
      "Episode 1857 finished (timesteps: 2957427/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -6.64\n",
      "***************************\n",
      "Episode 1858 finished (timesteps: 2960144/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: -6.63\n",
      "***************************\n",
      "Episode 1859 finished (timesteps: 2962960/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -6.50\n",
      "***************************\n",
      "Episode 1860 finished (timesteps: 2964799/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -6.51\n",
      "***************************\n",
      "Episode 1861 finished (timesteps: 2967058/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -6.44\n",
      "***************************\n",
      "Episode 1862 finished (timesteps: 2968408/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -6.50\n",
      "***************************\n",
      "Episode 1863 finished (timesteps: 2969692/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -6.54\n",
      "***************************\n",
      "Episode 1864 finished (timesteps: 2971318/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -6.55\n",
      "***************************\n",
      "Episode 1865 finished (timesteps: 2973339/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -6.58\n",
      "***************************\n",
      "Episode 1866 finished (timesteps: 2975523/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -6.59\n",
      "***************************\n",
      "Episode 1867 finished (timesteps: 2977698/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -6.57\n",
      "***************************\n",
      "Episode 1868 finished (timesteps: 2979869/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -6.52\n",
      "***************************\n",
      "Episode 1869 finished (timesteps: 2982201/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -6.45\n",
      "***************************\n",
      "Episode 1870 finished (timesteps: 2984699/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -6.35\n",
      "***************************\n",
      "Episode 1871 finished (timesteps: 2987832/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: -6.19\n",
      "***************************\n",
      "Episode 1872 finished (timesteps: 2989942/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -6.16\n",
      "***************************\n",
      "Episode 1873 finished (timesteps: 2991694/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -6.12\n",
      "***************************\n",
      "Episode 1874 finished (timesteps: 2993822/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: -5.89\n",
      "***************************\n",
      "Episode 1875 finished (timesteps: 2995683/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -5.92\n",
      "***************************\n",
      "Episode 1876 finished (timesteps: 2997892/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -5.87\n",
      "***************************\n",
      "Episode 1877 finished (timesteps: 3000154/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -6.01\n",
      "***************************\n",
      "Episode 1878 finished (timesteps: 3002521/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -5.95\n",
      "***************************\n",
      "Episode 1879 finished (timesteps: 3005239/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -6.03\n",
      "***************************\n",
      "Episode 1880 finished (timesteps: 3007264/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -6.02\n",
      "***************************\n",
      "Episode 1881 finished (timesteps: 3009426/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -6.10\n",
      "***************************\n",
      "Episode 1882 finished (timesteps: 3010785/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -6.11\n",
      "***************************\n",
      "Episode 1883 finished (timesteps: 3012381/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -6.13\n",
      "***************************\n",
      "Episode 1884 finished (timesteps: 3014586/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -6.25\n",
      "***************************\n",
      "Episode 1885 finished (timesteps: 3016443/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -6.41\n",
      "***************************\n",
      "Episode 1886 finished (timesteps: 3018778/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -6.59\n",
      "***************************\n",
      "Episode 1887 finished (timesteps: 3021038/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: -6.58\n",
      "***************************\n",
      "Episode 1888 finished (timesteps: 3023002/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: -6.48\n",
      "***************************\n",
      "Episode 1889 finished (timesteps: 3025278/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -6.43\n",
      "***************************\n",
      "Episode 1890 finished (timesteps: 3026543/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -6.59\n",
      "***************************\n",
      "Episode 1891 finished (timesteps: 3029086/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -6.44\n",
      "***************************\n",
      "Episode 1892 finished (timesteps: 3031322/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -6.39\n",
      "***************************\n",
      "Episode 1893 finished (timesteps: 3033621/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -6.33\n",
      "***************************\n",
      "Episode 1894 finished (timesteps: 3035681/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -6.36\n",
      "***************************\n",
      "Episode 1895 finished (timesteps: 3038150/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: -6.23\n",
      "***************************\n",
      "Episode 1896 finished (timesteps: 3040019/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: -5.99\n",
      "***************************\n",
      "Episode 1897 finished (timesteps: 3042259/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -6.00\n",
      "***************************\n",
      "Episode 1898 finished (timesteps: 3043165/10000000)\n",
      "Epsilon: 0.01, Episode reward: -20.0, Mean reward: -6.08\n",
      "***************************\n",
      "Episode 1899 finished (timesteps: 3043944/10000000)\n",
      "Epsilon: 0.01, Episode reward: -21.0, Mean reward: -6.22\n",
      "***************************\n",
      "Episode 1900 finished (timesteps: 3046468/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: -6.18\n",
      "***************************\n",
      "Episode 1901 finished (timesteps: 3048521/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -6.23\n",
      "***************************\n",
      "Episode 1902 finished (timesteps: 3050992/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -6.19\n",
      "***************************\n",
      "Episode 1903 finished (timesteps: 3052967/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -6.38\n",
      "***************************\n",
      "Episode 1904 finished (timesteps: 3055769/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: -6.46\n",
      "***************************\n",
      "Episode 1905 finished (timesteps: 3057403/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -6.71\n",
      "***************************\n",
      "Episode 1906 finished (timesteps: 3060078/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: -6.85\n",
      "***************************\n",
      "Episode 1907 finished (timesteps: 3062742/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: -6.79\n",
      "***************************\n",
      "Episode 1908 finished (timesteps: 3065198/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -6.73\n",
      "***************************\n",
      "Episode 1909 finished (timesteps: 3067492/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -6.68\n",
      "***************************\n",
      "Episode 1910 finished (timesteps: 3070298/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: -6.51\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1911 finished (timesteps: 3072808/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -6.33\n",
      "***************************\n",
      "Episode 1912 finished (timesteps: 3075161/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -6.36\n",
      "***************************\n",
      "Episode 1913 finished (timesteps: 3078190/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: -6.33\n",
      "***************************\n",
      "Episode 1914 finished (timesteps: 3080669/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -6.15\n",
      "***************************\n",
      "Episode 1915 finished (timesteps: 3083195/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: -6.07\n",
      "***************************\n",
      "Episode 1916 finished (timesteps: 3085604/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -6.02\n",
      "***************************\n",
      "Episode 1917 finished (timesteps: 3086588/10000000)\n",
      "Epsilon: 0.01, Episode reward: -20.0, Mean reward: -6.10\n",
      "***************************\n",
      "Episode 1918 finished (timesteps: 3087824/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -6.15\n",
      "***************************\n",
      "Episode 1919 finished (timesteps: 3089001/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -6.21\n",
      "***************************\n",
      "Episode 1920 finished (timesteps: 3090679/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -6.40\n",
      "***************************\n",
      "Episode 1921 finished (timesteps: 3093392/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: -6.33\n",
      "***************************\n",
      "Episode 1922 finished (timesteps: 3096075/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: -6.30\n",
      "***************************\n",
      "Episode 1923 finished (timesteps: 3098829/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -6.36\n",
      "***************************\n",
      "Episode 1924 finished (timesteps: 3101239/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -6.34\n",
      "***************************\n",
      "Episode 1925 finished (timesteps: 3102972/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -6.29\n",
      "***************************\n",
      "Episode 1926 finished (timesteps: 3105154/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -6.26\n",
      "***************************\n",
      "Episode 1927 finished (timesteps: 3107553/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -6.23\n",
      "***************************\n",
      "Episode 1928 finished (timesteps: 3109855/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -6.19\n",
      "***************************\n",
      "Episode 1929 finished (timesteps: 3112032/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -6.20\n",
      "***************************\n",
      "Episode 1930 finished (timesteps: 3113089/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -6.39\n",
      "***************************\n",
      "Episode 1931 finished (timesteps: 3115581/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: -6.24\n",
      "***************************\n",
      "Episode 1932 finished (timesteps: 3118004/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -6.31\n",
      "***************************\n",
      "Episode 1933 finished (timesteps: 3120174/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -6.27\n",
      "***************************\n",
      "Episode 1934 finished (timesteps: 3122714/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -6.24\n",
      "***************************\n",
      "Episode 1935 finished (timesteps: 3125001/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: -6.06\n",
      "***************************\n",
      "Episode 1936 finished (timesteps: 3127067/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -6.04\n",
      "***************************\n",
      "Episode 1937 finished (timesteps: 3128359/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -6.12\n",
      "***************************\n",
      "Episode 1938 finished (timesteps: 3129542/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -6.32\n",
      "***************************\n",
      "Episode 1939 finished (timesteps: 3132092/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: -6.27\n",
      "***************************\n",
      "Episode 1940 finished (timesteps: 3134619/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -6.26\n",
      "***************************\n",
      "Episode 1941 finished (timesteps: 3136708/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -6.28\n",
      "***************************\n",
      "Episode 1942 finished (timesteps: 3139146/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: -6.08\n",
      "***************************\n",
      "Episode 1943 finished (timesteps: 3141175/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -6.22\n",
      "***************************\n",
      "Episode 1944 finished (timesteps: 3143808/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -6.12\n",
      "***************************\n",
      "Episode 1945 finished (timesteps: 3146075/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: -5.95\n",
      "***************************\n",
      "Episode 1946 finished (timesteps: 3148945/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: -6.05\n",
      "***************************\n",
      "Episode 1947 finished (timesteps: 3150804/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -6.13\n",
      "***************************\n",
      "Episode 1948 finished (timesteps: 3153280/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: -6.05\n",
      "***************************\n",
      "Episode 1949 finished (timesteps: 3155693/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -6.06\n",
      "***************************\n",
      "Episode 1950 finished (timesteps: 3157812/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -6.08\n",
      "***************************\n",
      "Episode 1951 finished (timesteps: 3159960/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: -5.84\n",
      "***************************\n",
      "Episode 1952 finished (timesteps: 3162241/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -5.92\n",
      "***************************\n",
      "Episode 1953 finished (timesteps: 3164955/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -6.12\n",
      "***************************\n",
      "Episode 1954 finished (timesteps: 3166828/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -6.12\n",
      "***************************\n",
      "Episode 1955 finished (timesteps: 3168961/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -6.14\n",
      "***************************\n",
      "Episode 1956 finished (timesteps: 3170480/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -6.22\n",
      "***************************\n",
      "Episode 1957 finished (timesteps: 3171980/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -6.29\n",
      "***************************\n",
      "Episode 1958 finished (timesteps: 3173582/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -6.50\n",
      "***************************\n",
      "Episode 1959 finished (timesteps: 3175724/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -6.55\n",
      "***************************\n",
      "Episode 1960 finished (timesteps: 3178444/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -6.40\n",
      "***************************\n",
      "Episode 1961 finished (timesteps: 3180981/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -6.36\n",
      "***************************\n",
      "Episode 1962 finished (timesteps: 3182750/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -6.33\n",
      "***************************\n",
      "Episode 1963 finished (timesteps: 3184785/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -6.29\n",
      "***************************\n",
      "Episode 1964 finished (timesteps: 3187824/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: -6.16\n",
      "***************************\n",
      "Episode 1965 finished (timesteps: 3190741/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: -6.05\n",
      "***************************\n",
      "Episode 1966 finished (timesteps: 3192632/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: -5.80\n",
      "***************************\n",
      "Episode 1967 finished (timesteps: 3194686/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: -5.58\n",
      "***************************\n",
      "Episode 1968 finished (timesteps: 3196418/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -5.63\n",
      "***************************\n",
      "Episode 1969 finished (timesteps: 3199075/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: -5.56\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-3200000.mp4.\n",
      "Moviepy - Writing video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-3200000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-3200000.mp4\n",
      "Episode 1970 finished (timesteps: 3201358/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -5.58\n",
      "***************************\n",
      "Episode 1971 finished (timesteps: 3203399/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: -5.45\n",
      "***************************\n",
      "Episode 1972 finished (timesteps: 3206040/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -5.41\n",
      "***************************\n",
      "Episode 1973 finished (timesteps: 3208393/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -5.38\n",
      "***************************\n",
      "Episode 1974 finished (timesteps: 3210671/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: -5.43\n",
      "***************************\n",
      "Episode 1975 finished (timesteps: 3213814/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: -5.29\n",
      "***************************\n",
      "Episode 1976 finished (timesteps: 3216072/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -5.32\n",
      "***************************\n",
      "Episode 1977 finished (timesteps: 3218622/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -5.28\n",
      "***************************\n",
      "Episode 1978 finished (timesteps: 3221893/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -5.18\n",
      "***************************\n",
      "Episode 1979 finished (timesteps: 3224585/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: -5.05\n",
      "***************************\n",
      "Episode 1980 finished (timesteps: 3227150/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: -4.82\n",
      "***************************\n",
      "Episode 1981 finished (timesteps: 3229904/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -4.76\n",
      "***************************\n",
      "Episode 1982 finished (timesteps: 3232495/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -4.65\n",
      "***************************\n",
      "Episode 1983 finished (timesteps: 3235025/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -4.56\n",
      "***************************\n",
      "Episode 1984 finished (timesteps: 3237669/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -4.58\n",
      "***************************\n",
      "Episode 1985 finished (timesteps: 3239494/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: -4.29\n",
      "***************************\n",
      "Episode 1986 finished (timesteps: 3241640/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -4.34\n",
      "***************************\n",
      "Episode 1987 finished (timesteps: 3243768/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -4.51\n",
      "***************************\n",
      "Episode 1988 finished (timesteps: 3245785/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -4.75\n",
      "***************************\n",
      "Episode 1989 finished (timesteps: 3248260/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -4.73\n",
      "***************************\n",
      "Episode 1990 finished (timesteps: 3250730/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -4.63\n",
      "***************************\n",
      "Episode 1991 finished (timesteps: 3253377/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -4.60\n",
      "***************************\n",
      "Episode 1992 finished (timesteps: 3255512/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -4.61\n",
      "***************************\n",
      "Episode 1993 finished (timesteps: 3258643/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: -4.56\n",
      "***************************\n",
      "Episode 1994 finished (timesteps: 3261042/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: -4.30\n",
      "***************************\n",
      "Episode 1995 finished (timesteps: 3263453/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -4.39\n",
      "***************************\n",
      "Episode 1996 finished (timesteps: 3266238/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: -4.44\n",
      "***************************\n",
      "Episode 1997 finished (timesteps: 3269194/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -4.31\n",
      "***************************\n",
      "Episode 1998 finished (timesteps: 3272017/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -4.15\n",
      "***************************\n",
      "Episode 1999 finished (timesteps: 3274660/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -3.88\n",
      "***************************\n",
      "Episode 2000 finished (timesteps: 3277453/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -3.87\n",
      "Saving checkpoint...\n",
      "Checkpoint saved into runs/checkpoints/CNN_DDQN_Pong-v5_11-04-2023_16:28:30\n",
      "***************************\n",
      "Episode 2001 finished (timesteps: 3280037/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: -3.67\n",
      "***************************\n",
      "Episode 2002 finished (timesteps: 3283425/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: -3.58\n",
      "***************************\n",
      "Episode 2003 finished (timesteps: 3285658/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -3.55\n",
      "***************************\n",
      "Episode 2004 finished (timesteps: 3288486/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: -3.47\n",
      "***************************\n",
      "Episode 2005 finished (timesteps: 3290644/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -3.47\n",
      "***************************\n",
      "Episode 2006 finished (timesteps: 3293000/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: -3.40\n",
      "***************************\n",
      "Episode 2007 finished (timesteps: 3295397/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -3.45\n",
      "***************************\n",
      "Episode 2008 finished (timesteps: 3297966/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: -3.30\n",
      "***************************\n",
      "Episode 2009 finished (timesteps: 3300674/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -3.27\n",
      "***************************\n",
      "Episode 2010 finished (timesteps: 3303206/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -3.23\n",
      "***************************\n",
      "Episode 2011 finished (timesteps: 3305508/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -3.34\n",
      "***************************\n",
      "Episode 2012 finished (timesteps: 3306732/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -3.44\n",
      "***************************\n",
      "Episode 2013 finished (timesteps: 3309108/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -3.55\n",
      "***************************\n",
      "Episode 2014 finished (timesteps: 3312669/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: -3.63\n",
      "***************************\n",
      "Episode 2015 finished (timesteps: 3315721/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -3.73\n",
      "***************************\n",
      "Episode 2016 finished (timesteps: 3318295/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -3.86\n",
      "***************************\n",
      "Episode 2017 finished (timesteps: 3321693/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -3.70\n",
      "***************************\n",
      "Episode 2018 finished (timesteps: 3323833/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -3.65\n",
      "***************************\n",
      "Episode 2019 finished (timesteps: 3324729/10000000)\n",
      "Epsilon: 0.01, Episode reward: -20.0, Mean reward: -3.68\n",
      "***************************\n",
      "Episode 2020 finished (timesteps: 3326901/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -3.65\n",
      "***************************\n",
      "Episode 2021 finished (timesteps: 3330217/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: -3.61\n",
      "***************************\n",
      "Episode 2022 finished (timesteps: 3331609/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: -3.78\n",
      "***************************\n",
      "Episode 2023 finished (timesteps: 3334470/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: -3.65\n",
      "***************************\n",
      "Episode 2024 finished (timesteps: 3338067/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -3.68\n",
      "***************************\n",
      "Episode 2025 finished (timesteps: 3342033/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -3.63\n",
      "***************************\n",
      "Episode 2026 finished (timesteps: 3345992/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: -3.50\n",
      "***************************\n",
      "Episode 2027 finished (timesteps: 3348799/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: -3.28\n",
      "***************************\n",
      "Episode 2028 finished (timesteps: 3351378/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: -3.07\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2029 finished (timesteps: 3353614/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -3.09\n",
      "***************************\n",
      "Episode 2030 finished (timesteps: 3356694/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -2.85\n",
      "***************************\n",
      "Episode 2031 finished (timesteps: 3359763/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -2.95\n",
      "***************************\n",
      "Episode 2032 finished (timesteps: 3361773/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: -2.71\n",
      "***************************\n",
      "Episode 2033 finished (timesteps: 3364693/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: -2.60\n",
      "***************************\n",
      "Episode 2034 finished (timesteps: 3366570/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -2.69\n",
      "***************************\n",
      "Episode 2035 finished (timesteps: 3368723/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: -2.67\n",
      "***************************\n",
      "Episode 2036 finished (timesteps: 3371172/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -2.67\n",
      "***************************\n",
      "Episode 2037 finished (timesteps: 3373007/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -2.62\n",
      "***************************\n",
      "Episode 2038 finished (timesteps: 3374817/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -2.60\n",
      "***************************\n",
      "Episode 2039 finished (timesteps: 3377141/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: -2.51\n",
      "***************************\n",
      "Episode 2040 finished (timesteps: 3379804/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -2.65\n",
      "***************************\n",
      "Episode 2041 finished (timesteps: 3382580/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: -2.52\n",
      "***************************\n",
      "Episode 2042 finished (timesteps: 3385774/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -2.56\n",
      "***************************\n",
      "Episode 2043 finished (timesteps: 3388525/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: -2.33\n",
      "***************************\n",
      "Episode 2044 finished (timesteps: 3391717/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: -2.30\n",
      "***************************\n",
      "Episode 2045 finished (timesteps: 3394659/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -2.41\n",
      "***************************\n",
      "Episode 2046 finished (timesteps: 3397502/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -2.46\n",
      "***************************\n",
      "Episode 2047 finished (timesteps: 3399877/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: -2.15\n",
      "***************************\n",
      "Episode 2048 finished (timesteps: 3403522/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: -2.15\n",
      "***************************\n",
      "Episode 2049 finished (timesteps: 3406035/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: -2.00\n",
      "***************************\n",
      "Episode 2050 finished (timesteps: 3408995/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: -1.82\n",
      "***************************\n",
      "Episode 2051 finished (timesteps: 3411072/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: -1.98\n",
      "***************************\n",
      "Episode 2052 finished (timesteps: 3413817/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: -1.87\n",
      "***************************\n",
      "Episode 2053 finished (timesteps: 3416705/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -1.89\n",
      "***************************\n",
      "Episode 2054 finished (timesteps: 3419918/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: -1.70\n",
      "***************************\n",
      "Episode 2055 finished (timesteps: 3423131/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: -1.53\n",
      "***************************\n",
      "Episode 2056 finished (timesteps: 3426827/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: -1.30\n",
      "***************************\n",
      "Episode 2057 finished (timesteps: 3429640/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -1.18\n",
      "***************************\n",
      "Episode 2058 finished (timesteps: 3431708/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -1.15\n",
      "***************************\n",
      "Episode 2059 finished (timesteps: 3433507/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -1.21\n",
      "***************************\n",
      "Episode 2060 finished (timesteps: 3436559/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -1.28\n",
      "***************************\n",
      "Episode 2061 finished (timesteps: 3439802/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -1.19\n",
      "***************************\n",
      "Episode 2062 finished (timesteps: 3443440/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: -1.05\n",
      "***************************\n",
      "Episode 2063 finished (timesteps: 3446907/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: -0.85\n",
      "***************************\n",
      "Episode 2064 finished (timesteps: 3450874/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: -0.84\n",
      "***************************\n",
      "Episode 2065 finished (timesteps: 3454232/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: -0.81\n",
      "***************************\n",
      "Episode 2066 finished (timesteps: 3457483/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: -0.92\n",
      "***************************\n",
      "Episode 2067 finished (timesteps: 3460342/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: -1.00\n",
      "***************************\n",
      "Episode 2068 finished (timesteps: 3463260/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: -0.75\n",
      "***************************\n",
      "Episode 2069 finished (timesteps: 3466870/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: -0.69\n",
      "***************************\n",
      "Episode 2070 finished (timesteps: 3469320/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: -0.53\n",
      "***************************\n",
      "Episode 2071 finished (timesteps: 3473059/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -0.72\n",
      "***************************\n",
      "Episode 2072 finished (timesteps: 3475214/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -0.77\n",
      "***************************\n",
      "Episode 2073 finished (timesteps: 3478287/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: -0.67\n",
      "***************************\n",
      "Episode 2074 finished (timesteps: 3481579/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -0.71\n",
      "***************************\n",
      "Episode 2075 finished (timesteps: 3483865/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: -0.62\n",
      "***************************\n",
      "Episode 2076 finished (timesteps: 3487306/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -0.56\n",
      "***************************\n",
      "Episode 2077 finished (timesteps: 3489626/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -0.65\n",
      "***************************\n",
      "Episode 2078 finished (timesteps: 3493236/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -0.65\n",
      "***************************\n",
      "Episode 2079 finished (timesteps: 3496105/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: -0.67\n",
      "***************************\n",
      "Episode 2080 finished (timesteps: 3499213/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -0.83\n",
      "***************************\n",
      "Episode 2081 finished (timesteps: 3502224/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -0.79\n",
      "***************************\n",
      "Episode 2082 finished (timesteps: 3505114/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -0.77\n",
      "***************************\n",
      "Episode 2083 finished (timesteps: 3508323/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: -0.73\n",
      "***************************\n",
      "Episode 2084 finished (timesteps: 3509148/10000000)\n",
      "Epsilon: 0.01, Episode reward: -21.0, Mean reward: -0.88\n",
      "***************************\n",
      "Episode 2085 finished (timesteps: 3512545/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -1.07\n",
      "***************************\n",
      "Episode 2086 finished (timesteps: 3515231/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -1.05\n",
      "***************************\n",
      "Episode 2087 finished (timesteps: 3518412/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -1.06\n",
      "***************************\n",
      "Episode 2088 finished (timesteps: 3521293/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -1.08\n",
      "***************************\n",
      "Episode 2089 finished (timesteps: 3524837/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -1.00\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2090 finished (timesteps: 3527292/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -1.05\n",
      "***************************\n",
      "Episode 2091 finished (timesteps: 3530313/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: -1.06\n",
      "***************************\n",
      "Episode 2092 finished (timesteps: 3534308/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -1.06\n",
      "***************************\n",
      "Episode 2093 finished (timesteps: 3536503/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: -0.87\n",
      "***************************\n",
      "Episode 2094 finished (timesteps: 3538845/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -1.13\n",
      "***************************\n",
      "Episode 2095 finished (timesteps: 3542173/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -1.11\n",
      "***************************\n",
      "Episode 2096 finished (timesteps: 3545193/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: -1.17\n",
      "***************************\n",
      "Episode 2097 finished (timesteps: 3547608/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: -1.12\n",
      "***************************\n",
      "Episode 2098 finished (timesteps: 3550655/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: -1.03\n",
      "***************************\n",
      "Episode 2099 finished (timesteps: 3553684/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -1.13\n",
      "***************************\n",
      "Episode 2100 finished (timesteps: 3556380/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: -1.10\n",
      "***************************\n",
      "Episode 2101 finished (timesteps: 3559789/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -1.16\n",
      "***************************\n",
      "Episode 2102 finished (timesteps: 3562433/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: -1.04\n",
      "***************************\n",
      "Episode 2103 finished (timesteps: 3564867/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -1.01\n",
      "***************************\n",
      "Episode 2104 finished (timesteps: 3567421/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -1.18\n",
      "***************************\n",
      "Episode 2105 finished (timesteps: 3570071/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -1.16\n",
      "***************************\n",
      "Episode 2106 finished (timesteps: 3572626/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: -1.36\n",
      "***************************\n",
      "Episode 2107 finished (timesteps: 3574298/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: -1.46\n",
      "***************************\n",
      "Episode 2108 finished (timesteps: 3576262/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: -1.68\n",
      "***************************\n",
      "Episode 2109 finished (timesteps: 3578891/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -1.73\n",
      "***************************\n",
      "Episode 2110 finished (timesteps: 3581789/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -1.82\n",
      "***************************\n",
      "Episode 2111 finished (timesteps: 3584088/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -1.84\n",
      "***************************\n",
      "Episode 2112 finished (timesteps: 3587167/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: -1.66\n",
      "***************************\n",
      "Episode 2113 finished (timesteps: 3589972/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: -1.55\n",
      "***************************\n",
      "Episode 2114 finished (timesteps: 3591849/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -1.67\n",
      "***************************\n",
      "Episode 2115 finished (timesteps: 3593929/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -1.77\n",
      "***************************\n",
      "Episode 2116 finished (timesteps: 3598377/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -1.64\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-3600000.mp4.\n",
      "Moviepy - Writing video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-3600000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-3600000.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2117 finished (timesteps: 3601548/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -1.65\n",
      "***************************\n",
      "Episode 2118 finished (timesteps: 3605338/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -1.56\n",
      "***************************\n",
      "Episode 2119 finished (timesteps: 3609633/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -1.33\n",
      "***************************\n",
      "Episode 2120 finished (timesteps: 3611965/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: -1.36\n",
      "***************************\n",
      "Episode 2121 finished (timesteps: 3617362/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -1.42\n",
      "***************************\n",
      "Episode 2122 finished (timesteps: 3620467/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: -1.15\n",
      "***************************\n",
      "Episode 2123 finished (timesteps: 3624019/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: -1.26\n",
      "***************************\n",
      "Episode 2124 finished (timesteps: 3627575/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -1.23\n",
      "***************************\n",
      "Episode 2125 finished (timesteps: 3630937/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: -1.11\n",
      "***************************\n",
      "Episode 2126 finished (timesteps: 3634301/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: -1.09\n",
      "***************************\n",
      "Episode 2127 finished (timesteps: 3638432/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: -1.19\n",
      "***************************\n",
      "Episode 2128 finished (timesteps: 3641815/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: -1.26\n",
      "***************************\n",
      "Episode 2129 finished (timesteps: 3644702/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -1.20\n",
      "***************************\n",
      "Episode 2130 finished (timesteps: 3647882/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -1.35\n",
      "***************************\n",
      "Episode 2131 finished (timesteps: 3651220/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -1.27\n",
      "***************************\n",
      "Episode 2132 finished (timesteps: 3655669/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: -1.42\n",
      "***************************\n",
      "Episode 2133 finished (timesteps: 3659121/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: -1.35\n",
      "***************************\n",
      "Episode 2134 finished (timesteps: 3662596/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -1.16\n",
      "***************************\n",
      "Episode 2135 finished (timesteps: 3665541/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -1.30\n",
      "***************************\n",
      "Episode 2136 finished (timesteps: 3668747/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: -1.21\n",
      "***************************\n",
      "Episode 2137 finished (timesteps: 3671893/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -1.07\n",
      "***************************\n",
      "Episode 2138 finished (timesteps: 3674706/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: -0.84\n",
      "***************************\n",
      "Episode 2139 finished (timesteps: 3677920/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: -0.95\n",
      "***************************\n",
      "Episode 2140 finished (timesteps: 3680662/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -0.92\n",
      "***************************\n",
      "Episode 2141 finished (timesteps: 3683489/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -0.98\n",
      "***************************\n",
      "Episode 2142 finished (timesteps: 3685794/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -1.12\n",
      "***************************\n",
      "Episode 2143 finished (timesteps: 3688639/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: -1.25\n",
      "***************************\n",
      "Episode 2144 finished (timesteps: 3691057/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -1.18\n",
      "***************************\n",
      "Episode 2145 finished (timesteps: 3693932/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -1.19\n",
      "***************************\n",
      "Episode 2146 finished (timesteps: 3695929/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: -1.02\n",
      "***************************\n",
      "Episode 2147 finished (timesteps: 3698522/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -1.12\n",
      "***************************\n",
      "Episode 2148 finished (timesteps: 3700798/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -1.19\n",
      "***************************\n",
      "Episode 2149 finished (timesteps: 3703751/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: -1.27\n",
      "***************************\n",
      "Episode 2150 finished (timesteps: 3705979/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -1.45\n",
      "***************************\n",
      "Episode 2151 finished (timesteps: 3706877/10000000)\n",
      "Epsilon: 0.01, Episode reward: -21.0, Mean reward: -1.60\n",
      "***************************\n",
      "Episode 2152 finished (timesteps: 3709771/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: -1.56\n",
      "***************************\n",
      "Episode 2153 finished (timesteps: 3712897/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -1.45\n",
      "***************************\n",
      "Episode 2154 finished (timesteps: 3716229/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -1.47\n",
      "***************************\n",
      "Episode 2155 finished (timesteps: 3719266/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -1.62\n",
      "***************************\n",
      "Episode 2156 finished (timesteps: 3721743/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: -1.81\n",
      "***************************\n",
      "Episode 2157 finished (timesteps: 3724231/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: -1.68\n",
      "***************************\n",
      "Episode 2158 finished (timesteps: 3726158/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: -1.43\n",
      "***************************\n",
      "Episode 2159 finished (timesteps: 3728621/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: -1.19\n",
      "***************************\n",
      "Episode 2160 finished (timesteps: 3731383/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: -1.10\n",
      "***************************\n",
      "Episode 2161 finished (timesteps: 3734384/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: -1.14\n",
      "***************************\n",
      "Episode 2162 finished (timesteps: 3738165/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -1.12\n",
      "***************************\n",
      "Episode 2163 finished (timesteps: 3741301/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -1.14\n",
      "***************************\n",
      "Episode 2164 finished (timesteps: 3744050/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: -1.08\n",
      "***************************\n",
      "Episode 2165 finished (timesteps: 3747006/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: -0.99\n",
      "***************************\n",
      "Episode 2166 finished (timesteps: 3749817/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: -0.96\n",
      "***************************\n",
      "Episode 2167 finished (timesteps: 3752606/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: -0.91\n",
      "***************************\n",
      "Episode 2168 finished (timesteps: 3755574/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: -0.92\n",
      "***************************\n",
      "Episode 2169 finished (timesteps: 3758372/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: -0.84\n",
      "***************************\n",
      "Episode 2170 finished (timesteps: 3761751/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: -0.94\n",
      "***************************\n",
      "Episode 2171 finished (timesteps: 3764899/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -0.90\n",
      "***************************\n",
      "Episode 2172 finished (timesteps: 3767537/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -0.74\n",
      "***************************\n",
      "Episode 2173 finished (timesteps: 3770839/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: -0.77\n",
      "***************************\n",
      "Episode 2174 finished (timesteps: 3773433/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -0.90\n",
      "***************************\n",
      "Episode 2175 finished (timesteps: 3775273/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: -1.14\n",
      "***************************\n",
      "Episode 2176 finished (timesteps: 3777823/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: -1.19\n",
      "***************************\n",
      "Episode 2177 finished (timesteps: 3779815/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: -0.92\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2178 finished (timesteps: 3782618/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -0.98\n",
      "***************************\n",
      "Episode 2179 finished (timesteps: 3784894/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -1.16\n",
      "***************************\n",
      "Episode 2180 finished (timesteps: 3787809/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: -1.14\n",
      "***************************\n",
      "Episode 2181 finished (timesteps: 3789940/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: -0.99\n",
      "***************************\n",
      "Episode 2182 finished (timesteps: 3792774/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: -1.01\n",
      "***************************\n",
      "Episode 2183 finished (timesteps: 3794052/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: -1.16\n",
      "***************************\n",
      "Episode 2184 finished (timesteps: 3796788/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: -0.87\n",
      "***************************\n",
      "Episode 2185 finished (timesteps: 3799642/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: -0.80\n",
      "***************************\n",
      "Episode 2186 finished (timesteps: 3801979/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: -0.52\n",
      "***************************\n",
      "Episode 2187 finished (timesteps: 3804500/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: -0.27\n",
      "***************************\n",
      "Episode 2188 finished (timesteps: 3807596/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: -0.16\n",
      "***************************\n",
      "Episode 2189 finished (timesteps: 3810211/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: -0.23\n",
      "***************************\n",
      "Episode 2190 finished (timesteps: 3812438/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: -0.20\n",
      "***************************\n",
      "Episode 2191 finished (timesteps: 3815468/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: -0.24\n",
      "***************************\n",
      "Episode 2192 finished (timesteps: 3818308/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: -0.13\n",
      "***************************\n",
      "Episode 2193 finished (timesteps: 3821262/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: -0.23\n",
      "***************************\n",
      "Episode 2194 finished (timesteps: 3824274/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: -0.08\n",
      "***************************\n",
      "Episode 2195 finished (timesteps: 3825155/10000000)\n",
      "Epsilon: 0.01, Episode reward: -21.0, Mean reward: -0.26\n",
      "***************************\n",
      "Episode 2196 finished (timesteps: 3827188/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: -0.40\n",
      "***************************\n",
      "Episode 2197 finished (timesteps: 3830355/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: -0.47\n",
      "***************************\n",
      "Episode 2198 finished (timesteps: 3833083/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: -0.45\n",
      "***************************\n",
      "Episode 2199 finished (timesteps: 3835276/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: -0.33\n",
      "***************************\n",
      "Episode 2200 finished (timesteps: 3837976/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: -0.41\n",
      "***************************\n",
      "Episode 2201 finished (timesteps: 3840684/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -0.38\n",
      "***************************\n",
      "Episode 2202 finished (timesteps: 3844278/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: -0.48\n",
      "***************************\n",
      "Episode 2203 finished (timesteps: 3846949/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: -0.48\n",
      "***************************\n",
      "Episode 2204 finished (timesteps: 3850094/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: -0.32\n",
      "***************************\n",
      "Episode 2205 finished (timesteps: 3852255/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: -0.09\n",
      "***************************\n",
      "Episode 2206 finished (timesteps: 3854963/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 0.04\n",
      "***************************\n",
      "Episode 2207 finished (timesteps: 3857836/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: 0.17\n",
      "***************************\n",
      "Episode 2208 finished (timesteps: 3860856/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 0.27\n",
      "***************************\n",
      "Episode 2209 finished (timesteps: 3863477/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 0.31\n",
      "***************************\n",
      "Episode 2210 finished (timesteps: 3866781/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 0.36\n",
      "***************************\n",
      "Episode 2211 finished (timesteps: 3870046/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 0.44\n",
      "***************************\n",
      "Episode 2212 finished (timesteps: 3873445/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: 0.42\n",
      "***************************\n",
      "Episode 2213 finished (timesteps: 3875924/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: 0.29\n",
      "***************************\n",
      "Episode 2214 finished (timesteps: 3878970/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 0.45\n",
      "***************************\n",
      "Episode 2215 finished (timesteps: 3881745/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 0.67\n",
      "***************************\n",
      "Episode 2216 finished (timesteps: 3883811/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 0.80\n",
      "***************************\n",
      "Episode 2217 finished (timesteps: 3886822/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 0.80\n",
      "***************************\n",
      "Episode 2218 finished (timesteps: 3890058/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 0.79\n",
      "***************************\n",
      "Episode 2219 finished (timesteps: 3892361/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: 0.62\n",
      "***************************\n",
      "Episode 2220 finished (timesteps: 3896117/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 0.71\n",
      "***************************\n",
      "Episode 2221 finished (timesteps: 3899321/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 0.71\n",
      "***************************\n",
      "Episode 2222 finished (timesteps: 3903331/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: 0.61\n",
      "***************************\n",
      "Episode 2223 finished (timesteps: 3906157/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 0.69\n",
      "***************************\n",
      "Episode 2224 finished (timesteps: 3909437/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 0.69\n",
      "***************************\n",
      "Episode 2225 finished (timesteps: 3912290/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 0.59\n",
      "***************************\n",
      "Episode 2226 finished (timesteps: 3914949/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 0.68\n",
      "***************************\n",
      "Episode 2227 finished (timesteps: 3917315/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 0.77\n",
      "***************************\n",
      "Episode 2228 finished (timesteps: 3920110/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 0.82\n",
      "***************************\n",
      "Episode 2229 finished (timesteps: 3922877/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 0.97\n",
      "***************************\n",
      "Episode 2230 finished (timesteps: 3926104/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: 1.05\n",
      "***************************\n",
      "Episode 2231 finished (timesteps: 3929106/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 1.14\n",
      "***************************\n",
      "Episode 2232 finished (timesteps: 3933246/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: 1.10\n",
      "***************************\n",
      "Episode 2233 finished (timesteps: 3937278/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 1.04\n",
      "***************************\n",
      "Episode 2234 finished (timesteps: 3941191/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: 0.89\n",
      "***************************\n",
      "Episode 2235 finished (timesteps: 3945208/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 0.97\n",
      "***************************\n",
      "Episode 2236 finished (timesteps: 3948658/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 1.02\n",
      "***************************\n",
      "Episode 2237 finished (timesteps: 3952623/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: 0.92\n",
      "***************************\n",
      "Episode 2238 finished (timesteps: 3956385/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 0.85\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2239 finished (timesteps: 3959882/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 0.84\n",
      "***************************\n",
      "Episode 2240 finished (timesteps: 3962310/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: 0.79\n",
      "***************************\n",
      "Episode 2241 finished (timesteps: 3964864/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 0.92\n",
      "***************************\n",
      "Episode 2242 finished (timesteps: 3968207/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 1.05\n",
      "***************************\n",
      "Episode 2243 finished (timesteps: 3971892/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: 1.03\n",
      "***************************\n",
      "Episode 2244 finished (timesteps: 3975585/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 1.03\n",
      "***************************\n",
      "Episode 2245 finished (timesteps: 3977929/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 1.23\n",
      "***************************\n",
      "Episode 2246 finished (timesteps: 3981563/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 1.17\n",
      "***************************\n",
      "Episode 2247 finished (timesteps: 3983817/10000000)\n",
      "Epsilon: 0.01, Episode reward: 20.0, Mean reward: 1.34\n",
      "***************************\n",
      "Episode 2248 finished (timesteps: 3987472/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 1.48\n",
      "***************************\n",
      "Episode 2249 finished (timesteps: 3990114/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: 1.37\n",
      "***************************\n",
      "Episode 2250 finished (timesteps: 3993057/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 1.41\n",
      "***************************\n",
      "Episode 2251 finished (timesteps: 3995703/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 1.71\n",
      "***************************\n",
      "Episode 2252 finished (timesteps: 3998273/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 1.58\n",
      "***************************\n",
      "Episode 2253 finished (timesteps: 3999102/10000000)\n",
      "Epsilon: 0.01, Episode reward: -21.0, Mean reward: 1.34\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-4000000.mp4.\n",
      "Moviepy - Writing video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-4000000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-4000000.mp4\n",
      "Episode 2254 finished (timesteps: 4002198/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 1.33\n",
      "***************************\n",
      "Episode 2255 finished (timesteps: 4005013/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 1.35\n",
      "***************************\n",
      "Episode 2256 finished (timesteps: 4008220/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 1.47\n",
      "***************************\n",
      "Episode 2257 finished (timesteps: 4010864/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: 1.31\n",
      "***************************\n",
      "Episode 2258 finished (timesteps: 4014817/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 1.19\n",
      "***************************\n",
      "Episode 2259 finished (timesteps: 4018367/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 1.06\n",
      "***************************\n",
      "Episode 2260 finished (timesteps: 4022650/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 1.06\n",
      "***************************\n",
      "Episode 2261 finished (timesteps: 4027570/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 1.10\n",
      "***************************\n",
      "Episode 2262 finished (timesteps: 4031429/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 1.10\n",
      "***************************\n",
      "Episode 2263 finished (timesteps: 4034060/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: 0.94\n",
      "***************************\n",
      "Episode 2264 finished (timesteps: 4036398/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 1.05\n",
      "***************************\n",
      "Episode 2265 finished (timesteps: 4039080/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: 0.83\n",
      "***************************\n",
      "Episode 2266 finished (timesteps: 4042363/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 0.84\n",
      "***************************\n",
      "Episode 2267 finished (timesteps: 4046358/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 0.71\n",
      "***************************\n",
      "Episode 2268 finished (timesteps: 4049116/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: 0.52\n",
      "***************************\n",
      "Episode 2269 finished (timesteps: 4051453/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 0.54\n",
      "***************************\n",
      "Episode 2270 finished (timesteps: 4054408/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 0.66\n",
      "***************************\n",
      "Episode 2271 finished (timesteps: 4057990/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: 0.66\n",
      "***************************\n",
      "Episode 2272 finished (timesteps: 4062165/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 0.65\n",
      "***************************\n",
      "Episode 2273 finished (timesteps: 4066059/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 0.69\n",
      "***************************\n",
      "Episode 2274 finished (timesteps: 4069224/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 0.86\n",
      "***************************\n",
      "Episode 2275 finished (timesteps: 4072324/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 1.08\n",
      "***************************\n",
      "Episode 2276 finished (timesteps: 4075690/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 1.18\n",
      "***************************\n",
      "Episode 2277 finished (timesteps: 4077344/10000000)\n",
      "Epsilon: 0.01, Episode reward: 19.0, Mean reward: 1.22\n",
      "***************************\n",
      "Episode 2278 finished (timesteps: 4079992/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 1.19\n",
      "***************************\n",
      "Episode 2279 finished (timesteps: 4083002/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 1.24\n",
      "***************************\n",
      "Episode 2280 finished (timesteps: 4085392/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 1.36\n",
      "***************************\n",
      "Episode 2281 finished (timesteps: 4087861/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 1.36\n",
      "***************************\n",
      "Episode 2282 finished (timesteps: 4091572/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 1.39\n",
      "***************************\n",
      "Episode 2283 finished (timesteps: 4094471/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 1.62\n",
      "***************************\n",
      "Episode 2284 finished (timesteps: 4098091/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 1.49\n",
      "***************************\n",
      "Episode 2285 finished (timesteps: 4101170/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 1.55\n",
      "***************************\n",
      "Episode 2286 finished (timesteps: 4104253/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: 1.30\n",
      "***************************\n",
      "Episode 2287 finished (timesteps: 4106353/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 1.28\n",
      "***************************\n",
      "Episode 2288 finished (timesteps: 4108610/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 1.42\n",
      "***************************\n",
      "Episode 2289 finished (timesteps: 4111807/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 1.44\n",
      "***************************\n",
      "Episode 2290 finished (timesteps: 4114650/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: 1.50\n",
      "***************************\n",
      "Episode 2291 finished (timesteps: 4117595/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 1.51\n",
      "***************************\n",
      "Episode 2292 finished (timesteps: 4119472/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: 1.40\n",
      "***************************\n",
      "Episode 2293 finished (timesteps: 4121630/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: 1.21\n",
      "***************************\n",
      "Episode 2294 finished (timesteps: 4124145/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 1.13\n",
      "***************************\n",
      "Episode 2295 finished (timesteps: 4125816/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: 1.20\n",
      "***************************\n",
      "Episode 2296 finished (timesteps: 4128705/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 1.26\n",
      "***************************\n",
      "Episode 2297 finished (timesteps: 4130454/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: 1.08\n",
      "***************************\n",
      "Episode 2298 finished (timesteps: 4133770/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 1.02\n",
      "***************************\n",
      "Episode 2299 finished (timesteps: 4135729/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: 0.83\n",
      "***************************\n",
      "Episode 2300 finished (timesteps: 4138305/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 0.95\n",
      "***************************\n",
      "Episode 2301 finished (timesteps: 4141093/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 0.99\n",
      "***************************\n",
      "Episode 2302 finished (timesteps: 4144132/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 0.90\n",
      "***************************\n",
      "Episode 2303 finished (timesteps: 4147686/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 1.00\n",
      "***************************\n",
      "Episode 2304 finished (timesteps: 4150227/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: 0.87\n",
      "***************************\n",
      "Episode 2305 finished (timesteps: 4152878/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 0.86\n",
      "***************************\n",
      "Episode 2306 finished (timesteps: 4156378/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: 0.85\n",
      "***************************\n",
      "Episode 2307 finished (timesteps: 4159295/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 0.82\n",
      "***************************\n",
      "Episode 2308 finished (timesteps: 4162643/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 0.92\n",
      "***************************\n",
      "Episode 2309 finished (timesteps: 4166512/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 1.04\n",
      "***************************\n",
      "Episode 2310 finished (timesteps: 4170866/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 1.10\n",
      "***************************\n",
      "Episode 2311 finished (timesteps: 4174313/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 1.20\n",
      "***************************\n",
      "Episode 2312 finished (timesteps: 4177380/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 1.29\n",
      "***************************\n",
      "Episode 2313 finished (timesteps: 4179312/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 1.54\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2314 finished (timesteps: 4182573/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 1.62\n",
      "***************************\n",
      "Episode 2315 finished (timesteps: 4185656/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: 1.42\n",
      "***************************\n",
      "Episode 2316 finished (timesteps: 4189492/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 1.28\n",
      "***************************\n",
      "Episode 2317 finished (timesteps: 4192183/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 1.41\n",
      "***************************\n",
      "Episode 2318 finished (timesteps: 4195397/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 1.49\n",
      "***************************\n",
      "Episode 2319 finished (timesteps: 4198433/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 1.61\n",
      "***************************\n",
      "Episode 2320 finished (timesteps: 4201408/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 1.62\n",
      "***************************\n",
      "Episode 2321 finished (timesteps: 4204595/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 1.60\n",
      "***************************\n",
      "Episode 2322 finished (timesteps: 4207398/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: 1.52\n",
      "***************************\n",
      "Episode 2323 finished (timesteps: 4210154/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 1.53\n",
      "***************************\n",
      "Episode 2324 finished (timesteps: 4212678/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 1.67\n",
      "***************************\n",
      "Episode 2325 finished (timesteps: 4215204/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 1.80\n",
      "***************************\n",
      "Episode 2326 finished (timesteps: 4216383/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: 1.49\n",
      "***************************\n",
      "Episode 2327 finished (timesteps: 4219160/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 1.34\n",
      "***************************\n",
      "Episode 2328 finished (timesteps: 4222184/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 1.30\n",
      "***************************\n",
      "Episode 2329 finished (timesteps: 4225149/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 1.16\n",
      "***************************\n",
      "Episode 2330 finished (timesteps: 4227357/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 1.31\n",
      "***************************\n",
      "Episode 2331 finished (timesteps: 4229955/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 1.17\n",
      "***************************\n",
      "Episode 2332 finished (timesteps: 4231884/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: 1.05\n",
      "***************************\n",
      "Episode 2333 finished (timesteps: 4233668/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 1.19\n",
      "***************************\n",
      "Episode 2334 finished (timesteps: 4236541/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 1.24\n",
      "***************************\n",
      "Episode 2335 finished (timesteps: 4237319/10000000)\n",
      "Epsilon: 0.01, Episode reward: -21.0, Mean reward: 0.98\n",
      "***************************\n",
      "Episode 2336 finished (timesteps: 4240151/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 1.02\n",
      "***************************\n",
      "Episode 2337 finished (timesteps: 4242452/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 1.21\n",
      "***************************\n",
      "Episode 2338 finished (timesteps: 4245726/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 1.17\n",
      "***************************\n",
      "Episode 2339 finished (timesteps: 4247391/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: 1.06\n",
      "***************************\n",
      "Episode 2340 finished (timesteps: 4249979/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 1.26\n",
      "***************************\n",
      "Episode 2341 finished (timesteps: 4252121/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: 1.05\n",
      "***************************\n",
      "Episode 2342 finished (timesteps: 4254643/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: 0.94\n",
      "***************************\n",
      "Episode 2343 finished (timesteps: 4257331/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 1.06\n",
      "***************************\n",
      "Episode 2344 finished (timesteps: 4260056/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 0.95\n",
      "***************************\n",
      "Episode 2345 finished (timesteps: 4262006/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: 0.65\n",
      "***************************\n",
      "Episode 2346 finished (timesteps: 4264157/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 0.71\n",
      "***************************\n",
      "Episode 2347 finished (timesteps: 4266245/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: 0.41\n",
      "***************************\n",
      "Episode 2348 finished (timesteps: 4268360/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 0.49\n",
      "***************************\n",
      "Episode 2349 finished (timesteps: 4270227/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: 0.42\n",
      "***************************\n",
      "Episode 2350 finished (timesteps: 4272519/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: 0.38\n",
      "***************************\n",
      "Episode 2351 finished (timesteps: 4274244/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: 0.14\n",
      "***************************\n",
      "Episode 2352 finished (timesteps: 4277771/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 0.19\n",
      "***************************\n",
      "Episode 2353 finished (timesteps: 4280113/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: 0.30\n",
      "***************************\n",
      "Episode 2354 finished (timesteps: 4282443/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 0.37\n",
      "***************************\n",
      "Episode 2355 finished (timesteps: 4284617/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 0.56\n",
      "***************************\n",
      "Episode 2356 finished (timesteps: 4288202/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 0.59\n",
      "***************************\n",
      "Episode 2357 finished (timesteps: 4291992/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 0.69\n",
      "***************************\n",
      "Episode 2358 finished (timesteps: 4294716/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 0.79\n",
      "***************************\n",
      "Episode 2359 finished (timesteps: 4298266/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 0.88\n",
      "***************************\n",
      "Episode 2360 finished (timesteps: 4301182/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 0.86\n",
      "***************************\n",
      "Episode 2361 finished (timesteps: 4304821/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 0.84\n",
      "***************************\n",
      "Episode 2362 finished (timesteps: 4307618/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 0.96\n",
      "***************************\n",
      "Episode 2363 finished (timesteps: 4310112/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 1.18\n",
      "***************************\n",
      "Episode 2364 finished (timesteps: 4313472/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: 0.99\n",
      "***************************\n",
      "Episode 2365 finished (timesteps: 4315837/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 1.19\n",
      "***************************\n",
      "Episode 2366 finished (timesteps: 4318973/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 1.16\n",
      "***************************\n",
      "Episode 2367 finished (timesteps: 4321712/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: 1.08\n",
      "***************************\n",
      "Episode 2368 finished (timesteps: 4323813/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: 1.04\n",
      "***************************\n",
      "Episode 2369 finished (timesteps: 4327369/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 0.92\n",
      "***************************\n",
      "Episode 2370 finished (timesteps: 4330623/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 0.94\n",
      "***************************\n",
      "Episode 2371 finished (timesteps: 4333927/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 1.02\n",
      "***************************\n",
      "Episode 2372 finished (timesteps: 4337442/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 0.93\n",
      "***************************\n",
      "Episode 2373 finished (timesteps: 4341120/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 0.87\n",
      "***************************\n",
      "Episode 2374 finished (timesteps: 4344163/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 0.96\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2375 finished (timesteps: 4347930/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 0.91\n",
      "***************************\n",
      "Episode 2376 finished (timesteps: 4351334/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 0.97\n",
      "***************************\n",
      "Episode 2377 finished (timesteps: 4353910/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 0.92\n",
      "***************************\n",
      "Episode 2378 finished (timesteps: 4356503/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 1.06\n",
      "***************************\n",
      "Episode 2379 finished (timesteps: 4359617/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 1.23\n",
      "***************************\n",
      "Episode 2380 finished (timesteps: 4362022/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: 1.03\n",
      "***************************\n",
      "Episode 2381 finished (timesteps: 4365934/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 0.94\n",
      "***************************\n",
      "Episode 2382 finished (timesteps: 4369750/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 1.05\n",
      "***************************\n",
      "Episode 2383 finished (timesteps: 4372982/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 1.13\n",
      "***************************\n",
      "Episode 2384 finished (timesteps: 4376499/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 1.23\n",
      "***************************\n",
      "Episode 2385 finished (timesteps: 4380367/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 1.22\n",
      "***************************\n",
      "Episode 2386 finished (timesteps: 4383988/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 1.27\n",
      "***************************\n",
      "Episode 2387 finished (timesteps: 4386914/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: 1.07\n",
      "***************************\n",
      "Episode 2388 finished (timesteps: 4391175/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 0.96\n",
      "***************************\n",
      "Episode 2389 finished (timesteps: 4394399/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 1.07\n",
      "***************************\n",
      "Episode 2390 finished (timesteps: 4396653/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: 0.96\n",
      "***************************\n",
      "Episode 2391 finished (timesteps: 4399343/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 1.09\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-4400000.mp4.\n",
      "Moviepy - Writing video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-4400000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-4400000.mp4\n",
      "Episode 2392 finished (timesteps: 4402553/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: 1.07\n",
      "***************************\n",
      "Episode 2393 finished (timesteps: 4407416/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 1.21\n",
      "***************************\n",
      "Episode 2394 finished (timesteps: 4411682/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: 1.22\n",
      "***************************\n",
      "Episode 2395 finished (timesteps: 4415510/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 1.32\n",
      "***************************\n",
      "Episode 2396 finished (timesteps: 4419286/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 1.42\n",
      "***************************\n",
      "Episode 2397 finished (timesteps: 4423343/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 1.63\n",
      "***************************\n",
      "Episode 2398 finished (timesteps: 4427862/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 1.63\n",
      "***************************\n",
      "Episode 2399 finished (timesteps: 4430912/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 1.78\n",
      "***************************\n",
      "Episode 2400 finished (timesteps: 4434053/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 1.60\n",
      "***************************\n",
      "Episode 2401 finished (timesteps: 4437240/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 1.52\n",
      "***************************\n",
      "Episode 2402 finished (timesteps: 4440068/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 1.75\n",
      "***************************\n",
      "Episode 2403 finished (timesteps: 4444667/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: 1.71\n",
      "***************************\n",
      "Episode 2404 finished (timesteps: 4448289/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 1.73\n",
      "***************************\n",
      "Episode 2405 finished (timesteps: 4452621/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: 1.56\n",
      "***************************\n",
      "Episode 2406 finished (timesteps: 4456320/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 1.65\n",
      "***************************\n",
      "Episode 2407 finished (timesteps: 4459886/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 1.76\n",
      "***************************\n",
      "Episode 2408 finished (timesteps: 4462506/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 1.77\n",
      "***************************\n",
      "Episode 2409 finished (timesteps: 4465911/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: 1.63\n",
      "***************************\n",
      "Episode 2410 finished (timesteps: 4466932/10000000)\n",
      "Epsilon: 0.01, Episode reward: -20.0, Mean reward: 1.35\n",
      "***************************\n",
      "Episode 2411 finished (timesteps: 4470533/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 1.36\n",
      "***************************\n",
      "Episode 2412 finished (timesteps: 4473931/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 1.32\n",
      "***************************\n",
      "Episode 2413 finished (timesteps: 4476400/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: 1.11\n",
      "***************************\n",
      "Episode 2414 finished (timesteps: 4478873/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 1.18\n",
      "***************************\n",
      "Episode 2415 finished (timesteps: 4481732/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 1.38\n",
      "***************************\n",
      "Episode 2416 finished (timesteps: 4484224/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: 1.27\n",
      "***************************\n",
      "Episode 2417 finished (timesteps: 4486140/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: 1.03\n",
      "***************************\n",
      "Episode 2418 finished (timesteps: 4489237/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 1.02\n",
      "***************************\n",
      "Episode 2419 finished (timesteps: 4492528/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 1.11\n",
      "***************************\n",
      "Episode 2420 finished (timesteps: 4495802/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 1.29\n",
      "***************************\n",
      "Episode 2421 finished (timesteps: 4498914/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 1.45\n",
      "***************************\n",
      "Episode 2422 finished (timesteps: 4501085/10000000)\n",
      "Epsilon: 0.01, Episode reward: 19.0, Mean reward: 1.73\n",
      "***************************\n",
      "Episode 2423 finished (timesteps: 4505486/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 1.67\n",
      "***************************\n",
      "Episode 2424 finished (timesteps: 4510342/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 1.60\n",
      "***************************\n",
      "Episode 2425 finished (timesteps: 4513585/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 1.55\n",
      "***************************\n",
      "Episode 2426 finished (timesteps: 4515470/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 1.89\n",
      "***************************\n",
      "Episode 2427 finished (timesteps: 4517970/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: 1.77\n",
      "***************************\n",
      "Episode 2428 finished (timesteps: 4520836/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: 1.61\n",
      "***************************\n",
      "Episode 2429 finished (timesteps: 4524161/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 1.74\n",
      "***************************\n",
      "Episode 2430 finished (timesteps: 4527768/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 1.66\n",
      "***************************\n",
      "Episode 2431 finished (timesteps: 4530537/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 1.81\n",
      "***************************\n",
      "Episode 2432 finished (timesteps: 4533761/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 2.05\n",
      "***************************\n",
      "Episode 2433 finished (timesteps: 4537144/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 2.00\n",
      "***************************\n",
      "Episode 2434 finished (timesteps: 4539345/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 2.18\n",
      "***************************\n",
      "Episode 2435 finished (timesteps: 4540726/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: 2.21\n",
      "***************************\n",
      "Episode 2436 finished (timesteps: 4544728/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 2.09\n",
      "***************************\n",
      "Episode 2437 finished (timesteps: 4547482/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: 1.90\n",
      "***************************\n",
      "Episode 2438 finished (timesteps: 4551451/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 1.86\n",
      "***************************\n",
      "Episode 2439 finished (timesteps: 4554079/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 2.10\n",
      "***************************\n",
      "Episode 2440 finished (timesteps: 4558044/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 2.01\n",
      "***************************\n",
      "Episode 2441 finished (timesteps: 4561553/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 2.07\n",
      "***************************\n",
      "Episode 2442 finished (timesteps: 4564349/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 2.34\n",
      "***************************\n",
      "Episode 2443 finished (timesteps: 4568249/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: 2.11\n",
      "***************************\n",
      "Episode 2444 finished (timesteps: 4573121/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: 2.06\n",
      "***************************\n",
      "Episode 2445 finished (timesteps: 4577497/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 2.30\n",
      "***************************\n",
      "Episode 2446 finished (timesteps: 4581450/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 2.28\n",
      "***************************\n",
      "Episode 2447 finished (timesteps: 4584520/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 2.46\n",
      "***************************\n",
      "Episode 2448 finished (timesteps: 4588603/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 2.37\n",
      "***************************\n",
      "Episode 2449 finished (timesteps: 4592416/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: 2.44\n",
      "***************************\n",
      "Episode 2450 finished (timesteps: 4597017/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 2.55\n",
      "***************************\n",
      "Episode 2451 finished (timesteps: 4601114/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 2.72\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2452 finished (timesteps: 4604630/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 2.76\n",
      "***************************\n",
      "Episode 2453 finished (timesteps: 4609706/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 2.87\n",
      "***************************\n",
      "Episode 2454 finished (timesteps: 4613335/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 2.77\n",
      "***************************\n",
      "Episode 2455 finished (timesteps: 4616502/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: 2.55\n",
      "***************************\n",
      "Episode 2456 finished (timesteps: 4620316/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: 2.43\n",
      "***************************\n",
      "Episode 2457 finished (timesteps: 4623810/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 2.49\n",
      "***************************\n",
      "Episode 2458 finished (timesteps: 4627726/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: 2.28\n",
      "***************************\n",
      "Episode 2459 finished (timesteps: 4631034/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 2.17\n",
      "***************************\n",
      "Episode 2460 finished (timesteps: 4635067/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 2.16\n",
      "***************************\n",
      "Episode 2461 finished (timesteps: 4637980/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 2.21\n",
      "***************************\n",
      "Episode 2462 finished (timesteps: 4639953/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 2.23\n",
      "***************************\n",
      "Episode 2463 finished (timesteps: 4643292/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 2.16\n",
      "***************************\n",
      "Episode 2464 finished (timesteps: 4646070/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: 2.12\n",
      "***************************\n",
      "Episode 2465 finished (timesteps: 4648832/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 2.16\n",
      "***************************\n",
      "Episode 2466 finished (timesteps: 4652539/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 2.17\n",
      "***************************\n",
      "Episode 2467 finished (timesteps: 4655121/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 2.42\n",
      "***************************\n",
      "Episode 2468 finished (timesteps: 4658098/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 2.57\n",
      "***************************\n",
      "Episode 2469 finished (timesteps: 4661556/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: 2.53\n",
      "***************************\n",
      "Episode 2470 finished (timesteps: 4664842/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 2.45\n",
      "***************************\n",
      "Episode 2471 finished (timesteps: 4668992/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 2.43\n",
      "***************************\n",
      "Episode 2472 finished (timesteps: 4673165/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 2.50\n",
      "***************************\n",
      "Episode 2473 finished (timesteps: 4676505/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 2.67\n",
      "***************************\n",
      "Episode 2474 finished (timesteps: 4681469/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 2.54\n",
      "***************************\n",
      "Episode 2475 finished (timesteps: 4685076/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 2.61\n",
      "***************************\n",
      "Episode 2476 finished (timesteps: 4688396/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 2.60\n",
      "***************************\n",
      "Episode 2477 finished (timesteps: 4691713/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: 2.45\n",
      "***************************\n",
      "Episode 2478 finished (timesteps: 4695094/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: 2.34\n",
      "***************************\n",
      "Episode 2479 finished (timesteps: 4698709/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 2.16\n",
      "***************************\n",
      "Episode 2480 finished (timesteps: 4702834/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 2.35\n",
      "***************************\n",
      "Episode 2481 finished (timesteps: 4705169/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 2.47\n",
      "***************************\n",
      "Episode 2482 finished (timesteps: 4708912/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 2.44\n",
      "***************************\n",
      "Episode 2483 finished (timesteps: 4713018/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 2.35\n",
      "***************************\n",
      "Episode 2484 finished (timesteps: 4716768/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: 2.27\n",
      "***************************\n",
      "Episode 2485 finished (timesteps: 4719860/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 2.26\n",
      "***************************\n",
      "Episode 2486 finished (timesteps: 4722521/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 2.33\n",
      "***************************\n",
      "Episode 2487 finished (timesteps: 4726122/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 2.46\n",
      "***************************\n",
      "Episode 2488 finished (timesteps: 4730478/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 2.49\n",
      "***************************\n",
      "Episode 2489 finished (timesteps: 4734116/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 2.45\n",
      "***************************\n",
      "Episode 2490 finished (timesteps: 4737865/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 2.68\n",
      "***************************\n",
      "Episode 2491 finished (timesteps: 4741638/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 2.61\n",
      "***************************\n",
      "Episode 2492 finished (timesteps: 4746150/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 2.72\n",
      "***************************\n",
      "Episode 2493 finished (timesteps: 4749085/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 2.65\n",
      "***************************\n",
      "Episode 2494 finished (timesteps: 4751966/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: 2.54\n",
      "***************************\n",
      "Episode 2495 finished (timesteps: 4755250/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 2.71\n",
      "***************************\n",
      "Episode 2496 finished (timesteps: 4759038/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 2.68\n",
      "***************************\n",
      "Episode 2497 finished (timesteps: 4763510/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 2.64\n",
      "***************************\n",
      "Episode 2498 finished (timesteps: 4766972/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 2.68\n",
      "***************************\n",
      "Episode 2499 finished (timesteps: 4769118/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: 2.50\n",
      "***************************\n",
      "Episode 2500 finished (timesteps: 4772946/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 2.50\n",
      "Saving checkpoint...\n",
      "Checkpoint saved into runs/checkpoints/CNN_DDQN_Pong-v5_11-04-2023_16:28:30\n",
      "***************************\n",
      "Episode 2501 finished (timesteps: 4776091/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: 2.32\n",
      "***************************\n",
      "Episode 2502 finished (timesteps: 4779631/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 2.26\n",
      "***************************\n",
      "Episode 2503 finished (timesteps: 4782889/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 2.37\n",
      "***************************\n",
      "Episode 2504 finished (timesteps: 4786671/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 2.47\n",
      "***************************\n",
      "Episode 2505 finished (timesteps: 4790164/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 2.58\n",
      "***************************\n",
      "Episode 2506 finished (timesteps: 4792980/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 2.62\n",
      "***************************\n",
      "Episode 2507 finished (timesteps: 4796211/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 2.58\n",
      "***************************\n",
      "Episode 2508 finished (timesteps: 4798349/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 2.64\n",
      "***************************\n",
      "Episode 2509 finished (timesteps: 4800738/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: 2.58\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-4800000.mp4.\n",
      "Moviepy - Writing video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-4800000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-4800000.mp4\n",
      "Episode 2510 finished (timesteps: 4803446/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: 2.71\n",
      "***************************\n",
      "Episode 2511 finished (timesteps: 4806340/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 2.74\n",
      "***************************\n",
      "Episode 2512 finished (timesteps: 4809609/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 2.73\n",
      "***************************\n",
      "Episode 2513 finished (timesteps: 4813019/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 2.76\n",
      "***************************\n",
      "Episode 2514 finished (timesteps: 4815954/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 2.68\n",
      "***************************\n",
      "Episode 2515 finished (timesteps: 4819038/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 2.71\n",
      "***************************\n",
      "Episode 2516 finished (timesteps: 4823440/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 2.89\n",
      "***************************\n",
      "Episode 2517 finished (timesteps: 4826756/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 3.13\n",
      "***************************\n",
      "Episode 2518 finished (timesteps: 4831292/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 3.19\n",
      "***************************\n",
      "Episode 2519 finished (timesteps: 4835411/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 3.21\n",
      "***************************\n",
      "Episode 2520 finished (timesteps: 4838572/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 3.11\n",
      "***************************\n",
      "Episode 2521 finished (timesteps: 4842388/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 3.05\n",
      "***************************\n",
      "Episode 2522 finished (timesteps: 4846261/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 2.89\n",
      "***************************\n",
      "Episode 2523 finished (timesteps: 4849455/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 2.84\n",
      "***************************\n",
      "Episode 2524 finished (timesteps: 4852277/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 2.92\n",
      "***************************\n",
      "Episode 2525 finished (timesteps: 4854735/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 3.04\n",
      "***************************\n",
      "Episode 2526 finished (timesteps: 4858271/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 2.84\n",
      "***************************\n",
      "Episode 2527 finished (timesteps: 4861290/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: 2.88\n",
      "***************************\n",
      "Episode 2528 finished (timesteps: 4862917/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: 2.83\n",
      "***************************\n",
      "Episode 2529 finished (timesteps: 4865457/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 2.88\n",
      "***************************\n",
      "Episode 2530 finished (timesteps: 4869224/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 2.83\n",
      "***************************\n",
      "Episode 2531 finished (timesteps: 4872511/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: 2.69\n",
      "***************************\n",
      "Episode 2532 finished (timesteps: 4876231/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 2.66\n",
      "***************************\n",
      "Episode 2533 finished (timesteps: 4879474/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 2.72\n",
      "***************************\n",
      "Episode 2534 finished (timesteps: 4881286/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: 2.47\n",
      "***************************\n",
      "Episode 2535 finished (timesteps: 4882516/10000000)\n",
      "Epsilon: 0.01, Episode reward: -19.0, Mean reward: 2.46\n",
      "***************************\n",
      "Episode 2536 finished (timesteps: 4885696/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 2.48\n",
      "***************************\n",
      "Episode 2537 finished (timesteps: 4888073/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 2.67\n",
      "***************************\n",
      "Episode 2538 finished (timesteps: 4890516/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 2.91\n",
      "***************************\n",
      "Episode 2539 finished (timesteps: 4893229/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 2.91\n",
      "***************************\n",
      "Episode 2540 finished (timesteps: 4896638/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: 2.89\n",
      "***************************\n",
      "Episode 2541 finished (timesteps: 4899622/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 2.96\n",
      "***************************\n",
      "Episode 2542 finished (timesteps: 4901548/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: 2.62\n",
      "***************************\n",
      "Episode 2543 finished (timesteps: 4904886/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 2.79\n",
      "***************************\n",
      "Episode 2544 finished (timesteps: 4909394/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 2.87\n",
      "***************************\n",
      "Episode 2545 finished (timesteps: 4912406/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: 2.67\n",
      "***************************\n",
      "Episode 2546 finished (timesteps: 4915530/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 2.59\n",
      "***************************\n",
      "Episode 2547 finished (timesteps: 4918933/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 2.60\n",
      "***************************\n",
      "Episode 2548 finished (timesteps: 4922432/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 2.63\n",
      "***************************\n",
      "Episode 2549 finished (timesteps: 4926566/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 2.79\n",
      "***************************\n",
      "Episode 2550 finished (timesteps: 4930705/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 2.78\n",
      "***************************\n",
      "Episode 2551 finished (timesteps: 4933410/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 2.86\n",
      "***************************\n",
      "Episode 2552 finished (timesteps: 4936492/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 2.85\n",
      "***************************\n",
      "Episode 2553 finished (timesteps: 4937672/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: 2.66\n",
      "***************************\n",
      "Episode 2554 finished (timesteps: 4940113/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 2.75\n",
      "***************************\n",
      "Episode 2555 finished (timesteps: 4942313/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: 2.71\n",
      "***************************\n",
      "Episode 2556 finished (timesteps: 4945850/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 2.75\n",
      "***************************\n",
      "Episode 2557 finished (timesteps: 4948976/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: 2.55\n",
      "***************************\n",
      "Episode 2558 finished (timesteps: 4952461/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 2.72\n",
      "***************************\n",
      "Episode 2559 finished (timesteps: 4954931/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: 2.67\n",
      "***************************\n",
      "Episode 2560 finished (timesteps: 4959232/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 2.67\n",
      "***************************\n",
      "Episode 2561 finished (timesteps: 4963878/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 2.64\n",
      "***************************\n",
      "Episode 2562 finished (timesteps: 4966745/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: 2.29\n",
      "***************************\n",
      "Episode 2563 finished (timesteps: 4971081/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 2.29\n",
      "***************************\n",
      "Episode 2564 finished (timesteps: 4975448/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 2.37\n",
      "***************************\n",
      "Episode 2565 finished (timesteps: 4977944/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 2.39\n",
      "***************************\n",
      "Episode 2566 finished (timesteps: 4980527/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 2.45\n",
      "***************************\n",
      "Episode 2567 finished (timesteps: 4983082/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 2.50\n",
      "***************************\n",
      "Episode 2568 finished (timesteps: 4985483/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 2.65\n",
      "***************************\n",
      "Episode 2569 finished (timesteps: 4988620/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 2.72\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2570 finished (timesteps: 4991398/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 2.78\n",
      "***************************\n",
      "Episode 2571 finished (timesteps: 4994426/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 2.86\n",
      "***************************\n",
      "Episode 2572 finished (timesteps: 4998955/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 2.87\n",
      "***************************\n",
      "Episode 2573 finished (timesteps: 5003222/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 2.80\n",
      "***************************\n",
      "Episode 2574 finished (timesteps: 5006534/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 2.87\n",
      "***************************\n",
      "Episode 2575 finished (timesteps: 5009761/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 2.80\n",
      "***************************\n",
      "Episode 2576 finished (timesteps: 5013237/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 2.85\n",
      "***************************\n",
      "Episode 2577 finished (timesteps: 5016448/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 2.94\n",
      "***************************\n",
      "Episode 2578 finished (timesteps: 5021347/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 3.01\n",
      "***************************\n",
      "Episode 2579 finished (timesteps: 5025385/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 3.19\n",
      "***************************\n",
      "Episode 2580 finished (timesteps: 5028466/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 3.22\n",
      "***************************\n",
      "Episode 2581 finished (timesteps: 5031740/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 3.15\n",
      "***************************\n",
      "Episode 2582 finished (timesteps: 5034131/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: 2.98\n",
      "***************************\n",
      "Episode 2583 finished (timesteps: 5036446/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: 2.81\n",
      "***************************\n",
      "Episode 2584 finished (timesteps: 5039092/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 2.95\n",
      "***************************\n",
      "Episode 2585 finished (timesteps: 5042727/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 2.92\n",
      "***************************\n",
      "Episode 2586 finished (timesteps: 5045666/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 2.99\n",
      "***************************\n",
      "Episode 2587 finished (timesteps: 5048930/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: 2.81\n",
      "***************************\n",
      "Episode 2588 finished (timesteps: 5052453/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 2.82\n",
      "***************************\n",
      "Episode 2589 finished (timesteps: 5054675/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 2.95\n",
      "***************************\n",
      "Episode 2590 finished (timesteps: 5058912/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: 2.77\n",
      "***************************\n",
      "Episode 2591 finished (timesteps: 5064032/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 2.71\n",
      "***************************\n",
      "Episode 2592 finished (timesteps: 5068366/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 2.73\n",
      "***************************\n",
      "Episode 2593 finished (timesteps: 5072269/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 2.84\n",
      "***************************\n",
      "Episode 2594 finished (timesteps: 5076297/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 3.09\n",
      "***************************\n",
      "Episode 2595 finished (timesteps: 5078310/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: 2.78\n",
      "***************************\n",
      "Episode 2596 finished (timesteps: 5082853/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 2.77\n",
      "***************************\n",
      "Episode 2597 finished (timesteps: 5087018/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 2.86\n",
      "***************************\n",
      "Episode 2598 finished (timesteps: 5092203/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 2.86\n",
      "***************************\n",
      "Episode 2599 finished (timesteps: 5094828/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 3.15\n",
      "***************************\n",
      "Episode 2600 finished (timesteps: 5099391/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 3.14\n",
      "***************************\n",
      "Episode 2601 finished (timesteps: 5103180/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 3.36\n",
      "***************************\n",
      "Episode 2602 finished (timesteps: 5108139/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 3.22\n",
      "***************************\n",
      "Episode 2603 finished (timesteps: 5112263/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 3.26\n",
      "***************************\n",
      "Episode 2604 finished (timesteps: 5116078/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 3.32\n",
      "***************************\n",
      "Episode 2605 finished (timesteps: 5120810/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 3.34\n",
      "***************************\n",
      "Episode 2606 finished (timesteps: 5125299/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 3.28\n",
      "***************************\n",
      "Episode 2607 finished (timesteps: 5130454/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 3.30\n",
      "***************************\n",
      "Episode 2608 finished (timesteps: 5136076/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 3.22\n",
      "***************************\n",
      "Episode 2609 finished (timesteps: 5139005/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: 3.25\n",
      "***************************\n",
      "Episode 2610 finished (timesteps: 5143684/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 3.38\n",
      "***************************\n",
      "Episode 2611 finished (timesteps: 5146563/10000000)\n",
      "Epsilon: 0.01, Episode reward: 19.0, Mean reward: 3.45\n",
      "***************************\n",
      "Episode 2612 finished (timesteps: 5149781/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 3.58\n",
      "***************************\n",
      "Episode 2613 finished (timesteps: 5153286/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 3.73\n",
      "***************************\n",
      "Episode 2614 finished (timesteps: 5156446/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 3.75\n",
      "***************************\n",
      "Episode 2615 finished (timesteps: 5160070/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 3.70\n",
      "***************************\n",
      "Episode 2616 finished (timesteps: 5163498/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 3.68\n",
      "***************************\n",
      "Episode 2617 finished (timesteps: 5166685/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: 3.53\n",
      "***************************\n",
      "Episode 2618 finished (timesteps: 5170551/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 3.50\n",
      "***************************\n",
      "Episode 2619 finished (timesteps: 5174047/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 3.50\n",
      "***************************\n",
      "Episode 2620 finished (timesteps: 5176940/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: 3.32\n",
      "***************************\n",
      "Episode 2621 finished (timesteps: 5180087/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 3.44\n",
      "***************************\n",
      "Episode 2622 finished (timesteps: 5184530/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 3.45\n",
      "***************************\n",
      "Episode 2623 finished (timesteps: 5188015/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 3.57\n",
      "***************************\n",
      "Episode 2624 finished (timesteps: 5192243/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 3.51\n",
      "***************************\n",
      "Episode 2625 finished (timesteps: 5196569/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 3.44\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-5200000.mp4.\n",
      "Moviepy - Writing video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-5200000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-5200000.mp4\n",
      "Episode 2626 finished (timesteps: 5201730/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 3.46\n",
      "***************************\n",
      "Episode 2627 finished (timesteps: 5205295/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 3.65\n",
      "***************************\n",
      "Episode 2628 finished (timesteps: 5209171/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 3.84\n",
      "***************************\n",
      "Episode 2629 finished (timesteps: 5212686/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 3.76\n",
      "***************************\n",
      "Episode 2630 finished (timesteps: 5216879/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: 3.65\n",
      "***************************\n",
      "Episode 2631 finished (timesteps: 5220606/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 3.73\n",
      "***************************\n",
      "Episode 2632 finished (timesteps: 5224735/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 3.81\n",
      "***************************\n",
      "Episode 2633 finished (timesteps: 5228120/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 3.70\n",
      "***************************\n",
      "Episode 2634 finished (timesteps: 5232044/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 3.90\n",
      "***************************\n",
      "Episode 2635 finished (timesteps: 5235454/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 4.17\n",
      "***************************\n",
      "Episode 2636 finished (timesteps: 5237980/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 4.30\n",
      "***************************\n",
      "Episode 2637 finished (timesteps: 5241886/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 4.24\n",
      "***************************\n",
      "Episode 2638 finished (timesteps: 5244083/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 4.21\n",
      "***************************\n",
      "Episode 2639 finished (timesteps: 5247254/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 4.26\n",
      "***************************\n",
      "Episode 2640 finished (timesteps: 5250904/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 4.38\n",
      "***************************\n",
      "Episode 2641 finished (timesteps: 5254415/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 4.41\n",
      "***************************\n",
      "Episode 2642 finished (timesteps: 5257711/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 4.64\n",
      "***************************\n",
      "Episode 2643 finished (timesteps: 5260727/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 4.71\n",
      "***************************\n",
      "Episode 2644 finished (timesteps: 5264063/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 4.84\n",
      "***************************\n",
      "Episode 2645 finished (timesteps: 5267634/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 5.06\n",
      "***************************\n",
      "Episode 2646 finished (timesteps: 5271407/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 5.06\n",
      "***************************\n",
      "Episode 2647 finished (timesteps: 5275304/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 4.91\n",
      "***************************\n",
      "Episode 2648 finished (timesteps: 5278489/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 4.98\n",
      "***************************\n",
      "Episode 2649 finished (timesteps: 5280856/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 5.02\n",
      "***************************\n",
      "Episode 2650 finished (timesteps: 5284383/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 5.07\n",
      "***************************\n",
      "Episode 2651 finished (timesteps: 5288706/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 5.00\n",
      "***************************\n",
      "Episode 2652 finished (timesteps: 5292680/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 4.92\n",
      "***************************\n",
      "Episode 2653 finished (timesteps: 5296834/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 5.15\n",
      "***************************\n",
      "Episode 2654 finished (timesteps: 5300253/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 5.13\n",
      "***************************\n",
      "Episode 2655 finished (timesteps: 5304327/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 5.31\n",
      "***************************\n",
      "Episode 2656 finished (timesteps: 5306966/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 5.50\n",
      "***************************\n",
      "Episode 2657 finished (timesteps: 5310118/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 5.75\n",
      "***************************\n",
      "Episode 2658 finished (timesteps: 5313038/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 5.76\n",
      "***************************\n",
      "Episode 2659 finished (timesteps: 5316580/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 5.94\n",
      "***************************\n",
      "Episode 2660 finished (timesteps: 5317404/10000000)\n",
      "Epsilon: 0.01, Episode reward: -21.0, Mean reward: 5.71\n",
      "***************************\n",
      "Episode 2661 finished (timesteps: 5321125/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 5.72\n",
      "***************************\n",
      "Episode 2662 finished (timesteps: 5324737/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 5.96\n",
      "***************************\n",
      "Episode 2663 finished (timesteps: 5327986/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 5.99\n",
      "***************************\n",
      "Episode 2664 finished (timesteps: 5332205/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 6.05\n",
      "***************************\n",
      "Episode 2665 finished (timesteps: 5335392/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 5.86\n",
      "***************************\n",
      "Episode 2666 finished (timesteps: 5338729/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 5.85\n",
      "***************************\n",
      "Episode 2667 finished (timesteps: 5342307/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 5.76\n",
      "***************************\n",
      "Episode 2668 finished (timesteps: 5345907/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 5.61\n",
      "***************************\n",
      "Episode 2669 finished (timesteps: 5348797/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 5.65\n",
      "***************************\n",
      "Episode 2670 finished (timesteps: 5351561/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 5.69\n",
      "***************************\n",
      "Episode 2671 finished (timesteps: 5354840/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 5.56\n",
      "***************************\n",
      "Episode 2672 finished (timesteps: 5357941/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: 5.49\n",
      "***************************\n",
      "Episode 2673 finished (timesteps: 5361408/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 5.54\n",
      "***************************\n",
      "Episode 2674 finished (timesteps: 5364479/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 5.57\n",
      "***************************\n",
      "Episode 2675 finished (timesteps: 5368727/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 5.54\n",
      "***************************\n",
      "Episode 2676 finished (timesteps: 5372811/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 5.48\n",
      "***************************\n",
      "Episode 2677 finished (timesteps: 5375526/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 5.52\n",
      "***************************\n",
      "Episode 2678 finished (timesteps: 5379131/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 5.55\n",
      "***************************\n",
      "Episode 2679 finished (timesteps: 5381781/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 5.56\n",
      "***************************\n",
      "Episode 2680 finished (timesteps: 5384641/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 5.58\n",
      "***************************\n",
      "Episode 2681 finished (timesteps: 5388966/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: 5.44\n",
      "***************************\n",
      "Episode 2682 finished (timesteps: 5392700/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 5.72\n",
      "***************************\n",
      "Episode 2683 finished (timesteps: 5394906/10000000)\n",
      "Epsilon: 0.01, Episode reward: 19.0, Mean reward: 6.03\n",
      "***************************\n",
      "Episode 2684 finished (timesteps: 5397968/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 6.01\n",
      "***************************\n",
      "Episode 2685 finished (timesteps: 5403139/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 5.99\n",
      "***************************\n",
      "Episode 2686 finished (timesteps: 5408115/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 5.93\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2687 finished (timesteps: 5411991/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 6.08\n",
      "***************************\n",
      "Episode 2688 finished (timesteps: 5415976/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 6.04\n",
      "***************************\n",
      "Episode 2689 finished (timesteps: 5420274/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 5.90\n",
      "***************************\n",
      "Episode 2690 finished (timesteps: 5425547/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 6.03\n",
      "***************************\n",
      "Episode 2691 finished (timesteps: 5428829/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 6.13\n",
      "***************************\n",
      "Episode 2692 finished (timesteps: 5432201/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 6.21\n",
      "***************************\n",
      "Episode 2693 finished (timesteps: 5435436/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 6.28\n",
      "***************************\n",
      "Episode 2694 finished (timesteps: 5439407/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 6.27\n",
      "***************************\n",
      "Episode 2695 finished (timesteps: 5442995/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 6.49\n",
      "***************************\n",
      "Episode 2696 finished (timesteps: 5447660/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 6.43\n",
      "***************************\n",
      "Episode 2697 finished (timesteps: 5451583/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 6.46\n",
      "***************************\n",
      "Episode 2698 finished (timesteps: 5454696/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 6.58\n",
      "***************************\n",
      "Episode 2699 finished (timesteps: 5458198/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 6.60\n",
      "***************************\n",
      "Episode 2700 finished (timesteps: 5461483/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 6.74\n",
      "***************************\n",
      "Episode 2701 finished (timesteps: 5464819/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 6.78\n",
      "***************************\n",
      "Episode 2702 finished (timesteps: 5467017/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: 6.68\n",
      "***************************\n",
      "Episode 2703 finished (timesteps: 5469782/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: 6.47\n",
      "***************************\n",
      "Episode 2704 finished (timesteps: 5473032/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: 6.27\n",
      "***************************\n",
      "Episode 2705 finished (timesteps: 5476210/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 6.30\n",
      "***************************\n",
      "Episode 2706 finished (timesteps: 5479823/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 6.31\n",
      "***************************\n",
      "Episode 2707 finished (timesteps: 5483248/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 6.35\n",
      "***************************\n",
      "Episode 2708 finished (timesteps: 5486488/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 6.35\n",
      "***************************\n",
      "Episode 2709 finished (timesteps: 5490136/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 6.58\n",
      "***************************\n",
      "Episode 2710 finished (timesteps: 5493864/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 6.60\n",
      "***************************\n",
      "Episode 2711 finished (timesteps: 5497596/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: 6.40\n",
      "***************************\n",
      "Episode 2712 finished (timesteps: 5500972/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 6.33\n",
      "***************************\n",
      "Episode 2713 finished (timesteps: 5503854/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 6.33\n",
      "***************************\n",
      "Episode 2714 finished (timesteps: 5508745/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 6.32\n",
      "***************************\n",
      "Episode 2715 finished (timesteps: 5511625/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: 6.13\n",
      "***************************\n",
      "Episode 2716 finished (timesteps: 5515441/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 6.18\n",
      "***************************\n",
      "Episode 2717 finished (timesteps: 5519065/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 6.27\n",
      "***************************\n",
      "Episode 2718 finished (timesteps: 5522594/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 6.16\n",
      "***************************\n",
      "Episode 2719 finished (timesteps: 5525441/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 6.19\n",
      "***************************\n",
      "Episode 2720 finished (timesteps: 5528842/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 6.28\n",
      "***************************\n",
      "Episode 2721 finished (timesteps: 5531652/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 6.18\n",
      "***************************\n",
      "Episode 2722 finished (timesteps: 5535722/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: 6.13\n",
      "***************************\n",
      "Episode 2723 finished (timesteps: 5539412/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 6.10\n",
      "***************************\n",
      "Episode 2724 finished (timesteps: 5543264/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 6.00\n",
      "***************************\n",
      "Episode 2725 finished (timesteps: 5546176/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 6.00\n",
      "***************************\n",
      "Episode 2726 finished (timesteps: 5549510/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 6.03\n",
      "***************************\n",
      "Episode 2727 finished (timesteps: 5552103/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 6.10\n",
      "***************************\n",
      "Episode 2728 finished (timesteps: 5555150/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 6.17\n",
      "***************************\n",
      "Episode 2729 finished (timesteps: 5559055/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 6.09\n",
      "***************************\n",
      "Episode 2730 finished (timesteps: 5563814/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 6.26\n",
      "***************************\n",
      "Episode 2731 finished (timesteps: 5565910/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 6.35\n",
      "***************************\n",
      "Episode 2732 finished (timesteps: 5568525/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 6.36\n",
      "***************************\n",
      "Episode 2733 finished (timesteps: 5571771/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 6.44\n",
      "***************************\n",
      "Episode 2734 finished (timesteps: 5574211/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 6.50\n",
      "***************************\n",
      "Episode 2735 finished (timesteps: 5577992/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 6.52\n",
      "***************************\n",
      "Episode 2736 finished (timesteps: 5581279/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 6.52\n",
      "***************************\n",
      "Episode 2737 finished (timesteps: 5585691/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 6.49\n",
      "***************************\n",
      "Episode 2738 finished (timesteps: 5588740/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 6.42\n",
      "***************************\n",
      "Episode 2739 finished (timesteps: 5592321/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 6.22\n",
      "***************************\n",
      "Episode 2740 finished (timesteps: 5596891/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 6.14\n",
      "***************************\n",
      "Episode 2741 finished (timesteps: 5600099/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 6.25\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-5600000.mp4.\n",
      "Moviepy - Writing video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-5600000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-5600000.mp4\n",
      "Episode 2742 finished (timesteps: 5603172/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 6.34\n",
      "***************************\n",
      "Episode 2743 finished (timesteps: 5606642/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: 6.17\n",
      "***************************\n",
      "Episode 2744 finished (timesteps: 5609619/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 6.18\n",
      "***************************\n",
      "Episode 2745 finished (timesteps: 5613039/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 6.18\n",
      "***************************\n",
      "Episode 2746 finished (timesteps: 5616009/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 6.29\n",
      "***************************\n",
      "Episode 2747 finished (timesteps: 5618377/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 6.47\n",
      "***************************\n",
      "Episode 2748 finished (timesteps: 5622347/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 6.45\n",
      "***************************\n",
      "Episode 2749 finished (timesteps: 5626966/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 6.35\n",
      "***************************\n",
      "Episode 2750 finished (timesteps: 5630039/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 6.46\n",
      "***************************\n",
      "Episode 2751 finished (timesteps: 5633753/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 6.44\n",
      "***************************\n",
      "Episode 2752 finished (timesteps: 5636323/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 6.64\n",
      "***************************\n",
      "Episode 2753 finished (timesteps: 5638283/10000000)\n",
      "Epsilon: 0.01, Episode reward: 20.0, Mean reward: 6.79\n",
      "***************************\n",
      "Episode 2754 finished (timesteps: 5641453/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 6.79\n",
      "***************************\n",
      "Episode 2755 finished (timesteps: 5644545/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 6.88\n",
      "***************************\n",
      "Episode 2756 finished (timesteps: 5648883/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 6.71\n",
      "***************************\n",
      "Episode 2757 finished (timesteps: 5651120/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 6.75\n",
      "***************************\n",
      "Episode 2758 finished (timesteps: 5654041/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 6.77\n",
      "***************************\n",
      "Episode 2759 finished (timesteps: 5657533/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 6.82\n",
      "***************************\n",
      "Episode 2760 finished (timesteps: 5660701/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 7.09\n",
      "***************************\n",
      "Episode 2761 finished (timesteps: 5663215/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 7.22\n",
      "***************************\n",
      "Episode 2762 finished (timesteps: 5666430/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 7.27\n",
      "***************************\n",
      "Episode 2763 finished (timesteps: 5670203/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 7.25\n",
      "***************************\n",
      "Episode 2764 finished (timesteps: 5674390/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: 7.17\n",
      "***************************\n",
      "Episode 2765 finished (timesteps: 5676924/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 7.32\n",
      "***************************\n",
      "Episode 2766 finished (timesteps: 5680081/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 7.27\n",
      "***************************\n",
      "Episode 2767 finished (timesteps: 5683875/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 7.19\n",
      "***************************\n",
      "Episode 2768 finished (timesteps: 5687046/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 7.29\n",
      "***************************\n",
      "Episode 2769 finished (timesteps: 5690154/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 7.32\n",
      "***************************\n",
      "Episode 2770 finished (timesteps: 5694162/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: 7.17\n",
      "***************************\n",
      "Episode 2771 finished (timesteps: 5697204/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 7.27\n",
      "***************************\n",
      "Episode 2772 finished (timesteps: 5700429/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 7.33\n",
      "***************************\n",
      "Episode 2773 finished (timesteps: 5704258/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 7.24\n",
      "***************************\n",
      "Episode 2774 finished (timesteps: 5707694/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 7.09\n",
      "***************************\n",
      "Episode 2775 finished (timesteps: 5711402/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 7.12\n",
      "***************************\n",
      "Episode 2776 finished (timesteps: 5714954/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 7.09\n",
      "***************************\n",
      "Episode 2777 finished (timesteps: 5719240/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 7.04\n",
      "***************************\n",
      "Episode 2778 finished (timesteps: 5723071/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 7.00\n",
      "***************************\n",
      "Episode 2779 finished (timesteps: 5727065/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 6.99\n",
      "***************************\n",
      "Episode 2780 finished (timesteps: 5730456/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 6.95\n",
      "***************************\n",
      "Episode 2781 finished (timesteps: 5733865/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 7.14\n",
      "***************************\n",
      "Episode 2782 finished (timesteps: 5737727/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 7.02\n",
      "***************************\n",
      "Episode 2783 finished (timesteps: 5740311/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 6.99\n",
      "***************************\n",
      "Episode 2784 finished (timesteps: 5743391/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 6.98\n",
      "***************************\n",
      "Episode 2785 finished (timesteps: 5746524/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 7.07\n",
      "***************************\n",
      "Episode 2786 finished (timesteps: 5749358/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 7.11\n",
      "***************************\n",
      "Episode 2787 finished (timesteps: 5752277/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 7.21\n",
      "***************************\n",
      "Episode 2788 finished (timesteps: 5755616/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 7.30\n",
      "***************************\n",
      "Episode 2789 finished (timesteps: 5758751/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 7.39\n",
      "***************************\n",
      "Episode 2790 finished (timesteps: 5762781/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 7.36\n",
      "***************************\n",
      "Episode 2791 finished (timesteps: 5765815/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 7.36\n",
      "***************************\n",
      "Episode 2792 finished (timesteps: 5768417/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 7.39\n",
      "***************************\n",
      "Episode 2793 finished (timesteps: 5771356/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: 7.18\n",
      "***************************\n",
      "Episode 2794 finished (timesteps: 5774287/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 7.24\n",
      "***************************\n",
      "Episode 2795 finished (timesteps: 5777982/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: 7.17\n",
      "***************************\n",
      "Episode 2796 finished (timesteps: 5781473/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 7.19\n",
      "***************************\n",
      "Episode 2797 finished (timesteps: 5784469/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 7.20\n",
      "***************************\n",
      "Episode 2798 finished (timesteps: 5788200/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 7.11\n",
      "***************************\n",
      "Episode 2799 finished (timesteps: 5791848/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 7.03\n",
      "***************************\n",
      "Episode 2800 finished (timesteps: 5794891/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 7.06\n",
      "***************************\n",
      "Episode 2801 finished (timesteps: 5798198/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 7.07\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2802 finished (timesteps: 5801496/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 7.33\n",
      "***************************\n",
      "Episode 2803 finished (timesteps: 5805805/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: 7.39\n",
      "***************************\n",
      "Episode 2804 finished (timesteps: 5808575/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 7.61\n",
      "***************************\n",
      "Episode 2805 finished (timesteps: 5812224/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 7.57\n",
      "***************************\n",
      "Episode 2806 finished (timesteps: 5813484/10000000)\n",
      "Epsilon: 0.01, Episode reward: -20.0, Mean reward: 7.32\n",
      "***************************\n",
      "Episode 2807 finished (timesteps: 5816104/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: 7.15\n",
      "***************************\n",
      "Episode 2808 finished (timesteps: 5820086/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 7.19\n",
      "***************************\n",
      "Episode 2809 finished (timesteps: 5823280/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: 7.00\n",
      "***************************\n",
      "Episode 2810 finished (timesteps: 5826684/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 6.87\n",
      "***************************\n",
      "Episode 2811 finished (timesteps: 5829369/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 7.05\n",
      "***************************\n",
      "Episode 2812 finished (timesteps: 5834995/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 6.97\n",
      "***************************\n",
      "Episode 2813 finished (timesteps: 5839279/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 6.96\n",
      "***************************\n",
      "Episode 2814 finished (timesteps: 5842782/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 6.95\n",
      "***************************\n",
      "Episode 2815 finished (timesteps: 5846701/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 7.05\n",
      "***************************\n",
      "Episode 2816 finished (timesteps: 5849256/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 7.06\n",
      "***************************\n",
      "Episode 2817 finished (timesteps: 5853703/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 7.16\n",
      "***************************\n",
      "Episode 2818 finished (timesteps: 5857119/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 7.24\n",
      "***************************\n",
      "Episode 2819 finished (timesteps: 5859653/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 7.27\n",
      "***************************\n",
      "Episode 2820 finished (timesteps: 5863487/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 7.26\n",
      "***************************\n",
      "Episode 2821 finished (timesteps: 5866026/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: 7.09\n",
      "***************************\n",
      "Episode 2822 finished (timesteps: 5869772/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 7.24\n",
      "***************************\n",
      "Episode 2823 finished (timesteps: 5873401/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 7.22\n",
      "***************************\n",
      "Episode 2824 finished (timesteps: 5875726/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 7.42\n",
      "***************************\n",
      "Episode 2825 finished (timesteps: 5879122/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 7.48\n",
      "***************************\n",
      "Episode 2826 finished (timesteps: 5882206/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 7.61\n",
      "***************************\n",
      "Episode 2827 finished (timesteps: 5885927/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 7.64\n",
      "***************************\n",
      "Episode 2828 finished (timesteps: 5889487/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 7.55\n",
      "***************************\n",
      "Episode 2829 finished (timesteps: 5893142/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 7.66\n",
      "***************************\n",
      "Episode 2830 finished (timesteps: 5896524/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 7.68\n",
      "***************************\n",
      "Episode 2831 finished (timesteps: 5900634/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 7.57\n",
      "***************************\n",
      "Episode 2832 finished (timesteps: 5904074/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 7.48\n",
      "***************************\n",
      "Episode 2833 finished (timesteps: 5906928/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 7.50\n",
      "***************************\n",
      "Episode 2834 finished (timesteps: 5909536/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 7.53\n",
      "***************************\n",
      "Episode 2835 finished (timesteps: 5912885/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 7.52\n",
      "***************************\n",
      "Episode 2836 finished (timesteps: 5915707/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: 7.34\n",
      "***************************\n",
      "Episode 2837 finished (timesteps: 5919422/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: 7.28\n",
      "***************************\n",
      "Episode 2838 finished (timesteps: 5924722/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 7.21\n",
      "***************************\n",
      "Episode 2839 finished (timesteps: 5928331/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 7.38\n",
      "***************************\n",
      "Episode 2840 finished (timesteps: 5932221/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 7.45\n",
      "***************************\n",
      "Episode 2841 finished (timesteps: 5937528/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 7.38\n",
      "***************************\n",
      "Episode 2842 finished (timesteps: 5940585/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 7.33\n",
      "***************************\n",
      "Episode 2843 finished (timesteps: 5944194/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 7.44\n",
      "***************************\n",
      "Episode 2844 finished (timesteps: 5947943/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 7.43\n",
      "***************************\n",
      "Episode 2845 finished (timesteps: 5951615/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 7.44\n",
      "***************************\n",
      "Episode 2846 finished (timesteps: 5955866/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 7.25\n",
      "***************************\n",
      "Episode 2847 finished (timesteps: 5958862/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: 7.02\n",
      "***************************\n",
      "Episode 2848 finished (timesteps: 5963070/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 6.92\n",
      "***************************\n",
      "Episode 2849 finished (timesteps: 5965932/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 6.97\n",
      "***************************\n",
      "Episode 2850 finished (timesteps: 5970021/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 6.84\n",
      "***************************\n",
      "Episode 2851 finished (timesteps: 5972274/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 7.00\n",
      "***************************\n",
      "Episode 2852 finished (timesteps: 5975596/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 6.94\n",
      "***************************\n",
      "Episode 2853 finished (timesteps: 5980473/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 6.78\n",
      "***************************\n",
      "Episode 2854 finished (timesteps: 5983374/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 6.83\n",
      "***************************\n",
      "Episode 2855 finished (timesteps: 5987983/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 6.74\n",
      "***************************\n",
      "Episode 2856 finished (timesteps: 5990864/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: 6.64\n",
      "***************************\n",
      "Episode 2857 finished (timesteps: 5995701/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 6.57\n",
      "***************************\n",
      "Episode 2858 finished (timesteps: 6000001/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: 6.39\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-6000000.mp4.\n",
      "Moviepy - Writing video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-6000000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-6000000.mp4\n",
      "Episode 2859 finished (timesteps: 6002630/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 6.42\n",
      "***************************\n",
      "Episode 2860 finished (timesteps: 6006062/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 6.46\n",
      "***************************\n",
      "Episode 2861 finished (timesteps: 6011190/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 6.30\n",
      "***************************\n",
      "Episode 2862 finished (timesteps: 6014718/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 6.28\n",
      "***************************\n",
      "Episode 2863 finished (timesteps: 6018471/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 6.34\n",
      "***************************\n",
      "Episode 2864 finished (timesteps: 6023345/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 6.48\n",
      "***************************\n",
      "Episode 2865 finished (timesteps: 6028672/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 6.45\n",
      "***************************\n",
      "Episode 2866 finished (timesteps: 6033064/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 6.45\n",
      "***************************\n",
      "Episode 2867 finished (timesteps: 6035915/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: 6.35\n",
      "***************************\n",
      "Episode 2868 finished (timesteps: 6039270/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 6.36\n",
      "***************************\n",
      "Episode 2869 finished (timesteps: 6043448/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 6.33\n",
      "***************************\n",
      "Episode 2870 finished (timesteps: 6046560/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 6.51\n",
      "***************************\n",
      "Episode 2871 finished (timesteps: 6049800/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 6.38\n",
      "***************************\n",
      "Episode 2872 finished (timesteps: 6053820/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 6.43\n",
      "***************************\n",
      "Episode 2873 finished (timesteps: 6058918/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 6.48\n",
      "***************************\n",
      "Episode 2874 finished (timesteps: 6062953/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 6.60\n",
      "***************************\n",
      "Episode 2875 finished (timesteps: 6066460/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 6.65\n",
      "***************************\n",
      "Episode 2876 finished (timesteps: 6069957/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 6.74\n",
      "***************************\n",
      "Episode 2877 finished (timesteps: 6072618/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 6.80\n",
      "***************************\n",
      "Episode 2878 finished (timesteps: 6076821/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 6.80\n",
      "***************************\n",
      "Episode 2879 finished (timesteps: 6079347/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 6.79\n",
      "***************************\n",
      "Episode 2880 finished (timesteps: 6082151/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 6.81\n",
      "***************************\n",
      "Episode 2881 finished (timesteps: 6084766/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 6.81\n",
      "***************************\n",
      "Episode 2882 finished (timesteps: 6087272/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 6.88\n",
      "***************************\n",
      "Episode 2883 finished (timesteps: 6091696/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 6.74\n",
      "***************************\n",
      "Episode 2884 finished (timesteps: 6094593/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 6.73\n",
      "***************************\n",
      "Episode 2885 finished (timesteps: 6098355/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 6.66\n",
      "***************************\n",
      "Episode 2886 finished (timesteps: 6100439/10000000)\n",
      "Epsilon: 0.01, Episode reward: 19.0, Mean reward: 6.75\n",
      "***************************\n",
      "Episode 2887 finished (timesteps: 6104169/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 6.66\n",
      "***************************\n",
      "Episode 2888 finished (timesteps: 6107345/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 6.64\n",
      "***************************\n",
      "Episode 2889 finished (timesteps: 6111273/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 6.52\n",
      "***************************\n",
      "Episode 2890 finished (timesteps: 6113820/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 6.65\n",
      "***************************\n",
      "Episode 2891 finished (timesteps: 6116998/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 6.70\n",
      "***************************\n",
      "Episode 2892 finished (timesteps: 6120631/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 6.71\n",
      "***************************\n",
      "Episode 2893 finished (timesteps: 6124589/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 6.73\n",
      "***************************\n",
      "Episode 2894 finished (timesteps: 6128057/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 6.70\n",
      "***************************\n",
      "Episode 2895 finished (timesteps: 6131441/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 6.80\n",
      "***************************\n",
      "Episode 2896 finished (timesteps: 6134862/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 6.97\n",
      "***************************\n",
      "Episode 2897 finished (timesteps: 6137921/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 6.96\n",
      "***************************\n",
      "Episode 2898 finished (timesteps: 6141151/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 7.01\n",
      "***************************\n",
      "Episode 2899 finished (timesteps: 6142833/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: 6.74\n",
      "***************************\n",
      "Episode 2900 finished (timesteps: 6146236/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 6.67\n",
      "***************************\n",
      "Episode 2901 finished (timesteps: 6151079/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 6.57\n",
      "***************************\n",
      "Episode 2902 finished (timesteps: 6154004/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 6.55\n",
      "***************************\n",
      "Episode 2903 finished (timesteps: 6156507/10000000)\n",
      "Epsilon: 0.01, Episode reward: 20.0, Mean reward: 6.76\n",
      "***************************\n",
      "Episode 2904 finished (timesteps: 6159879/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 6.79\n",
      "***************************\n",
      "Episode 2905 finished (timesteps: 6163510/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 6.88\n",
      "***************************\n",
      "Episode 2906 finished (timesteps: 6167376/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 7.20\n",
      "***************************\n",
      "Episode 2907 finished (timesteps: 6172000/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 7.36\n",
      "***************************\n",
      "Episode 2908 finished (timesteps: 6175535/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 7.32\n",
      "***************************\n",
      "Episode 2909 finished (timesteps: 6179767/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 7.46\n",
      "***************************\n",
      "Episode 2910 finished (timesteps: 6182011/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 7.69\n",
      "***************************\n",
      "Episode 2911 finished (timesteps: 6185635/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 7.64\n",
      "***************************\n",
      "Episode 2912 finished (timesteps: 6189608/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 7.76\n",
      "***************************\n",
      "Episode 2913 finished (timesteps: 6192162/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: 7.57\n",
      "***************************\n",
      "Episode 2914 finished (timesteps: 6194780/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 7.65\n",
      "***************************\n",
      "Episode 2915 finished (timesteps: 6198535/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 7.75\n",
      "***************************\n",
      "Episode 2916 finished (timesteps: 6201841/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 7.73\n",
      "***************************\n",
      "Episode 2917 finished (timesteps: 6205338/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 7.71\n",
      "***************************\n",
      "Episode 2918 finished (timesteps: 6208722/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 7.76\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2919 finished (timesteps: 6211690/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 7.73\n",
      "***************************\n",
      "Episode 2920 finished (timesteps: 6214313/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 7.94\n",
      "***************************\n",
      "Episode 2921 finished (timesteps: 6217061/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 8.18\n",
      "***************************\n",
      "Episode 2922 finished (timesteps: 6220445/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 8.12\n",
      "***************************\n",
      "Episode 2923 finished (timesteps: 6223469/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: 7.99\n",
      "***************************\n",
      "Episode 2924 finished (timesteps: 6227147/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 7.90\n",
      "***************************\n",
      "Episode 2925 finished (timesteps: 6230386/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 7.90\n",
      "***************************\n",
      "Episode 2926 finished (timesteps: 6233674/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 7.86\n",
      "***************************\n",
      "Episode 2927 finished (timesteps: 6237109/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 7.77\n",
      "***************************\n",
      "Episode 2928 finished (timesteps: 6240718/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 7.89\n",
      "***************************\n",
      "Episode 2929 finished (timesteps: 6244588/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 7.89\n",
      "***************************\n",
      "Episode 2930 finished (timesteps: 6248118/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 7.92\n",
      "***************************\n",
      "Episode 2931 finished (timesteps: 6251484/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 7.92\n",
      "***************************\n",
      "Episode 2932 finished (timesteps: 6254943/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 7.82\n",
      "***************************\n",
      "Episode 2933 finished (timesteps: 6260213/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 7.63\n",
      "***************************\n",
      "Episode 2934 finished (timesteps: 6263101/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 7.41\n",
      "***************************\n",
      "Episode 2935 finished (timesteps: 6266322/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 7.41\n",
      "***************************\n",
      "Episode 2936 finished (timesteps: 6270243/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 7.50\n",
      "***************************\n",
      "Episode 2937 finished (timesteps: 6275082/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: 7.52\n",
      "***************************\n",
      "Episode 2938 finished (timesteps: 6278699/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 7.65\n",
      "***************************\n",
      "Episode 2939 finished (timesteps: 6282897/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 7.58\n",
      "***************************\n",
      "Episode 2940 finished (timesteps: 6286069/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 7.59\n",
      "***************************\n",
      "Episode 2941 finished (timesteps: 6288868/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 7.63\n",
      "***************************\n",
      "Episode 2942 finished (timesteps: 6292896/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 7.62\n",
      "***************************\n",
      "Episode 2943 finished (timesteps: 6296072/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 7.72\n",
      "***************************\n",
      "Episode 2944 finished (timesteps: 6298903/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 7.73\n",
      "***************************\n",
      "Episode 2945 finished (timesteps: 6302981/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 7.69\n",
      "***************************\n",
      "Episode 2946 finished (timesteps: 6307250/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 7.84\n",
      "***************************\n",
      "Episode 2947 finished (timesteps: 6312279/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: 7.94\n",
      "***************************\n",
      "Episode 2948 finished (timesteps: 6316676/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: 7.84\n",
      "***************************\n",
      "Episode 2949 finished (timesteps: 6321009/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 7.81\n",
      "***************************\n",
      "Episode 2950 finished (timesteps: 6325237/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 7.82\n",
      "***************************\n",
      "Episode 2951 finished (timesteps: 6327998/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: 7.52\n",
      "***************************\n",
      "Episode 2952 finished (timesteps: 6332128/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 7.56\n",
      "***************************\n",
      "Episode 2953 finished (timesteps: 6336221/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 7.62\n",
      "***************************\n",
      "Episode 2954 finished (timesteps: 6340013/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 7.56\n",
      "***************************\n",
      "Episode 2955 finished (timesteps: 6342694/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 7.68\n",
      "***************************\n",
      "Episode 2956 finished (timesteps: 6346786/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 7.76\n",
      "***************************\n",
      "Episode 2957 finished (timesteps: 6350245/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 7.78\n",
      "***************************\n",
      "Episode 2958 finished (timesteps: 6353703/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 7.94\n",
      "***************************\n",
      "Episode 2959 finished (timesteps: 6356302/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: 7.67\n",
      "***************************\n",
      "Episode 2960 finished (timesteps: 6360394/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 7.60\n",
      "***************************\n",
      "Episode 2961 finished (timesteps: 6363297/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 7.70\n",
      "***************************\n",
      "Episode 2962 finished (timesteps: 6368546/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: 7.60\n",
      "***************************\n",
      "Episode 2963 finished (timesteps: 6372055/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 7.42\n",
      "***************************\n",
      "Episode 2964 finished (timesteps: 6376116/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 7.42\n",
      "***************************\n",
      "Episode 2965 finished (timesteps: 6379218/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 7.45\n",
      "***************************\n",
      "Episode 2966 finished (timesteps: 6382333/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 7.45\n",
      "***************************\n",
      "Episode 2967 finished (timesteps: 6386817/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 7.55\n",
      "***************************\n",
      "Episode 2968 finished (timesteps: 6389572/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 7.60\n",
      "***************************\n",
      "Episode 2969 finished (timesteps: 6392241/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 7.67\n",
      "***************************\n",
      "Episode 2970 finished (timesteps: 6396754/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 7.58\n",
      "***************************\n",
      "Episode 2971 finished (timesteps: 6400436/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 7.69\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-6400000.mp4.\n",
      "Moviepy - Writing video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-6400000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-6400000.mp4\n",
      "Episode 2972 finished (timesteps: 6402230/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: 7.49\n",
      "***************************\n",
      "Episode 2973 finished (timesteps: 6403397/10000000)\n",
      "Epsilon: 0.01, Episode reward: -18.0, Mean reward: 7.24\n",
      "***************************\n",
      "Episode 2974 finished (timesteps: 6405975/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: 7.05\n",
      "***************************\n",
      "Episode 2975 finished (timesteps: 6410217/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 7.02\n",
      "***************************\n",
      "Episode 2976 finished (timesteps: 6413199/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 6.94\n",
      "***************************\n",
      "Episode 2977 finished (timesteps: 6416106/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 6.94\n",
      "***************************\n",
      "Episode 2978 finished (timesteps: 6419363/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 6.86\n",
      "***************************\n",
      "Episode 2979 finished (timesteps: 6422761/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 6.86\n",
      "***************************\n",
      "Episode 2980 finished (timesteps: 6428269/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 6.77\n",
      "***************************\n",
      "Episode 2981 finished (timesteps: 6432155/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 6.66\n",
      "***************************\n",
      "Episode 2982 finished (timesteps: 6435697/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 6.63\n",
      "***************************\n",
      "Episode 2983 finished (timesteps: 6438350/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 6.76\n",
      "***************************\n",
      "Episode 2984 finished (timesteps: 6442353/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 6.70\n",
      "***************************\n",
      "Episode 2985 finished (timesteps: 6446279/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 6.70\n",
      "***************************\n",
      "Episode 2986 finished (timesteps: 6449745/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 6.59\n",
      "***************************\n",
      "Episode 2987 finished (timesteps: 6452653/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 6.50\n",
      "***************************\n",
      "Episode 2988 finished (timesteps: 6457996/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 6.40\n",
      "***************************\n",
      "Episode 2989 finished (timesteps: 6461602/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: 6.36\n",
      "***************************\n",
      "Episode 2990 finished (timesteps: 6466126/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: 6.19\n",
      "***************************\n",
      "Episode 2991 finished (timesteps: 6470259/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 6.05\n",
      "***************************\n",
      "Episode 2992 finished (timesteps: 6475015/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 5.98\n",
      "***************************\n",
      "Episode 2993 finished (timesteps: 6479212/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 6.05\n",
      "***************************\n",
      "Episode 2994 finished (timesteps: 6482065/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 6.01\n",
      "***************************\n",
      "Episode 2995 finished (timesteps: 6484963/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 6.10\n",
      "***************************\n",
      "Episode 2996 finished (timesteps: 6488147/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 6.09\n",
      "***************************\n",
      "Episode 2997 finished (timesteps: 6492456/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 5.96\n",
      "***************************\n",
      "Episode 2998 finished (timesteps: 6496200/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 5.92\n",
      "***************************\n",
      "Episode 2999 finished (timesteps: 6498434/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 6.27\n",
      "***************************\n",
      "Episode 3000 finished (timesteps: 6502593/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 6.25\n",
      "Saving checkpoint...\n",
      "Checkpoint saved into runs/checkpoints/CNN_DDQN_Pong-v5_11-04-2023_16:28:30\n",
      "***************************\n",
      "Episode 3001 finished (timesteps: 6506004/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 6.25\n",
      "***************************\n",
      "Episode 3002 finished (timesteps: 6508856/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 6.23\n",
      "***************************\n",
      "Episode 3003 finished (timesteps: 6512849/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 6.12\n",
      "***************************\n",
      "Episode 3004 finished (timesteps: 6516916/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 6.05\n",
      "***************************\n",
      "Episode 3005 finished (timesteps: 6517766/10000000)\n",
      "Epsilon: 0.01, Episode reward: -21.0, Mean reward: 5.70\n",
      "***************************\n",
      "Episode 3006 finished (timesteps: 6520544/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 5.69\n",
      "***************************\n",
      "Episode 3007 finished (timesteps: 6524894/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: 5.62\n",
      "***************************\n",
      "Episode 3008 finished (timesteps: 6529637/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: 5.54\n",
      "***************************\n",
      "Episode 3009 finished (timesteps: 6533678/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 5.58\n",
      "***************************\n",
      "Episode 3010 finished (timesteps: 6537512/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 5.48\n",
      "***************************\n",
      "Episode 3011 finished (timesteps: 6540720/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 5.31\n",
      "***************************\n",
      "Episode 3012 finished (timesteps: 6543549/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 5.25\n",
      "***************************\n",
      "Episode 3013 finished (timesteps: 6546416/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 5.47\n",
      "***************************\n",
      "Episode 3014 finished (timesteps: 6550101/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 5.35\n",
      "***************************\n",
      "Episode 3015 finished (timesteps: 6553414/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 5.40\n",
      "***************************\n",
      "Episode 3016 finished (timesteps: 6556874/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 5.35\n",
      "***************************\n",
      "Episode 3017 finished (timesteps: 6559594/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 5.40\n",
      "***************************\n",
      "Episode 3018 finished (timesteps: 6563657/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 5.39\n",
      "***************************\n",
      "Episode 3019 finished (timesteps: 6566714/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 5.41\n",
      "***************************\n",
      "Episode 3020 finished (timesteps: 6570414/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 5.32\n",
      "***************************\n",
      "Episode 3021 finished (timesteps: 6574517/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 5.25\n",
      "***************************\n",
      "Episode 3022 finished (timesteps: 6578093/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 5.28\n",
      "***************************\n",
      "Episode 3023 finished (timesteps: 6581713/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 5.52\n",
      "***************************\n",
      "Episode 3024 finished (timesteps: 6585492/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 5.57\n",
      "***************************\n",
      "Episode 3025 finished (timesteps: 6589201/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 5.53\n",
      "***************************\n",
      "Episode 3026 finished (timesteps: 6593729/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 5.44\n",
      "***************************\n",
      "Episode 3027 finished (timesteps: 6597485/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 5.44\n",
      "***************************\n",
      "Episode 3028 finished (timesteps: 6602377/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 5.40\n",
      "***************************\n",
      "Episode 3029 finished (timesteps: 6604662/10000000)\n",
      "Epsilon: 0.01, Episode reward: 19.0, Mean reward: 5.50\n",
      "***************************\n",
      "Episode 3030 finished (timesteps: 6609703/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 5.45\n",
      "***************************\n",
      "Episode 3031 finished (timesteps: 6613429/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 5.45\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3032 finished (timesteps: 6617500/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 5.55\n",
      "***************************\n",
      "Episode 3033 finished (timesteps: 6619616/10000000)\n",
      "Epsilon: 0.01, Episode reward: 21.0, Mean reward: 5.78\n",
      "***************************\n",
      "Episode 3034 finished (timesteps: 6623416/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 5.91\n",
      "***************************\n",
      "Episode 3035 finished (timesteps: 6627709/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 5.87\n",
      "***************************\n",
      "Episode 3036 finished (timesteps: 6632274/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 5.91\n",
      "***************************\n",
      "Episode 3037 finished (timesteps: 6636016/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 5.87\n",
      "***************************\n",
      "Episode 3038 finished (timesteps: 6638602/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 5.91\n",
      "***************************\n",
      "Episode 3039 finished (timesteps: 6641234/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 6.01\n",
      "***************************\n",
      "Episode 3040 finished (timesteps: 6645188/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 5.98\n",
      "***************************\n",
      "Episode 3041 finished (timesteps: 6648878/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 5.87\n",
      "***************************\n",
      "Episode 3042 finished (timesteps: 6651742/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 5.90\n",
      "***************************\n",
      "Episode 3043 finished (timesteps: 6653779/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 5.89\n",
      "***************************\n",
      "Episode 3044 finished (timesteps: 6657363/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 5.83\n",
      "***************************\n",
      "Episode 3045 finished (timesteps: 6659364/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 5.91\n",
      "***************************\n",
      "Episode 3046 finished (timesteps: 6662734/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 5.89\n",
      "***************************\n",
      "Episode 3047 finished (timesteps: 6666304/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 5.99\n",
      "***************************\n",
      "Episode 3048 finished (timesteps: 6669500/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 6.15\n",
      "***************************\n",
      "Episode 3049 finished (timesteps: 6672700/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 6.13\n",
      "***************************\n",
      "Episode 3050 finished (timesteps: 6675451/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 6.17\n",
      "***************************\n",
      "Episode 3051 finished (timesteps: 6678361/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 6.43\n",
      "***************************\n",
      "Episode 3052 finished (timesteps: 6681654/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 6.36\n",
      "***************************\n",
      "Episode 3053 finished (timesteps: 6684454/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 6.37\n",
      "***************************\n",
      "Episode 3054 finished (timesteps: 6686908/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 6.45\n",
      "***************************\n",
      "Episode 3055 finished (timesteps: 6689874/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 6.40\n",
      "***************************\n",
      "Episode 3056 finished (timesteps: 6693576/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 6.45\n",
      "***************************\n",
      "Episode 3057 finished (timesteps: 6697057/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 6.41\n",
      "***************************\n",
      "Episode 3058 finished (timesteps: 6700170/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 6.41\n",
      "***************************\n",
      "Episode 3059 finished (timesteps: 6702859/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 6.64\n",
      "***************************\n",
      "Episode 3060 finished (timesteps: 6706282/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 6.73\n",
      "***************************\n",
      "Episode 3061 finished (timesteps: 6709952/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 6.73\n",
      "***************************\n",
      "Episode 3062 finished (timesteps: 6713255/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 6.72\n",
      "***************************\n",
      "Episode 3063 finished (timesteps: 6716263/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 6.86\n",
      "***************************\n",
      "Episode 3064 finished (timesteps: 6719148/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 6.82\n",
      "***************************\n",
      "Episode 3065 finished (timesteps: 6722336/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 6.78\n",
      "***************************\n",
      "Episode 3066 finished (timesteps: 6725801/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 6.65\n",
      "***************************\n",
      "Episode 3067 finished (timesteps: 6729179/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 6.72\n",
      "***************************\n",
      "Episode 3068 finished (timesteps: 6732877/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 6.50\n",
      "***************************\n",
      "Episode 3069 finished (timesteps: 6735759/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 6.35\n",
      "***************************\n",
      "Episode 3070 finished (timesteps: 6738782/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 6.28\n",
      "***************************\n",
      "Episode 3071 finished (timesteps: 6743343/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 6.25\n",
      "***************************\n",
      "Episode 3072 finished (timesteps: 6745728/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 6.50\n",
      "***************************\n",
      "Episode 3073 finished (timesteps: 6748429/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 6.83\n",
      "***************************\n",
      "Episode 3074 finished (timesteps: 6751783/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 7.05\n",
      "***************************\n",
      "Episode 3075 finished (timesteps: 6755108/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 7.09\n",
      "***************************\n",
      "Episode 3076 finished (timesteps: 6757397/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 7.22\n",
      "***************************\n",
      "Episode 3077 finished (timesteps: 6761951/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 7.19\n",
      "***************************\n",
      "Episode 3078 finished (timesteps: 6764647/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 7.37\n",
      "***************************\n",
      "Episode 3079 finished (timesteps: 6768838/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: 7.25\n",
      "***************************\n",
      "Episode 3080 finished (timesteps: 6772552/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 7.19\n",
      "***************************\n",
      "Episode 3081 finished (timesteps: 6774980/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 7.34\n",
      "***************************\n",
      "Episode 3082 finished (timesteps: 6778226/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 7.35\n",
      "***************************\n",
      "Episode 3083 finished (timesteps: 6781027/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 7.36\n",
      "***************************\n",
      "Episode 3084 finished (timesteps: 6785900/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 7.37\n",
      "***************************\n",
      "Episode 3085 finished (timesteps: 6789441/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 7.44\n",
      "***************************\n",
      "Episode 3086 finished (timesteps: 6792939/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 7.52\n",
      "***************************\n",
      "Episode 3087 finished (timesteps: 6798476/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 7.60\n",
      "***************************\n",
      "Episode 3088 finished (timesteps: 6800837/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 7.78\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-6800000.mp4.\n",
      "Moviepy - Writing video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-6800000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-6800000.mp4\n",
      "Episode 3089 finished (timesteps: 6803890/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 7.92\n",
      "***************************\n",
      "Episode 3090 finished (timesteps: 6807239/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 8.09\n",
      "***************************\n",
      "Episode 3091 finished (timesteps: 6810547/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 8.17\n",
      "***************************\n",
      "Episode 3092 finished (timesteps: 6813987/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 8.20\n",
      "***************************\n",
      "Episode 3093 finished (timesteps: 6818548/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 8.28\n",
      "***************************\n",
      "Episode 3094 finished (timesteps: 6821586/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 8.32\n",
      "***************************\n",
      "Episode 3095 finished (timesteps: 6825071/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: 8.06\n",
      "***************************\n",
      "Episode 3096 finished (timesteps: 6826879/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: 7.78\n",
      "***************************\n",
      "Episode 3097 finished (timesteps: 6831336/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 7.78\n",
      "***************************\n",
      "Episode 3098 finished (timesteps: 6836239/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 7.70\n",
      "***************************\n",
      "Episode 3099 finished (timesteps: 6840883/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 7.58\n",
      "***************************\n",
      "Episode 3100 finished (timesteps: 6845604/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 7.60\n",
      "***************************\n",
      "Episode 3101 finished (timesteps: 6848831/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 7.72\n",
      "***************************\n",
      "Episode 3102 finished (timesteps: 6853222/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 7.66\n",
      "***************************\n",
      "Episode 3103 finished (timesteps: 6857965/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 7.64\n",
      "***************************\n",
      "Episode 3104 finished (timesteps: 6861164/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 7.66\n",
      "***************************\n",
      "Episode 3105 finished (timesteps: 6864868/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 7.96\n",
      "***************************\n",
      "Episode 3106 finished (timesteps: 6868163/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 8.01\n",
      "***************************\n",
      "Episode 3107 finished (timesteps: 6872441/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 8.15\n",
      "***************************\n",
      "Episode 3108 finished (timesteps: 6877095/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.28\n",
      "***************************\n",
      "Episode 3109 finished (timesteps: 6880736/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 8.30\n",
      "***************************\n",
      "Episode 3110 finished (timesteps: 6885075/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 8.30\n",
      "***************************\n",
      "Episode 3111 finished (timesteps: 6888430/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 8.45\n",
      "***************************\n",
      "Episode 3112 finished (timesteps: 6892990/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: 8.37\n",
      "***************************\n",
      "Episode 3113 finished (timesteps: 6897992/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 8.35\n",
      "***************************\n",
      "Episode 3114 finished (timesteps: 6902407/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: 8.29\n",
      "***************************\n",
      "Episode 3115 finished (timesteps: 6907134/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 8.26\n",
      "***************************\n",
      "Episode 3116 finished (timesteps: 6910187/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 8.29\n",
      "***************************\n",
      "Episode 3117 finished (timesteps: 6912934/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 8.30\n",
      "***************************\n",
      "Episode 3118 finished (timesteps: 6917736/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 8.30\n",
      "***************************\n",
      "Episode 3119 finished (timesteps: 6920492/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 8.30\n",
      "***************************\n",
      "Episode 3120 finished (timesteps: 6922887/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 8.41\n",
      "***************************\n",
      "Episode 3121 finished (timesteps: 6926822/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 8.43\n",
      "***************************\n",
      "Episode 3122 finished (timesteps: 6929961/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 8.42\n",
      "***************************\n",
      "Episode 3123 finished (timesteps: 6932153/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 8.42\n",
      "***************************\n",
      "Episode 3124 finished (timesteps: 6935074/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 8.42\n",
      "***************************\n",
      "Episode 3125 finished (timesteps: 6938463/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 8.39\n",
      "***************************\n",
      "Episode 3126 finished (timesteps: 6941333/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 8.47\n",
      "***************************\n",
      "Episode 3127 finished (timesteps: 6945842/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 8.42\n",
      "***************************\n",
      "Episode 3128 finished (timesteps: 6948772/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 8.48\n",
      "***************************\n",
      "Episode 3129 finished (timesteps: 6952308/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 8.40\n",
      "***************************\n",
      "Episode 3130 finished (timesteps: 6955519/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 8.41\n",
      "***************************\n",
      "Episode 3131 finished (timesteps: 6958151/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 8.49\n",
      "***************************\n",
      "Episode 3132 finished (timesteps: 6960999/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 8.37\n",
      "***************************\n",
      "Episode 3133 finished (timesteps: 6962025/10000000)\n",
      "Epsilon: 0.01, Episode reward: -19.0, Mean reward: 7.97\n",
      "***************************\n",
      "Episode 3134 finished (timesteps: 6962924/10000000)\n",
      "Epsilon: 0.01, Episode reward: -20.0, Mean reward: 7.68\n",
      "***************************\n",
      "Episode 3135 finished (timesteps: 6966385/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 7.57\n",
      "***************************\n",
      "Episode 3136 finished (timesteps: 6969509/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 7.58\n",
      "***************************\n",
      "Episode 3137 finished (timesteps: 6972975/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 7.75\n",
      "***************************\n",
      "Episode 3138 finished (timesteps: 6975603/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 7.73\n",
      "***************************\n",
      "Episode 3139 finished (timesteps: 6978612/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 7.75\n",
      "***************************\n",
      "Episode 3140 finished (timesteps: 6982133/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 7.61\n",
      "***************************\n",
      "Episode 3141 finished (timesteps: 6984856/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 7.73\n",
      "***************************\n",
      "Episode 3142 finished (timesteps: 6987312/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 7.54\n",
      "***************************\n",
      "Episode 3143 finished (timesteps: 6990918/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 7.47\n",
      "***************************\n",
      "Episode 3144 finished (timesteps: 6994342/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 7.48\n",
      "***************************\n",
      "Episode 3145 finished (timesteps: 6997012/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 7.47\n",
      "***************************\n",
      "Episode 3146 finished (timesteps: 7000453/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 7.56\n",
      "***************************\n",
      "Episode 3147 finished (timesteps: 7002593/10000000)\n",
      "Epsilon: 0.01, Episode reward: 19.0, Mean reward: 7.66\n",
      "***************************\n",
      "Episode 3148 finished (timesteps: 7005353/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 7.75\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3149 finished (timesteps: 7009313/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 7.76\n",
      "***************************\n",
      "Episode 3150 finished (timesteps: 7012783/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 7.77\n",
      "***************************\n",
      "Episode 3151 finished (timesteps: 7016508/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 7.73\n",
      "***************************\n",
      "Episode 3152 finished (timesteps: 7019252/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 7.81\n",
      "***************************\n",
      "Episode 3153 finished (timesteps: 7021416/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 7.87\n",
      "***************************\n",
      "Episode 3154 finished (timesteps: 7024814/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 7.74\n",
      "***************************\n",
      "Episode 3155 finished (timesteps: 7028713/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 7.75\n",
      "***************************\n",
      "Episode 3156 finished (timesteps: 7033344/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 7.81\n",
      "***************************\n",
      "Episode 3157 finished (timesteps: 7036732/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 7.87\n",
      "***************************\n",
      "Episode 3158 finished (timesteps: 7041100/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 7.88\n",
      "***************************\n",
      "Episode 3159 finished (timesteps: 7043167/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 7.95\n",
      "***************************\n",
      "Episode 3160 finished (timesteps: 7046695/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 7.92\n",
      "***************************\n",
      "Episode 3161 finished (timesteps: 7049729/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 7.91\n",
      "***************************\n",
      "Episode 3162 finished (timesteps: 7054349/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 7.94\n",
      "***************************\n",
      "Episode 3163 finished (timesteps: 7057763/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 7.94\n",
      "***************************\n",
      "Episode 3164 finished (timesteps: 7061373/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 7.95\n",
      "***************************\n",
      "Episode 3165 finished (timesteps: 7065787/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 8.01\n",
      "***************************\n",
      "Episode 3166 finished (timesteps: 7070226/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 8.12\n",
      "***************************\n",
      "Episode 3167 finished (timesteps: 7074393/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.16\n",
      "***************************\n",
      "Episode 3168 finished (timesteps: 7077707/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 8.34\n",
      "***************************\n",
      "Episode 3169 finished (timesteps: 7080858/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.44\n",
      "***************************\n",
      "Episode 3170 finished (timesteps: 7085737/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 8.44\n",
      "***************************\n",
      "Episode 3171 finished (timesteps: 7090572/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 8.51\n",
      "***************************\n",
      "Episode 3172 finished (timesteps: 7095425/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 8.46\n",
      "***************************\n",
      "Episode 3173 finished (timesteps: 7098027/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 8.46\n",
      "***************************\n",
      "Episode 3174 finished (timesteps: 7101173/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 8.39\n",
      "***************************\n",
      "Episode 3175 finished (timesteps: 7104488/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 8.36\n",
      "***************************\n",
      "Episode 3176 finished (timesteps: 7108139/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 8.23\n",
      "***************************\n",
      "Episode 3177 finished (timesteps: 7111289/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 8.23\n",
      "***************************\n",
      "Episode 3178 finished (timesteps: 7115284/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 8.17\n",
      "***************************\n",
      "Episode 3179 finished (timesteps: 7118123/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.30\n",
      "***************************\n",
      "Episode 3180 finished (timesteps: 7122594/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 8.37\n",
      "***************************\n",
      "Episode 3181 finished (timesteps: 7124767/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 8.38\n",
      "***************************\n",
      "Episode 3182 finished (timesteps: 7128572/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 8.39\n",
      "***************************\n",
      "Episode 3183 finished (timesteps: 7131405/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 8.36\n",
      "***************************\n",
      "Episode 3184 finished (timesteps: 7134699/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 8.28\n",
      "***************************\n",
      "Episode 3185 finished (timesteps: 7138306/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 8.22\n",
      "***************************\n",
      "Episode 3186 finished (timesteps: 7142530/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 8.08\n",
      "***************************\n",
      "Episode 3187 finished (timesteps: 7145408/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.18\n",
      "***************************\n",
      "Episode 3188 finished (timesteps: 7149139/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 8.10\n",
      "***************************\n",
      "Episode 3189 finished (timesteps: 7151392/10000000)\n",
      "Epsilon: 0.01, Episode reward: 19.0, Mean reward: 8.18\n",
      "***************************\n",
      "Episode 3190 finished (timesteps: 7155128/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 8.19\n",
      "***************************\n",
      "Episode 3191 finished (timesteps: 7158977/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 8.23\n",
      "***************************\n",
      "Episode 3192 finished (timesteps: 7162632/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.24\n",
      "***************************\n",
      "Episode 3193 finished (timesteps: 7165467/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 8.26\n",
      "***************************\n",
      "Episode 3194 finished (timesteps: 7168883/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 8.20\n",
      "***************************\n",
      "Episode 3195 finished (timesteps: 7172773/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 8.36\n",
      "***************************\n",
      "Episode 3196 finished (timesteps: 7176215/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 8.64\n",
      "***************************\n",
      "Episode 3197 finished (timesteps: 7180409/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 8.72\n",
      "***************************\n",
      "Episode 3198 finished (timesteps: 7184679/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 8.73\n",
      "***************************\n",
      "Episode 3199 finished (timesteps: 7189247/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 8.71\n",
      "***************************\n",
      "Episode 3200 finished (timesteps: 7194868/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 8.68\n",
      "***************************\n",
      "Episode 3201 finished (timesteps: 7198792/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 8.64\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-7200000.mp4.\n",
      "Moviepy - Writing video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-7200000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-7200000.mp4\n",
      "Episode 3202 finished (timesteps: 7202462/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 8.69\n",
      "***************************\n",
      "Episode 3203 finished (timesteps: 7205239/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.74\n",
      "***************************\n",
      "Episode 3204 finished (timesteps: 7208338/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 8.72\n",
      "***************************\n",
      "Episode 3205 finished (timesteps: 7211947/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 8.61\n",
      "***************************\n",
      "Episode 3206 finished (timesteps: 7215287/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 8.40\n",
      "***************************\n",
      "Episode 3207 finished (timesteps: 7219521/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 8.36\n",
      "***************************\n",
      "Episode 3208 finished (timesteps: 7223380/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 8.32\n",
      "***************************\n",
      "Episode 3209 finished (timesteps: 7226873/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 8.27\n",
      "***************************\n",
      "Episode 3210 finished (timesteps: 7229744/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 8.30\n",
      "***************************\n",
      "Episode 3211 finished (timesteps: 7232841/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 8.28\n",
      "***************************\n",
      "Episode 3212 finished (timesteps: 7236391/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 8.37\n",
      "***************************\n",
      "Episode 3213 finished (timesteps: 7239585/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 8.33\n",
      "***************************\n",
      "Episode 3214 finished (timesteps: 7242135/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 8.49\n",
      "***************************\n",
      "Episode 3215 finished (timesteps: 7245314/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 8.55\n",
      "***************************\n",
      "Episode 3216 finished (timesteps: 7249554/10000000)\n",
      "Epsilon: 0.01, Episode reward: -9.0, Mean reward: 8.37\n",
      "***************************\n",
      "Episode 3217 finished (timesteps: 7253975/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 8.19\n",
      "***************************\n",
      "Episode 3218 finished (timesteps: 7256492/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.24\n",
      "***************************\n",
      "Episode 3219 finished (timesteps: 7260034/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 8.15\n",
      "***************************\n",
      "Episode 3220 finished (timesteps: 7263547/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 8.04\n",
      "***************************\n",
      "Episode 3221 finished (timesteps: 7267327/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 7.99\n",
      "***************************\n",
      "Episode 3222 finished (timesteps: 7270730/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 7.98\n",
      "***************************\n",
      "Episode 3223 finished (timesteps: 7272893/10000000)\n",
      "Epsilon: 0.01, Episode reward: 19.0, Mean reward: 8.03\n",
      "***************************\n",
      "Episode 3224 finished (timesteps: 7277084/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 8.00\n",
      "***************************\n",
      "Episode 3225 finished (timesteps: 7280762/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 8.02\n",
      "***************************\n",
      "Episode 3226 finished (timesteps: 7283794/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 8.11\n",
      "***************************\n",
      "Episode 3227 finished (timesteps: 7286117/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 8.24\n",
      "***************************\n",
      "Episode 3228 finished (timesteps: 7290339/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 8.18\n",
      "***************************\n",
      "Episode 3229 finished (timesteps: 7295596/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: 7.97\n",
      "***************************\n",
      "Episode 3230 finished (timesteps: 7299132/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 7.87\n",
      "***************************\n",
      "Episode 3231 finished (timesteps: 7302067/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: 7.64\n",
      "***************************\n",
      "Episode 3232 finished (timesteps: 7305917/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 7.79\n",
      "***************************\n",
      "Episode 3233 finished (timesteps: 7309514/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 8.11\n",
      "***************************\n",
      "Episode 3234 finished (timesteps: 7313347/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.43\n",
      "***************************\n",
      "Episode 3235 finished (timesteps: 7316489/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 8.57\n",
      "***************************\n",
      "Episode 3236 finished (timesteps: 7319019/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 8.65\n",
      "***************************\n",
      "Episode 3237 finished (timesteps: 7321813/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 8.67\n",
      "***************************\n",
      "Episode 3238 finished (timesteps: 7324478/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 8.67\n",
      "***************************\n",
      "Episode 3239 finished (timesteps: 7328477/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 8.59\n",
      "***************************\n",
      "Episode 3240 finished (timesteps: 7330966/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 8.80\n",
      "***************************\n",
      "Episode 3241 finished (timesteps: 7335740/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 8.71\n",
      "***************************\n",
      "Episode 3242 finished (timesteps: 7338935/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 8.91\n",
      "***************************\n",
      "Episode 3243 finished (timesteps: 7342412/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: 8.75\n",
      "***************************\n",
      "Episode 3244 finished (timesteps: 7345235/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 8.77\n",
      "***************************\n",
      "Episode 3245 finished (timesteps: 7348658/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 8.55\n",
      "***************************\n",
      "Episode 3246 finished (timesteps: 7351787/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 8.49\n",
      "***************************\n",
      "Episode 3247 finished (timesteps: 7355270/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 8.35\n",
      "***************************\n",
      "Episode 3248 finished (timesteps: 7359124/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 8.19\n",
      "***************************\n",
      "Episode 3249 finished (timesteps: 7363431/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 8.26\n",
      "***************************\n",
      "Episode 3250 finished (timesteps: 7367716/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 8.17\n",
      "***************************\n",
      "Episode 3251 finished (timesteps: 7371081/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.20\n",
      "***************************\n",
      "Episode 3252 finished (timesteps: 7375431/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 8.10\n",
      "***************************\n",
      "Episode 3253 finished (timesteps: 7378731/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 8.03\n",
      "***************************\n",
      "Episode 3254 finished (timesteps: 7382408/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 8.05\n",
      "***************************\n",
      "Episode 3255 finished (timesteps: 7385872/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: 7.80\n",
      "***************************\n",
      "Episode 3256 finished (timesteps: 7390172/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 7.82\n",
      "***************************\n",
      "Episode 3257 finished (timesteps: 7393837/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 7.83\n",
      "***************************\n",
      "Episode 3258 finished (timesteps: 7398966/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 7.71\n",
      "***************************\n",
      "Episode 3259 finished (timesteps: 7404153/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 7.57\n",
      "***************************\n",
      "Episode 3260 finished (timesteps: 7408299/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 7.57\n",
      "***************************\n",
      "Episode 3261 finished (timesteps: 7412220/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: 7.39\n",
      "***************************\n",
      "Episode 3262 finished (timesteps: 7415838/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 7.52\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3263 finished (timesteps: 7418835/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 7.56\n",
      "***************************\n",
      "Episode 3264 finished (timesteps: 7423018/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 7.52\n",
      "***************************\n",
      "Episode 3265 finished (timesteps: 7428092/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 7.35\n",
      "***************************\n",
      "Episode 3266 finished (timesteps: 7432763/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 7.34\n",
      "***************************\n",
      "Episode 3267 finished (timesteps: 7438410/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 7.26\n",
      "***************************\n",
      "Episode 3268 finished (timesteps: 7441501/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 7.26\n",
      "***************************\n",
      "Episode 3269 finished (timesteps: 7446554/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 7.17\n",
      "***************************\n",
      "Episode 3270 finished (timesteps: 7450287/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 7.28\n",
      "***************************\n",
      "Episode 3271 finished (timesteps: 7452301/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: 7.01\n",
      "***************************\n",
      "Episode 3272 finished (timesteps: 7453743/10000000)\n",
      "Epsilon: 0.01, Episode reward: -19.0, Mean reward: 6.74\n",
      "***************************\n",
      "Episode 3273 finished (timesteps: 7458475/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 6.65\n",
      "***************************\n",
      "Episode 3274 finished (timesteps: 7462339/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 6.67\n",
      "***************************\n",
      "Episode 3275 finished (timesteps: 7465648/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 6.68\n",
      "***************************\n",
      "Episode 3276 finished (timesteps: 7468270/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 6.80\n",
      "***************************\n",
      "Episode 3277 finished (timesteps: 7471580/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 6.76\n",
      "***************************\n",
      "Episode 3278 finished (timesteps: 7474341/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 6.80\n",
      "***************************\n",
      "Episode 3279 finished (timesteps: 7477400/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 6.76\n",
      "***************************\n",
      "Episode 3280 finished (timesteps: 7480580/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 6.88\n",
      "***************************\n",
      "Episode 3281 finished (timesteps: 7484226/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 6.84\n",
      "***************************\n",
      "Episode 3282 finished (timesteps: 7488217/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 6.79\n",
      "***************************\n",
      "Episode 3283 finished (timesteps: 7492127/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 6.76\n",
      "***************************\n",
      "Episode 3284 finished (timesteps: 7496622/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 6.89\n",
      "***************************\n",
      "Episode 3285 finished (timesteps: 7500218/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 6.95\n",
      "***************************\n",
      "Episode 3286 finished (timesteps: 7503343/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 7.04\n",
      "***************************\n",
      "Episode 3287 finished (timesteps: 7507336/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 6.99\n",
      "***************************\n",
      "Episode 3288 finished (timesteps: 7509966/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 7.02\n",
      "***************************\n",
      "Episode 3289 finished (timesteps: 7512908/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 6.95\n",
      "***************************\n",
      "Episode 3290 finished (timesteps: 7517006/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 6.87\n",
      "***************************\n",
      "Episode 3291 finished (timesteps: 7520871/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 6.78\n",
      "***************************\n",
      "Episode 3292 finished (timesteps: 7524474/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 6.77\n",
      "***************************\n",
      "Episode 3293 finished (timesteps: 7527620/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 6.78\n",
      "***************************\n",
      "Episode 3294 finished (timesteps: 7530252/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 6.88\n",
      "***************************\n",
      "Episode 3295 finished (timesteps: 7535266/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 6.88\n",
      "***************************\n",
      "Episode 3296 finished (timesteps: 7539079/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 6.80\n",
      "***************************\n",
      "Episode 3297 finished (timesteps: 7543003/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 6.81\n",
      "***************************\n",
      "Episode 3298 finished (timesteps: 7547325/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 6.86\n",
      "***************************\n",
      "Episode 3299 finished (timesteps: 7552466/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 6.86\n",
      "***************************\n",
      "Episode 3300 finished (timesteps: 7555589/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 7.01\n",
      "***************************\n",
      "Episode 3301 finished (timesteps: 7558848/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 7.02\n",
      "***************************\n",
      "Episode 3302 finished (timesteps: 7562143/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: 6.81\n",
      "***************************\n",
      "Episode 3303 finished (timesteps: 7564555/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 6.86\n",
      "***************************\n",
      "Episode 3304 finished (timesteps: 7567488/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 6.91\n",
      "***************************\n",
      "Episode 3305 finished (timesteps: 7572497/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 6.99\n",
      "***************************\n",
      "Episode 3306 finished (timesteps: 7575973/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 7.14\n",
      "***************************\n",
      "Episode 3307 finished (timesteps: 7578799/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 7.20\n",
      "***************************\n",
      "Episode 3308 finished (timesteps: 7582798/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 7.18\n",
      "***************************\n",
      "Episode 3309 finished (timesteps: 7586622/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 7.19\n",
      "***************************\n",
      "Episode 3310 finished (timesteps: 7590447/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 7.20\n",
      "***************************\n",
      "Episode 3311 finished (timesteps: 7594249/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 7.25\n",
      "***************************\n",
      "Episode 3312 finished (timesteps: 7597115/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 7.31\n",
      "***************************\n",
      "Episode 3313 finished (timesteps: 7600692/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 7.38\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-7600000.mp4.\n",
      "Moviepy - Writing video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-7600000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-7600000.mp4\n",
      "Episode 3314 finished (timesteps: 7604474/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 7.33\n",
      "***************************\n",
      "Episode 3315 finished (timesteps: 7608560/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 7.23\n",
      "***************************\n",
      "Episode 3316 finished (timesteps: 7612029/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 7.44\n",
      "***************************\n",
      "Episode 3317 finished (timesteps: 7616282/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 7.56\n",
      "***************************\n",
      "Episode 3318 finished (timesteps: 7620967/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 7.50\n",
      "***************************\n",
      "Episode 3319 finished (timesteps: 7624586/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 7.59\n",
      "***************************\n",
      "Episode 3320 finished (timesteps: 7627951/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 7.66\n",
      "***************************\n",
      "Episode 3321 finished (timesteps: 7631133/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: 7.53\n",
      "***************************\n",
      "Episode 3322 finished (timesteps: 7636149/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: 7.43\n",
      "***************************\n",
      "Episode 3323 finished (timesteps: 7639662/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 7.37\n",
      "***************************\n",
      "Episode 3324 finished (timesteps: 7644236/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 7.33\n",
      "***************************\n",
      "Episode 3325 finished (timesteps: 7646852/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 7.40\n",
      "***************************\n",
      "Episode 3326 finished (timesteps: 7651063/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 7.33\n",
      "***************************\n",
      "Episode 3327 finished (timesteps: 7655144/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 7.19\n",
      "***************************\n",
      "Episode 3328 finished (timesteps: 7658180/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 7.27\n",
      "***************************\n",
      "Episode 3329 finished (timesteps: 7661582/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 7.31\n",
      "***************************\n",
      "Episode 3330 finished (timesteps: 7664725/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 7.48\n",
      "***************************\n",
      "Episode 3331 finished (timesteps: 7669000/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 7.67\n",
      "***************************\n",
      "Episode 3332 finished (timesteps: 7674451/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 7.67\n",
      "***************************\n",
      "Episode 3333 finished (timesteps: 7679691/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 7.60\n",
      "***************************\n",
      "Episode 3334 finished (timesteps: 7684429/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 7.53\n",
      "***************************\n",
      "Episode 3335 finished (timesteps: 7687382/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 7.60\n",
      "***************************\n",
      "Episode 3336 finished (timesteps: 7691304/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 7.52\n",
      "***************************\n",
      "Episode 3337 finished (timesteps: 7694778/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 7.41\n",
      "***************************\n",
      "Episode 3338 finished (timesteps: 7698225/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 7.33\n",
      "***************************\n",
      "Episode 3339 finished (timesteps: 7702180/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 7.21\n",
      "***************************\n",
      "Episode 3340 finished (timesteps: 7704975/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 7.17\n",
      "***************************\n",
      "Episode 3341 finished (timesteps: 7707904/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 7.26\n",
      "***************************\n",
      "Episode 3342 finished (timesteps: 7710841/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 7.24\n",
      "***************************\n",
      "Episode 3343 finished (timesteps: 7713538/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 7.46\n",
      "***************************\n",
      "Episode 3344 finished (timesteps: 7718186/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 7.44\n",
      "***************************\n",
      "Episode 3345 finished (timesteps: 7722250/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 7.61\n",
      "***************************\n",
      "Episode 3346 finished (timesteps: 7725736/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 7.56\n",
      "***************************\n",
      "Episode 3347 finished (timesteps: 7729775/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 7.60\n",
      "***************************\n",
      "Episode 3348 finished (timesteps: 7733226/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 7.74\n",
      "***************************\n",
      "Episode 3349 finished (timesteps: 7736752/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 7.71\n",
      "***************************\n",
      "Episode 3350 finished (timesteps: 7739466/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 7.83\n",
      "***************************\n",
      "Episode 3351 finished (timesteps: 7743700/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 7.81\n",
      "***************************\n",
      "Episode 3352 finished (timesteps: 7746960/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 7.90\n",
      "***************************\n",
      "Episode 3353 finished (timesteps: 7749952/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 7.95\n",
      "***************************\n",
      "Episode 3354 finished (timesteps: 7752390/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 8.04\n",
      "***************************\n",
      "Episode 3355 finished (timesteps: 7756221/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 8.13\n",
      "***************************\n",
      "Episode 3356 finished (timesteps: 7759218/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 8.19\n",
      "***************************\n",
      "Episode 3357 finished (timesteps: 7762400/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 8.09\n",
      "***************************\n",
      "Episode 3358 finished (timesteps: 7766009/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.23\n",
      "***************************\n",
      "Episode 3359 finished (timesteps: 7769892/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 8.23\n",
      "***************************\n",
      "Episode 3360 finished (timesteps: 7772698/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 8.30\n",
      "***************************\n",
      "Episode 3361 finished (timesteps: 7777395/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 8.48\n",
      "***************************\n",
      "Episode 3362 finished (timesteps: 7782139/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 8.44\n",
      "***************************\n",
      "Episode 3363 finished (timesteps: 7784514/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 8.48\n",
      "***************************\n",
      "Episode 3364 finished (timesteps: 7787140/10000000)\n",
      "Epsilon: 0.01, Episode reward: 19.0, Mean reward: 8.61\n",
      "***************************\n",
      "Episode 3365 finished (timesteps: 7791266/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 8.69\n",
      "***************************\n",
      "Episode 3366 finished (timesteps: 7794790/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 8.60\n",
      "***************************\n",
      "Episode 3367 finished (timesteps: 7797750/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.68\n",
      "***************************\n",
      "Episode 3368 finished (timesteps: 7800898/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 8.65\n",
      "***************************\n",
      "Episode 3369 finished (timesteps: 7801988/10000000)\n",
      "Epsilon: 0.01, Episode reward: -19.0, Mean reward: 8.43\n",
      "***************************\n",
      "Episode 3370 finished (timesteps: 7805265/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 8.41\n",
      "***************************\n",
      "Episode 3371 finished (timesteps: 7810028/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 8.65\n",
      "***************************\n",
      "Episode 3372 finished (timesteps: 7812655/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: 8.77\n",
      "***************************\n",
      "Episode 3373 finished (timesteps: 7815074/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 8.84\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3374 finished (timesteps: 7818401/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 8.85\n",
      "***************************\n",
      "Episode 3375 finished (timesteps: 7821688/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 8.87\n",
      "***************************\n",
      "Episode 3376 finished (timesteps: 7825358/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 8.65\n",
      "***************************\n",
      "Episode 3377 finished (timesteps: 7826782/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: 8.45\n",
      "***************************\n",
      "Episode 3378 finished (timesteps: 7830102/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 8.50\n",
      "***************************\n",
      "Episode 3379 finished (timesteps: 7834205/10000000)\n",
      "Epsilon: 0.01, Episode reward: -14.0, Mean reward: 8.28\n",
      "***************************\n",
      "Episode 3380 finished (timesteps: 7838096/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 8.20\n",
      "***************************\n",
      "Episode 3381 finished (timesteps: 7844849/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 8.10\n",
      "***************************\n",
      "Episode 3382 finished (timesteps: 7848379/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 8.19\n",
      "***************************\n",
      "Episode 3383 finished (timesteps: 7853493/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 8.07\n",
      "***************************\n",
      "Episode 3384 finished (timesteps: 7856173/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 8.16\n",
      "***************************\n",
      "Episode 3385 finished (timesteps: 7860479/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.16\n",
      "***************************\n",
      "Episode 3386 finished (timesteps: 7864942/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 8.15\n",
      "***************************\n",
      "Episode 3387 finished (timesteps: 7868798/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 8.18\n",
      "***************************\n",
      "Episode 3388 finished (timesteps: 7873341/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 8.21\n",
      "***************************\n",
      "Episode 3389 finished (timesteps: 7878505/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 8.19\n",
      "***************************\n",
      "Episode 3390 finished (timesteps: 7881399/10000000)\n",
      "Epsilon: 0.01, Episode reward: 19.0, Mean reward: 8.31\n",
      "***************************\n",
      "Episode 3391 finished (timesteps: 7885777/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 8.34\n",
      "***************************\n",
      "Episode 3392 finished (timesteps: 7890431/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 8.32\n",
      "***************************\n",
      "Episode 3393 finished (timesteps: 7893605/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 8.37\n",
      "***************************\n",
      "Episode 3394 finished (timesteps: 7898017/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 8.30\n",
      "***************************\n",
      "Episode 3395 finished (timesteps: 7903459/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 8.31\n",
      "***************************\n",
      "Episode 3396 finished (timesteps: 7906819/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 8.38\n",
      "***************************\n",
      "Episode 3397 finished (timesteps: 7911462/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 8.32\n",
      "***************************\n",
      "Episode 3398 finished (timesteps: 7915193/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 8.35\n",
      "***************************\n",
      "Episode 3399 finished (timesteps: 7918578/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.44\n",
      "***************************\n",
      "Episode 3400 finished (timesteps: 7922943/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: 8.27\n",
      "***************************\n",
      "Episode 3401 finished (timesteps: 7926139/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 8.33\n",
      "***************************\n",
      "Episode 3402 finished (timesteps: 7929826/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 8.48\n",
      "***************************\n",
      "Episode 3403 finished (timesteps: 7933221/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.43\n",
      "***************************\n",
      "Episode 3404 finished (timesteps: 7936104/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 8.44\n",
      "***************************\n",
      "Episode 3405 finished (timesteps: 7939590/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 8.49\n",
      "***************************\n",
      "Episode 3406 finished (timesteps: 7942670/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 8.33\n",
      "***************************\n",
      "Episode 3407 finished (timesteps: 7945801/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.30\n",
      "***************************\n",
      "Episode 3408 finished (timesteps: 7948624/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 8.42\n",
      "***************************\n",
      "Episode 3409 finished (timesteps: 7952047/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 8.42\n",
      "***************************\n",
      "Episode 3410 finished (timesteps: 7956288/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 8.34\n",
      "***************************\n",
      "Episode 3411 finished (timesteps: 7959348/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 8.35\n",
      "***************************\n",
      "Episode 3412 finished (timesteps: 7963237/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 8.32\n",
      "***************************\n",
      "Episode 3413 finished (timesteps: 7966863/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 8.35\n",
      "***************************\n",
      "Episode 3414 finished (timesteps: 7968825/10000000)\n",
      "Epsilon: 0.01, Episode reward: 21.0, Mean reward: 8.46\n",
      "***************************\n",
      "Episode 3415 finished (timesteps: 7971221/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 8.54\n",
      "***************************\n",
      "Episode 3416 finished (timesteps: 7974578/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 8.43\n",
      "***************************\n",
      "Episode 3417 finished (timesteps: 7978160/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 8.42\n",
      "***************************\n",
      "Episode 3418 finished (timesteps: 7980864/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 8.51\n",
      "***************************\n",
      "Episode 3419 finished (timesteps: 7983649/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.49\n",
      "***************************\n",
      "Episode 3420 finished (timesteps: 7986966/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 8.42\n",
      "***************************\n",
      "Episode 3421 finished (timesteps: 7991901/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 8.56\n",
      "***************************\n",
      "Episode 3422 finished (timesteps: 7994569/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 8.74\n",
      "***************************\n",
      "Episode 3423 finished (timesteps: 7997214/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 8.63\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-8000000.mp4.\n",
      "Moviepy - Writing video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-8000000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-8000000.mp4\n",
      "Episode 3424 finished (timesteps: 8001117/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.71\n",
      "***************************\n",
      "Episode 3425 finished (timesteps: 8005234/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 8.51\n",
      "***************************\n",
      "Episode 3426 finished (timesteps: 8009428/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 8.53\n",
      "***************************\n",
      "Episode 3427 finished (timesteps: 8013085/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 8.60\n",
      "***************************\n",
      "Episode 3428 finished (timesteps: 8015573/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 8.59\n",
      "***************************\n",
      "Episode 3429 finished (timesteps: 8019495/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 8.76\n",
      "***************************\n",
      "Episode 3430 finished (timesteps: 8022839/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 8.63\n",
      "***************************\n",
      "Episode 3431 finished (timesteps: 8025346/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 8.68\n",
      "***************************\n",
      "Episode 3432 finished (timesteps: 8027986/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 8.69\n",
      "***************************\n",
      "Episode 3433 finished (timesteps: 8031532/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 8.73\n",
      "***************************\n",
      "Episode 3434 finished (timesteps: 8034610/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 8.81\n",
      "***************************\n",
      "Episode 3435 finished (timesteps: 8038023/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: 8.63\n",
      "***************************\n",
      "Episode 3436 finished (timesteps: 8042411/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 8.59\n",
      "***************************\n",
      "Episode 3437 finished (timesteps: 8046418/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 8.65\n",
      "***************************\n",
      "Episode 3438 finished (timesteps: 8049448/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 8.70\n",
      "***************************\n",
      "Episode 3439 finished (timesteps: 8052490/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 8.86\n",
      "***************************\n",
      "Episode 3440 finished (timesteps: 8056349/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 8.85\n",
      "***************************\n",
      "Episode 3441 finished (timesteps: 8059763/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 8.88\n",
      "***************************\n",
      "Episode 3442 finished (timesteps: 8063061/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 8.87\n",
      "***************************\n",
      "Episode 3443 finished (timesteps: 8065977/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 8.84\n",
      "***************************\n",
      "Episode 3444 finished (timesteps: 8068741/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 8.91\n",
      "***************************\n",
      "Episode 3445 finished (timesteps: 8071445/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 8.89\n",
      "***************************\n",
      "Episode 3446 finished (timesteps: 8075228/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 8.91\n",
      "***************************\n",
      "Episode 3447 finished (timesteps: 8078400/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 8.87\n",
      "***************************\n",
      "Episode 3448 finished (timesteps: 8082277/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 8.77\n",
      "***************************\n",
      "Episode 3449 finished (timesteps: 8085931/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 8.74\n",
      "***************************\n",
      "Episode 3450 finished (timesteps: 8089458/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 8.66\n",
      "***************************\n",
      "Episode 3451 finished (timesteps: 8092766/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 8.67\n",
      "***************************\n",
      "Episode 3452 finished (timesteps: 8096775/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 8.62\n",
      "***************************\n",
      "Episode 3453 finished (timesteps: 8099368/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.59\n",
      "***************************\n",
      "Episode 3454 finished (timesteps: 8103049/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 8.41\n",
      "***************************\n",
      "Episode 3455 finished (timesteps: 8107029/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 8.49\n",
      "***************************\n",
      "Episode 3456 finished (timesteps: 8109846/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 8.48\n",
      "***************************\n",
      "Episode 3457 finished (timesteps: 8111833/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 8.60\n",
      "***************************\n",
      "Episode 3458 finished (timesteps: 8115460/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.60\n",
      "***************************\n",
      "Episode 3459 finished (timesteps: 8118706/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 8.62\n",
      "***************************\n",
      "Episode 3460 finished (timesteps: 8120929/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 8.62\n",
      "***************************\n",
      "Episode 3461 finished (timesteps: 8123970/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 8.67\n",
      "***************************\n",
      "Episode 3462 finished (timesteps: 8127292/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: 8.49\n",
      "***************************\n",
      "Episode 3463 finished (timesteps: 8129631/10000000)\n",
      "Epsilon: 0.01, Episode reward: -12.0, Mean reward: 8.21\n",
      "***************************\n",
      "Episode 3464 finished (timesteps: 8133395/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 8.06\n",
      "***************************\n",
      "Episode 3465 finished (timesteps: 8137146/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 7.98\n",
      "***************************\n",
      "Episode 3466 finished (timesteps: 8141061/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.15\n",
      "***************************\n",
      "Episode 3467 finished (timesteps: 8144440/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 8.18\n",
      "***************************\n",
      "Episode 3468 finished (timesteps: 8147395/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.19\n",
      "***************************\n",
      "Episode 3469 finished (timesteps: 8151530/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 8.51\n",
      "***************************\n",
      "Episode 3470 finished (timesteps: 8154631/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.53\n",
      "***************************\n",
      "Episode 3471 finished (timesteps: 8159031/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 8.53\n",
      "***************************\n",
      "Episode 3472 finished (timesteps: 8162296/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 8.73\n",
      "***************************\n",
      "Episode 3473 finished (timesteps: 8164718/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 8.77\n",
      "***************************\n",
      "Episode 3474 finished (timesteps: 8169045/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 8.75\n",
      "***************************\n",
      "Episode 3475 finished (timesteps: 8171363/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 8.82\n",
      "***************************\n",
      "Episode 3476 finished (timesteps: 8173760/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 9.01\n",
      "***************************\n",
      "Episode 3477 finished (timesteps: 8177064/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 9.09\n",
      "***************************\n",
      "Episode 3478 finished (timesteps: 8179891/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 9.04\n",
      "***************************\n",
      "Episode 3479 finished (timesteps: 8183269/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 9.26\n",
      "***************************\n",
      "Episode 3480 finished (timesteps: 8186723/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 9.32\n",
      "***************************\n",
      "Episode 3481 finished (timesteps: 8190115/10000000)\n",
      "Epsilon: 0.01, Episode reward: 19.0, Mean reward: 9.48\n",
      "***************************\n",
      "Episode 3482 finished (timesteps: 8192763/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 9.47\n",
      "***************************\n",
      "Episode 3483 finished (timesteps: 8196897/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 9.53\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3484 finished (timesteps: 8199529/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 9.53\n",
      "***************************\n",
      "Episode 3485 finished (timesteps: 8201854/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 9.57\n",
      "***************************\n",
      "Episode 3486 finished (timesteps: 8206372/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 9.59\n",
      "***************************\n",
      "Episode 3487 finished (timesteps: 8209690/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 9.58\n",
      "***************************\n",
      "Episode 3488 finished (timesteps: 8211723/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 9.59\n",
      "***************************\n",
      "Episode 3489 finished (timesteps: 8216293/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 9.55\n",
      "***************************\n",
      "Episode 3490 finished (timesteps: 8219254/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 9.51\n",
      "***************************\n",
      "Episode 3491 finished (timesteps: 8221991/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 9.60\n",
      "***************************\n",
      "Episode 3492 finished (timesteps: 8225620/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 9.66\n",
      "***************************\n",
      "Episode 3493 finished (timesteps: 8229232/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 9.54\n",
      "***************************\n",
      "Episode 3494 finished (timesteps: 8230671/10000000)\n",
      "Epsilon: 0.01, Episode reward: -17.0, Mean reward: 9.27\n",
      "***************************\n",
      "Episode 3495 finished (timesteps: 8234481/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 9.31\n",
      "***************************\n",
      "Episode 3496 finished (timesteps: 8238383/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 9.29\n",
      "***************************\n",
      "Episode 3497 finished (timesteps: 8241341/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 9.35\n",
      "***************************\n",
      "Episode 3498 finished (timesteps: 8245231/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 9.26\n",
      "***************************\n",
      "Episode 3499 finished (timesteps: 8248961/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 9.23\n",
      "***************************\n",
      "Episode 3500 finished (timesteps: 8251507/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 9.38\n",
      "Saving checkpoint...\n",
      "Checkpoint saved into runs/checkpoints/CNN_DDQN_Pong-v5_11-04-2023_16:28:30\n",
      "***************************\n",
      "Episode 3501 finished (timesteps: 8255876/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 9.33\n",
      "***************************\n",
      "Episode 3502 finished (timesteps: 8258579/10000000)\n",
      "Epsilon: 0.01, Episode reward: 19.0, Mean reward: 9.49\n",
      "***************************\n",
      "Episode 3503 finished (timesteps: 8261059/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 9.54\n",
      "***************************\n",
      "Episode 3504 finished (timesteps: 8264053/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 9.55\n",
      "***************************\n",
      "Episode 3505 finished (timesteps: 8268530/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 9.55\n",
      "***************************\n",
      "Episode 3506 finished (timesteps: 8272311/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: 9.54\n",
      "***************************\n",
      "Episode 3507 finished (timesteps: 8276148/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 9.59\n",
      "***************************\n",
      "Episode 3508 finished (timesteps: 8280318/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: 9.33\n",
      "***************************\n",
      "Episode 3509 finished (timesteps: 8282643/10000000)\n",
      "Epsilon: 0.01, Episode reward: 19.0, Mean reward: 9.43\n",
      "***************************\n",
      "Episode 3510 finished (timesteps: 8287033/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 9.47\n",
      "***************************\n",
      "Episode 3511 finished (timesteps: 8290458/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 9.31\n",
      "***************************\n",
      "Episode 3512 finished (timesteps: 8295020/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 9.25\n",
      "***************************\n",
      "Episode 3513 finished (timesteps: 8299199/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 9.21\n",
      "***************************\n",
      "Episode 3514 finished (timesteps: 8302882/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 9.11\n",
      "***************************\n",
      "Episode 3515 finished (timesteps: 8306385/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 9.09\n",
      "***************************\n",
      "Episode 3516 finished (timesteps: 8310510/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 9.20\n",
      "***************************\n",
      "Episode 3517 finished (timesteps: 8313192/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 9.26\n",
      "***************************\n",
      "Episode 3518 finished (timesteps: 8316950/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 9.09\n",
      "***************************\n",
      "Episode 3519 finished (timesteps: 8320510/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 9.10\n",
      "***************************\n",
      "Episode 3520 finished (timesteps: 8323398/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 9.14\n",
      "***************************\n",
      "Episode 3521 finished (timesteps: 8326172/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 9.22\n",
      "***************************\n",
      "Episode 3522 finished (timesteps: 8329597/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 9.12\n",
      "***************************\n",
      "Episode 3523 finished (timesteps: 8333283/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 9.17\n",
      "***************************\n",
      "Episode 3524 finished (timesteps: 8335190/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: 8.89\n",
      "***************************\n",
      "Episode 3525 finished (timesteps: 8338150/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 9.12\n",
      "***************************\n",
      "Episode 3526 finished (timesteps: 8342694/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 9.03\n",
      "***************************\n",
      "Episode 3527 finished (timesteps: 8346024/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 9.10\n",
      "***************************\n",
      "Episode 3528 finished (timesteps: 8351837/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 8.95\n",
      "***************************\n",
      "Episode 3529 finished (timesteps: 8355044/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 8.95\n",
      "***************************\n",
      "Episode 3530 finished (timesteps: 8358354/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 9.06\n",
      "***************************\n",
      "Episode 3531 finished (timesteps: 8361814/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 9.07\n",
      "***************************\n",
      "Episode 3532 finished (timesteps: 8365639/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 9.06\n",
      "***************************\n",
      "Episode 3533 finished (timesteps: 8368375/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 9.11\n",
      "***************************\n",
      "Episode 3534 finished (timesteps: 8372822/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 9.06\n",
      "***************************\n",
      "Episode 3535 finished (timesteps: 8375170/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 9.27\n",
      "***************************\n",
      "Episode 3536 finished (timesteps: 8378683/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 9.34\n",
      "***************************\n",
      "Episode 3537 finished (timesteps: 8381470/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 9.39\n",
      "***************************\n",
      "Episode 3538 finished (timesteps: 8384939/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 9.40\n",
      "***************************\n",
      "Episode 3539 finished (timesteps: 8387762/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 9.40\n",
      "***************************\n",
      "Episode 3540 finished (timesteps: 8392741/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 9.35\n",
      "***************************\n",
      "Episode 3541 finished (timesteps: 8397608/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 9.34\n",
      "***************************\n",
      "Episode 3542 finished (timesteps: 8400061/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 9.36\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-8400000.mp4.\n",
      "Moviepy - Writing video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-8400000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-8400000.mp4\n",
      "Episode 3543 finished (timesteps: 8403624/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 9.39\n",
      "***************************\n",
      "Episode 3544 finished (timesteps: 8406698/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 9.40\n",
      "***************************\n",
      "Episode 3545 finished (timesteps: 8410899/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 9.46\n",
      "***************************\n",
      "Episode 3546 finished (timesteps: 8413497/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 9.55\n",
      "***************************\n",
      "Episode 3547 finished (timesteps: 8416931/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 9.57\n",
      "***************************\n",
      "Episode 3548 finished (timesteps: 8419931/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 9.65\n",
      "***************************\n",
      "Episode 3549 finished (timesteps: 8423284/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 9.75\n",
      "***************************\n",
      "Episode 3550 finished (timesteps: 8426046/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 9.81\n",
      "***************************\n",
      "Episode 3551 finished (timesteps: 8428165/10000000)\n",
      "Epsilon: 0.01, Episode reward: 21.0, Mean reward: 9.91\n",
      "***************************\n",
      "Episode 3552 finished (timesteps: 8430191/10000000)\n",
      "Epsilon: 0.01, Episode reward: 19.0, Mean reward: 10.01\n",
      "***************************\n",
      "Episode 3553 finished (timesteps: 8433197/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 10.03\n",
      "***************************\n",
      "Episode 3554 finished (timesteps: 8437024/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 10.11\n",
      "***************************\n",
      "Episode 3555 finished (timesteps: 8439980/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 10.15\n",
      "***************************\n",
      "Episode 3556 finished (timesteps: 8442255/10000000)\n",
      "Epsilon: 0.01, Episode reward: 19.0, Mean reward: 10.20\n",
      "***************************\n",
      "Episode 3557 finished (timesteps: 8445901/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 10.08\n",
      "***************************\n",
      "Episode 3558 finished (timesteps: 8448730/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 10.08\n",
      "***************************\n",
      "Episode 3559 finished (timesteps: 8451576/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 10.13\n",
      "***************************\n",
      "Episode 3560 finished (timesteps: 8454763/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: 9.89\n",
      "***************************\n",
      "Episode 3561 finished (timesteps: 8459062/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 9.81\n",
      "***************************\n",
      "Episode 3562 finished (timesteps: 8463036/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 10.01\n",
      "***************************\n",
      "Episode 3563 finished (timesteps: 8466518/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 10.23\n",
      "***************************\n",
      "Episode 3564 finished (timesteps: 8470749/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 10.27\n",
      "***************************\n",
      "Episode 3565 finished (timesteps: 8473802/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 10.47\n",
      "***************************\n",
      "Episode 3566 finished (timesteps: 8477266/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 10.45\n",
      "***************************\n",
      "Episode 3567 finished (timesteps: 8480568/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 10.45\n",
      "***************************\n",
      "Episode 3568 finished (timesteps: 8483783/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 10.43\n",
      "***************************\n",
      "Episode 3569 finished (timesteps: 8488114/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 10.42\n",
      "***************************\n",
      "Episode 3570 finished (timesteps: 8492630/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 10.38\n",
      "***************************\n",
      "Episode 3571 finished (timesteps: 8495905/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 10.48\n",
      "***************************\n",
      "Episode 3572 finished (timesteps: 8499143/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 10.44\n",
      "***************************\n",
      "Episode 3573 finished (timesteps: 8504015/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 10.41\n",
      "***************************\n",
      "Episode 3574 finished (timesteps: 8506794/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 10.48\n",
      "***************************\n",
      "Episode 3575 finished (timesteps: 8510823/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 10.34\n",
      "***************************\n",
      "Episode 3576 finished (timesteps: 8514799/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 10.28\n",
      "***************************\n",
      "Episode 3577 finished (timesteps: 8519014/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 10.43\n",
      "***************************\n",
      "Episode 3578 finished (timesteps: 8521739/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 10.46\n",
      "***************************\n",
      "Episode 3579 finished (timesteps: 8524365/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 10.52\n",
      "***************************\n",
      "Episode 3580 finished (timesteps: 8527509/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 10.48\n",
      "***************************\n",
      "Episode 3581 finished (timesteps: 8531158/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 10.27\n",
      "***************************\n",
      "Episode 3582 finished (timesteps: 8534083/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 10.22\n",
      "***************************\n",
      "Episode 3583 finished (timesteps: 8538674/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 10.27\n",
      "***************************\n",
      "Episode 3584 finished (timesteps: 8542697/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 10.16\n",
      "***************************\n",
      "Episode 3585 finished (timesteps: 8546489/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 10.14\n",
      "***************************\n",
      "Episode 3586 finished (timesteps: 8550416/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 10.15\n",
      "***************************\n",
      "Episode 3587 finished (timesteps: 8553662/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 10.22\n",
      "***************************\n",
      "Episode 3588 finished (timesteps: 8556574/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 10.19\n",
      "***************************\n",
      "Episode 3589 finished (timesteps: 8559070/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 10.30\n",
      "***************************\n",
      "Episode 3590 finished (timesteps: 8562137/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 10.29\n",
      "***************************\n",
      "Episode 3591 finished (timesteps: 8567295/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 10.14\n",
      "***************************\n",
      "Episode 3592 finished (timesteps: 8570995/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 10.09\n",
      "***************************\n",
      "Episode 3593 finished (timesteps: 8574528/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 10.17\n",
      "***************************\n",
      "Episode 3594 finished (timesteps: 8577559/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: 10.27\n",
      "***************************\n",
      "Episode 3595 finished (timesteps: 8581163/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 10.24\n",
      "***************************\n",
      "Episode 3596 finished (timesteps: 8586467/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: 10.10\n",
      "***************************\n",
      "Episode 3597 finished (timesteps: 8589839/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 10.07\n",
      "***************************\n",
      "Episode 3598 finished (timesteps: 8592941/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 10.23\n",
      "***************************\n",
      "Episode 3599 finished (timesteps: 8596613/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 10.26\n",
      "***************************\n",
      "Episode 3600 finished (timesteps: 8600080/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 10.27\n",
      "***************************\n",
      "Episode 3601 finished (timesteps: 8603168/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: 10.13\n",
      "***************************\n",
      "Episode 3602 finished (timesteps: 8606838/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 10.06\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3603 finished (timesteps: 8610611/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 10.01\n",
      "***************************\n",
      "Episode 3604 finished (timesteps: 8612802/10000000)\n",
      "Epsilon: 0.01, Episode reward: 19.0, Mean reward: 10.04\n",
      "***************************\n",
      "Episode 3605 finished (timesteps: 8615252/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 10.10\n",
      "***************************\n",
      "Episode 3606 finished (timesteps: 8617831/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 10.29\n",
      "***************************\n",
      "Episode 3607 finished (timesteps: 8622207/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 10.15\n",
      "***************************\n",
      "Episode 3608 finished (timesteps: 8627024/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 10.29\n",
      "***************************\n",
      "Episode 3609 finished (timesteps: 8630780/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 10.19\n",
      "***************************\n",
      "Episode 3610 finished (timesteps: 8632412/10000000)\n",
      "Epsilon: 0.01, Episode reward: -16.0, Mean reward: 9.95\n",
      "***************************\n",
      "Episode 3611 finished (timesteps: 8636791/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 10.00\n",
      "***************************\n",
      "Episode 3612 finished (timesteps: 8640131/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 10.09\n",
      "***************************\n",
      "Episode 3613 finished (timesteps: 8644489/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 10.00\n",
      "***************************\n",
      "Episode 3614 finished (timesteps: 8647668/10000000)\n",
      "Epsilon: 0.01, Episode reward: -8.0, Mean reward: 9.81\n",
      "***************************\n",
      "Episode 3615 finished (timesteps: 8651341/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 9.74\n",
      "***************************\n",
      "Episode 3616 finished (timesteps: 8655025/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 9.73\n",
      "***************************\n",
      "Episode 3617 finished (timesteps: 8657762/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 9.74\n",
      "***************************\n",
      "Episode 3618 finished (timesteps: 8660270/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 9.89\n",
      "***************************\n",
      "Episode 3619 finished (timesteps: 8663890/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 9.82\n",
      "***************************\n",
      "Episode 3620 finished (timesteps: 8666914/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 9.88\n",
      "***************************\n",
      "Episode 3621 finished (timesteps: 8669917/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 9.89\n",
      "***************************\n",
      "Episode 3622 finished (timesteps: 8672806/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 9.94\n",
      "***************************\n",
      "Episode 3623 finished (timesteps: 8676416/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 9.96\n",
      "***************************\n",
      "Episode 3624 finished (timesteps: 8678913/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 10.29\n",
      "***************************\n",
      "Episode 3625 finished (timesteps: 8683571/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 10.19\n",
      "***************************\n",
      "Episode 3626 finished (timesteps: 8686246/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 10.31\n",
      "***************************\n",
      "Episode 3627 finished (timesteps: 8689753/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 10.23\n",
      "***************************\n",
      "Episode 3628 finished (timesteps: 8692818/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 10.34\n",
      "***************************\n",
      "Episode 3629 finished (timesteps: 8698496/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 10.31\n",
      "***************************\n",
      "Episode 3630 finished (timesteps: 8703690/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 10.30\n",
      "***************************\n",
      "Episode 3631 finished (timesteps: 8707042/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 10.22\n",
      "***************************\n",
      "Episode 3632 finished (timesteps: 8710486/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 10.19\n",
      "***************************\n",
      "Episode 3633 finished (timesteps: 8714911/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 10.10\n",
      "***************************\n",
      "Episode 3634 finished (timesteps: 8718006/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 10.12\n",
      "***************************\n",
      "Episode 3635 finished (timesteps: 8721434/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 10.07\n",
      "***************************\n",
      "Episode 3636 finished (timesteps: 8724120/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 10.10\n",
      "***************************\n",
      "Episode 3637 finished (timesteps: 8727268/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 10.10\n",
      "***************************\n",
      "Episode 3638 finished (timesteps: 8729573/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 10.13\n",
      "***************************\n",
      "Episode 3639 finished (timesteps: 8732417/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 10.13\n",
      "***************************\n",
      "Episode 3640 finished (timesteps: 8735122/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 10.24\n",
      "***************************\n",
      "Episode 3641 finished (timesteps: 8740314/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 10.18\n",
      "***************************\n",
      "Episode 3642 finished (timesteps: 8744500/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 10.08\n",
      "***************************\n",
      "Episode 3643 finished (timesteps: 8747898/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 10.07\n",
      "***************************\n",
      "Episode 3644 finished (timesteps: 8751641/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 10.05\n",
      "***************************\n",
      "Episode 3645 finished (timesteps: 8755167/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 9.86\n",
      "***************************\n",
      "Episode 3646 finished (timesteps: 8756384/10000000)\n",
      "Epsilon: 0.01, Episode reward: -19.0, Mean reward: 9.50\n",
      "***************************\n",
      "Episode 3647 finished (timesteps: 8759571/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 9.38\n",
      "***************************\n",
      "Episode 3648 finished (timesteps: 8763143/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 9.36\n",
      "***************************\n",
      "Episode 3649 finished (timesteps: 8768186/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 9.24\n",
      "***************************\n",
      "Episode 3650 finished (timesteps: 8769227/10000000)\n",
      "Epsilon: 0.01, Episode reward: -20.0, Mean reward: 8.93\n",
      "***************************\n",
      "Episode 3651 finished (timesteps: 8771690/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 8.90\n",
      "***************************\n",
      "Episode 3652 finished (timesteps: 8775518/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 8.78\n",
      "***************************\n",
      "Episode 3653 finished (timesteps: 8779738/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 8.75\n",
      "***************************\n",
      "Episode 3654 finished (timesteps: 8783492/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 8.78\n",
      "***************************\n",
      "Episode 3655 finished (timesteps: 8787992/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 8.82\n",
      "***************************\n",
      "Episode 3656 finished (timesteps: 8790964/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 8.77\n",
      "***************************\n",
      "Episode 3657 finished (timesteps: 8794067/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 8.85\n",
      "***************************\n",
      "Episode 3658 finished (timesteps: 8797418/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.85\n",
      "***************************\n",
      "Episode 3659 finished (timesteps: 8800131/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 8.92\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-8800000.mp4.\n",
      "Moviepy - Writing video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-8800000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-8800000.mp4\n",
      "Episode 3660 finished (timesteps: 8804123/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 9.04\n",
      "***************************\n",
      "Episode 3661 finished (timesteps: 8807261/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 9.12\n",
      "***************************\n",
      "Episode 3662 finished (timesteps: 8811227/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 9.10\n",
      "***************************\n",
      "Episode 3663 finished (timesteps: 8813980/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 9.16\n",
      "***************************\n",
      "Episode 3664 finished (timesteps: 8817348/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 9.15\n",
      "***************************\n",
      "Episode 3665 finished (timesteps: 8820281/10000000)\n",
      "Epsilon: 0.01, Episode reward: 20.0, Mean reward: 9.19\n",
      "***************************\n",
      "Episode 3666 finished (timesteps: 8823352/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 9.19\n",
      "***************************\n",
      "Episode 3667 finished (timesteps: 8827428/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 9.17\n",
      "***************************\n",
      "Episode 3668 finished (timesteps: 8830736/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 9.02\n",
      "***************************\n",
      "Episode 3669 finished (timesteps: 8834794/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 8.88\n",
      "***************************\n",
      "Episode 3670 finished (timesteps: 8838897/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 8.78\n",
      "***************************\n",
      "Episode 3671 finished (timesteps: 8842572/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 8.68\n",
      "***************************\n",
      "Episode 3672 finished (timesteps: 8845242/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.71\n",
      "***************************\n",
      "Episode 3673 finished (timesteps: 8849370/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 8.68\n",
      "***************************\n",
      "Episode 3674 finished (timesteps: 8852732/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.66\n",
      "***************************\n",
      "Episode 3675 finished (timesteps: 8855304/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 8.79\n",
      "***************************\n",
      "Episode 3676 finished (timesteps: 8858806/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 8.82\n",
      "***************************\n",
      "Episode 3677 finished (timesteps: 8862915/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 8.77\n",
      "***************************\n",
      "Episode 3678 finished (timesteps: 8865348/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 8.81\n",
      "***************************\n",
      "Episode 3679 finished (timesteps: 8868375/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 8.82\n",
      "***************************\n",
      "Episode 3680 finished (timesteps: 8873062/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 8.84\n",
      "***************************\n",
      "Episode 3681 finished (timesteps: 8877207/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 8.97\n",
      "***************************\n",
      "Episode 3682 finished (timesteps: 8881412/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 8.98\n",
      "***************************\n",
      "Episode 3683 finished (timesteps: 8885454/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 8.95\n",
      "***************************\n",
      "Episode 3684 finished (timesteps: 8888155/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 9.08\n",
      "***************************\n",
      "Episode 3685 finished (timesteps: 8892111/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 9.01\n",
      "***************************\n",
      "Episode 3686 finished (timesteps: 8896822/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 8.90\n",
      "***************************\n",
      "Episode 3687 finished (timesteps: 8900346/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 8.82\n",
      "***************************\n",
      "Episode 3688 finished (timesteps: 8903780/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 8.81\n",
      "***************************\n",
      "Episode 3689 finished (timesteps: 8906544/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 8.81\n",
      "***************************\n",
      "Episode 3690 finished (timesteps: 8909947/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 8.80\n",
      "***************************\n",
      "Episode 3691 finished (timesteps: 8912259/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 8.92\n",
      "***************************\n",
      "Episode 3692 finished (timesteps: 8914725/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 8.97\n",
      "***************************\n",
      "Episode 3693 finished (timesteps: 8917967/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 9.00\n",
      "***************************\n",
      "Episode 3694 finished (timesteps: 8922108/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 9.11\n",
      "***************************\n",
      "Episode 3695 finished (timesteps: 8925337/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 9.17\n",
      "***************************\n",
      "Episode 3696 finished (timesteps: 8928046/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 9.33\n",
      "***************************\n",
      "Episode 3697 finished (timesteps: 8932481/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 9.31\n",
      "***************************\n",
      "Episode 3698 finished (timesteps: 8935469/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 9.31\n",
      "***************************\n",
      "Episode 3699 finished (timesteps: 8938595/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 9.34\n",
      "***************************\n",
      "Episode 3700 finished (timesteps: 8941052/10000000)\n",
      "Epsilon: 0.01, Episode reward: 19.0, Mean reward: 9.38\n",
      "***************************\n",
      "Episode 3701 finished (timesteps: 8946327/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 9.47\n",
      "***************************\n",
      "Episode 3702 finished (timesteps: 8950738/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 9.46\n",
      "***************************\n",
      "Episode 3703 finished (timesteps: 8953676/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 9.47\n",
      "***************************\n",
      "Episode 3704 finished (timesteps: 8958153/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: 9.27\n",
      "***************************\n",
      "Episode 3705 finished (timesteps: 8961055/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 9.23\n",
      "***************************\n",
      "Episode 3706 finished (timesteps: 8964477/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: 9.08\n",
      "***************************\n",
      "Episode 3707 finished (timesteps: 8968026/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 9.00\n",
      "***************************\n",
      "Episode 3708 finished (timesteps: 8971280/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 9.01\n",
      "***************************\n",
      "Episode 3709 finished (timesteps: 8974653/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: 8.82\n",
      "***************************\n",
      "Episode 3710 finished (timesteps: 8977924/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 9.05\n",
      "***************************\n",
      "Episode 3711 finished (timesteps: 8981601/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 9.06\n",
      "***************************\n",
      "Episode 3712 finished (timesteps: 8985602/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 9.02\n",
      "***************************\n",
      "Episode 3713 finished (timesteps: 8988935/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 9.08\n",
      "***************************\n",
      "Episode 3714 finished (timesteps: 8992524/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 9.11\n",
      "***************************\n",
      "Episode 3715 finished (timesteps: 8993525/10000000)\n",
      "Epsilon: 0.01, Episode reward: -21.0, Mean reward: 8.85\n",
      "***************************\n",
      "Episode 3716 finished (timesteps: 8997056/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 8.88\n",
      "***************************\n",
      "Episode 3717 finished (timesteps: 9001600/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 8.66\n",
      "***************************\n",
      "Episode 3718 finished (timesteps: 9004720/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.65\n",
      "***************************\n",
      "Episode 3719 finished (timesteps: 9008170/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 8.63\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3720 finished (timesteps: 9010405/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.59\n",
      "***************************\n",
      "Episode 3721 finished (timesteps: 9013680/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 8.59\n",
      "***************************\n",
      "Episode 3722 finished (timesteps: 9018070/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 8.62\n",
      "***************************\n",
      "Episode 3723 finished (timesteps: 9022752/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 8.54\n",
      "***************************\n",
      "Episode 3724 finished (timesteps: 9025861/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 8.50\n",
      "***************************\n",
      "Episode 3725 finished (timesteps: 9028162/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 8.59\n",
      "***************************\n",
      "Episode 3726 finished (timesteps: 9032310/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 8.50\n",
      "***************************\n",
      "Episode 3727 finished (timesteps: 9034751/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 8.56\n",
      "***************************\n",
      "Episode 3728 finished (timesteps: 9037864/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 8.48\n",
      "***************************\n",
      "Episode 3729 finished (timesteps: 9041000/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 8.49\n",
      "***************************\n",
      "Episode 3730 finished (timesteps: 9043815/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 8.48\n",
      "***************************\n",
      "Episode 3731 finished (timesteps: 9047189/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.53\n",
      "***************************\n",
      "Episode 3732 finished (timesteps: 9049947/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 8.60\n",
      "***************************\n",
      "Episode 3733 finished (timesteps: 9053220/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 8.57\n",
      "***************************\n",
      "Episode 3734 finished (timesteps: 9056046/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 8.54\n",
      "***************************\n",
      "Episode 3735 finished (timesteps: 9057985/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: 8.31\n",
      "***************************\n",
      "Episode 3736 finished (timesteps: 9060967/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 8.26\n",
      "***************************\n",
      "Episode 3737 finished (timesteps: 9064091/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 8.20\n",
      "***************************\n",
      "Episode 3738 finished (timesteps: 9067531/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 8.10\n",
      "***************************\n",
      "Episode 3739 finished (timesteps: 9070456/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 8.06\n",
      "***************************\n",
      "Episode 3740 finished (timesteps: 9074666/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 7.99\n",
      "***************************\n",
      "Episode 3741 finished (timesteps: 9077864/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: 7.89\n",
      "***************************\n",
      "Episode 3742 finished (timesteps: 9080715/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 8.00\n",
      "***************************\n",
      "Episode 3743 finished (timesteps: 9082989/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.01\n",
      "***************************\n",
      "Episode 3744 finished (timesteps: 9086348/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 8.02\n",
      "***************************\n",
      "Episode 3745 finished (timesteps: 9089673/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 8.13\n",
      "***************************\n",
      "Episode 3746 finished (timesteps: 9092750/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 8.37\n",
      "***************************\n",
      "Episode 3747 finished (timesteps: 9095928/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 8.56\n",
      "***************************\n",
      "Episode 3748 finished (timesteps: 9099835/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 8.48\n",
      "***************************\n",
      "Episode 3749 finished (timesteps: 9103043/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.57\n",
      "***************************\n",
      "Episode 3750 finished (timesteps: 9106521/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 8.95\n",
      "***************************\n",
      "Episode 3751 finished (timesteps: 9109618/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.89\n",
      "***************************\n",
      "Episode 3752 finished (timesteps: 9113275/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 8.87\n",
      "***************************\n",
      "Episode 3753 finished (timesteps: 9117365/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 8.82\n",
      "***************************\n",
      "Episode 3754 finished (timesteps: 9119925/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 8.86\n",
      "***************************\n",
      "Episode 3755 finished (timesteps: 9122841/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.84\n",
      "***************************\n",
      "Episode 3756 finished (timesteps: 9126512/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 8.71\n",
      "***************************\n",
      "Episode 3757 finished (timesteps: 9129676/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 8.63\n",
      "***************************\n",
      "Episode 3758 finished (timesteps: 9132196/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 8.65\n",
      "***************************\n",
      "Episode 3759 finished (timesteps: 9135376/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 8.61\n",
      "***************************\n",
      "Episode 3760 finished (timesteps: 9138978/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 8.72\n",
      "***************************\n",
      "Episode 3761 finished (timesteps: 9141117/10000000)\n",
      "Epsilon: 0.01, Episode reward: 19.0, Mean reward: 8.76\n",
      "***************************\n",
      "Episode 3762 finished (timesteps: 9144627/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 8.77\n",
      "***************************\n",
      "Episode 3763 finished (timesteps: 9148479/10000000)\n",
      "Epsilon: 0.01, Episode reward: 19.0, Mean reward: 8.80\n",
      "***************************\n",
      "Episode 3764 finished (timesteps: 9151289/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 8.84\n",
      "***************************\n",
      "Episode 3765 finished (timesteps: 9154659/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 8.80\n",
      "***************************\n",
      "Episode 3766 finished (timesteps: 9158814/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 8.77\n",
      "***************************\n",
      "Episode 3767 finished (timesteps: 9162677/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 8.66\n",
      "***************************\n",
      "Episode 3768 finished (timesteps: 9166468/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.83\n",
      "***************************\n",
      "Episode 3769 finished (timesteps: 9170360/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 8.99\n",
      "***************************\n",
      "Episode 3770 finished (timesteps: 9175277/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 9.12\n",
      "***************************\n",
      "Episode 3771 finished (timesteps: 9178032/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 9.18\n",
      "***************************\n",
      "Episode 3772 finished (timesteps: 9181506/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 9.16\n",
      "***************************\n",
      "Episode 3773 finished (timesteps: 9184557/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 9.19\n",
      "***************************\n",
      "Episode 3774 finished (timesteps: 9188479/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 9.16\n",
      "***************************\n",
      "Episode 3775 finished (timesteps: 9190982/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 9.14\n",
      "***************************\n",
      "Episode 3776 finished (timesteps: 9193412/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 9.19\n",
      "***************************\n",
      "Episode 3777 finished (timesteps: 9196870/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 9.30\n",
      "***************************\n",
      "Episode 3778 finished (timesteps: 9200117/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 9.21\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-9200000.mp4.\n",
      "Moviepy - Writing video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-9200000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-9200000.mp4\n",
      "Episode 3779 finished (timesteps: 9202838/10000000)\n",
      "Epsilon: 0.01, Episode reward: 19.0, Mean reward: 9.25\n",
      "***************************\n",
      "Episode 3780 finished (timesteps: 9206181/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 9.19\n",
      "***************************\n",
      "Episode 3781 finished (timesteps: 9209907/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 9.19\n",
      "***************************\n",
      "Episode 3782 finished (timesteps: 9212754/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 9.23\n",
      "***************************\n",
      "Episode 3783 finished (timesteps: 9216458/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: 9.10\n",
      "***************************\n",
      "Episode 3784 finished (timesteps: 9218688/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 9.08\n",
      "***************************\n",
      "Episode 3785 finished (timesteps: 9221796/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 9.10\n",
      "***************************\n",
      "Episode 3786 finished (timesteps: 9226883/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 9.17\n",
      "***************************\n",
      "Episode 3787 finished (timesteps: 9230753/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 9.19\n",
      "***************************\n",
      "Episode 3788 finished (timesteps: 9234024/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 9.20\n",
      "***************************\n",
      "Episode 3789 finished (timesteps: 9237302/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 9.17\n",
      "***************************\n",
      "Episode 3790 finished (timesteps: 9239872/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 9.21\n",
      "***************************\n",
      "Episode 3791 finished (timesteps: 9242572/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 9.20\n",
      "***************************\n",
      "Episode 3792 finished (timesteps: 9245668/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 9.15\n",
      "***************************\n",
      "Episode 3793 finished (timesteps: 9248947/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 9.02\n",
      "***************************\n",
      "Episode 3794 finished (timesteps: 9251066/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 9.15\n",
      "***************************\n",
      "Episode 3795 finished (timesteps: 9254693/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 9.11\n",
      "***************************\n",
      "Episode 3796 finished (timesteps: 9258613/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 9.06\n",
      "***************************\n",
      "Episode 3797 finished (timesteps: 9262583/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 9.14\n",
      "***************************\n",
      "Episode 3798 finished (timesteps: 9266114/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 9.09\n",
      "***************************\n",
      "Episode 3799 finished (timesteps: 9268203/10000000)\n",
      "Epsilon: 0.01, Episode reward: -10.0, Mean reward: 8.84\n",
      "***************************\n",
      "Episode 3800 finished (timesteps: 9270650/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 8.59\n",
      "***************************\n",
      "Episode 3801 finished (timesteps: 9271490/10000000)\n",
      "Epsilon: 0.01, Episode reward: -20.0, Mean reward: 8.33\n",
      "***************************\n",
      "Episode 3802 finished (timesteps: 9274162/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 8.35\n",
      "***************************\n",
      "Episode 3803 finished (timesteps: 9277249/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.34\n",
      "***************************\n",
      "Episode 3804 finished (timesteps: 9282181/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 8.46\n",
      "***************************\n",
      "Episode 3805 finished (timesteps: 9286423/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 8.36\n",
      "***************************\n",
      "Episode 3806 finished (timesteps: 9291191/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 8.46\n",
      "***************************\n",
      "Episode 3807 finished (timesteps: 9294724/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 8.64\n",
      "***************************\n",
      "Episode 3808 finished (timesteps: 9298121/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 8.74\n",
      "***************************\n",
      "Episode 3809 finished (timesteps: 9301372/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 8.89\n",
      "***************************\n",
      "Episode 3810 finished (timesteps: 9304561/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 8.96\n",
      "***************************\n",
      "Episode 3811 finished (timesteps: 9307153/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 9.04\n",
      "***************************\n",
      "Episode 3812 finished (timesteps: 9310274/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 9.05\n",
      "***************************\n",
      "Episode 3813 finished (timesteps: 9314276/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 9.05\n",
      "***************************\n",
      "Episode 3814 finished (timesteps: 9318005/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 9.22\n",
      "***************************\n",
      "Episode 3815 finished (timesteps: 9321006/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 9.57\n",
      "***************************\n",
      "Episode 3816 finished (timesteps: 9323958/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 9.57\n",
      "***************************\n",
      "Episode 3817 finished (timesteps: 9326591/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 9.76\n",
      "***************************\n",
      "Episode 3818 finished (timesteps: 9329803/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 9.75\n",
      "***************************\n",
      "Episode 3819 finished (timesteps: 9332748/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 9.84\n",
      "***************************\n",
      "Episode 3820 finished (timesteps: 9338197/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 9.80\n",
      "***************************\n",
      "Episode 3821 finished (timesteps: 9344777/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 9.71\n",
      "***************************\n",
      "Episode 3822 finished (timesteps: 9350223/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: 9.55\n",
      "***************************\n",
      "Episode 3823 finished (timesteps: 9353369/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 9.70\n",
      "***************************\n",
      "Episode 3824 finished (timesteps: 9357179/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 9.66\n",
      "***************************\n",
      "Episode 3825 finished (timesteps: 9361845/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 9.50\n",
      "***************************\n",
      "Episode 3826 finished (timesteps: 9365729/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 9.49\n",
      "***************************\n",
      "Episode 3827 finished (timesteps: 9369016/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 9.47\n",
      "***************************\n",
      "Episode 3828 finished (timesteps: 9372821/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 9.38\n",
      "***************************\n",
      "Episode 3829 finished (timesteps: 9376842/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 9.39\n",
      "***************************\n",
      "Episode 3830 finished (timesteps: 9379236/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 9.44\n",
      "***************************\n",
      "Episode 3831 finished (timesteps: 9383269/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: 9.29\n",
      "***************************\n",
      "Episode 3832 finished (timesteps: 9386197/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 9.25\n",
      "***************************\n",
      "Episode 3833 finished (timesteps: 9389294/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 9.34\n",
      "***************************\n",
      "Episode 3834 finished (timesteps: 9392285/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 9.42\n",
      "***************************\n",
      "Episode 3835 finished (timesteps: 9395009/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 9.66\n",
      "***************************\n",
      "Episode 3836 finished (timesteps: 9397662/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 9.73\n",
      "***************************\n",
      "Episode 3837 finished (timesteps: 9399958/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 9.83\n",
      "***************************\n",
      "Episode 3838 finished (timesteps: 9402856/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 9.92\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3839 finished (timesteps: 9406553/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 9.86\n",
      "***************************\n",
      "Episode 3840 finished (timesteps: 9410680/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 9.90\n",
      "***************************\n",
      "Episode 3841 finished (timesteps: 9413801/10000000)\n",
      "Epsilon: 0.01, Episode reward: -13.0, Mean reward: 9.78\n",
      "***************************\n",
      "Episode 3842 finished (timesteps: 9416225/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 9.79\n",
      "***************************\n",
      "Episode 3843 finished (timesteps: 9419847/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 9.80\n",
      "***************************\n",
      "Episode 3844 finished (timesteps: 9422491/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 9.77\n",
      "***************************\n",
      "Episode 3845 finished (timesteps: 9424890/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 9.87\n",
      "***************************\n",
      "Episode 3846 finished (timesteps: 9427842/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 9.98\n",
      "***************************\n",
      "Episode 3847 finished (timesteps: 9429995/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 10.02\n",
      "***************************\n",
      "Episode 3848 finished (timesteps: 9433883/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 10.10\n",
      "***************************\n",
      "Episode 3849 finished (timesteps: 9436728/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 10.10\n",
      "***************************\n",
      "Episode 3850 finished (timesteps: 9440116/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 10.01\n",
      "***************************\n",
      "Episode 3851 finished (timesteps: 9443439/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 10.02\n",
      "***************************\n",
      "Episode 3852 finished (timesteps: 9445598/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 10.14\n",
      "***************************\n",
      "Episode 3853 finished (timesteps: 9448002/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 10.21\n",
      "***************************\n",
      "Episode 3854 finished (timesteps: 9451051/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 10.23\n",
      "***************************\n",
      "Episode 3855 finished (timesteps: 9454377/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 10.18\n",
      "***************************\n",
      "Episode 3856 finished (timesteps: 9457704/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 10.20\n",
      "***************************\n",
      "Episode 3857 finished (timesteps: 9461238/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 10.20\n",
      "***************************\n",
      "Episode 3858 finished (timesteps: 9463836/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 10.21\n",
      "***************************\n",
      "Episode 3859 finished (timesteps: 9466358/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 10.25\n",
      "***************************\n",
      "Episode 3860 finished (timesteps: 9470095/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 10.21\n",
      "***************************\n",
      "Episode 3861 finished (timesteps: 9473031/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 10.16\n",
      "***************************\n",
      "Episode 3862 finished (timesteps: 9475896/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 10.17\n",
      "***************************\n",
      "Episode 3863 finished (timesteps: 9478332/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 10.13\n",
      "***************************\n",
      "Episode 3864 finished (timesteps: 9481327/10000000)\n",
      "Epsilon: 0.01, Episode reward: -5.0, Mean reward: 9.97\n",
      "***************************\n",
      "Episode 3865 finished (timesteps: 9484747/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 9.91\n",
      "***************************\n",
      "Episode 3866 finished (timesteps: 9487783/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 9.86\n",
      "***************************\n",
      "Episode 3867 finished (timesteps: 9490131/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 10.00\n",
      "***************************\n",
      "Episode 3868 finished (timesteps: 9492906/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 9.98\n",
      "***************************\n",
      "Episode 3869 finished (timesteps: 9497055/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 9.87\n",
      "***************************\n",
      "Episode 3870 finished (timesteps: 9499376/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 9.93\n",
      "***************************\n",
      "Episode 3871 finished (timesteps: 9502078/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 9.95\n",
      "***************************\n",
      "Episode 3872 finished (timesteps: 9505000/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 9.92\n",
      "***************************\n",
      "Episode 3873 finished (timesteps: 9508861/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 9.83\n",
      "***************************\n",
      "Episode 3874 finished (timesteps: 9511546/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 9.90\n",
      "***************************\n",
      "Episode 3875 finished (timesteps: 9513891/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 9.88\n",
      "***************************\n",
      "Episode 3876 finished (timesteps: 9515820/10000000)\n",
      "Epsilon: 0.01, Episode reward: 21.0, Mean reward: 9.94\n",
      "***************************\n",
      "Episode 3877 finished (timesteps: 9517658/10000000)\n",
      "Epsilon: 0.01, Episode reward: 19.0, Mean reward: 9.98\n",
      "***************************\n",
      "Episode 3878 finished (timesteps: 9521483/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 9.93\n",
      "***************************\n",
      "Episode 3879 finished (timesteps: 9525306/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 9.72\n",
      "***************************\n",
      "Episode 3880 finished (timesteps: 9528405/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 9.79\n",
      "***************************\n",
      "Episode 3881 finished (timesteps: 9532583/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 9.72\n",
      "***************************\n",
      "Episode 3882 finished (timesteps: 9535509/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 9.65\n",
      "***************************\n",
      "Episode 3883 finished (timesteps: 9540213/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 9.70\n",
      "***************************\n",
      "Episode 3884 finished (timesteps: 9543421/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 9.68\n",
      "***************************\n",
      "Episode 3885 finished (timesteps: 9546595/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 9.69\n",
      "***************************\n",
      "Episode 3886 finished (timesteps: 9550006/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 9.74\n",
      "***************************\n",
      "Episode 3887 finished (timesteps: 9552989/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 9.78\n",
      "***************************\n",
      "Episode 3888 finished (timesteps: 9555921/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 9.79\n",
      "***************************\n",
      "Episode 3889 finished (timesteps: 9557865/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: 9.50\n",
      "***************************\n",
      "Episode 3890 finished (timesteps: 9559712/10000000)\n",
      "Epsilon: 0.01, Episode reward: -11.0, Mean reward: 9.22\n",
      "***************************\n",
      "Episode 3891 finished (timesteps: 9562815/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 9.23\n",
      "***************************\n",
      "Episode 3892 finished (timesteps: 9565384/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 9.24\n",
      "***************************\n",
      "Episode 3893 finished (timesteps: 9568575/10000000)\n",
      "Epsilon: 0.01, Episode reward: -6.0, Mean reward: 9.15\n",
      "***************************\n",
      "Episode 3894 finished (timesteps: 9571852/10000000)\n",
      "Epsilon: 0.01, Episode reward: -3.0, Mean reward: 8.95\n",
      "***************************\n",
      "Episode 3895 finished (timesteps: 9574868/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 8.97\n",
      "***************************\n",
      "Episode 3896 finished (timesteps: 9578148/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 9.04\n",
      "***************************\n",
      "Episode 3897 finished (timesteps: 9580187/10000000)\n",
      "Epsilon: 0.01, Episode reward: 19.0, Mean reward: 9.09\n",
      "***************************\n",
      "Episode 3898 finished (timesteps: 9582291/10000000)\n",
      "Epsilon: 0.01, Episode reward: 20.0, Mean reward: 9.17\n",
      "***************************\n",
      "Episode 3899 finished (timesteps: 9585725/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 9.42\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3900 finished (timesteps: 9588744/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 9.59\n",
      "***************************\n",
      "Episode 3901 finished (timesteps: 9592002/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 9.89\n",
      "***************************\n",
      "Episode 3902 finished (timesteps: 9595285/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 9.92\n",
      "***************************\n",
      "Episode 3903 finished (timesteps: 9598586/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 9.90\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-9600000.mp4.\n",
      "Moviepy - Writing video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-9600000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-9600000.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3904 finished (timesteps: 9601157/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 9.93\n",
      "***************************\n",
      "Episode 3905 finished (timesteps: 9604037/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 10.07\n",
      "***************************\n",
      "Episode 3906 finished (timesteps: 9607305/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 9.98\n",
      "***************************\n",
      "Episode 3907 finished (timesteps: 9610445/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 9.90\n",
      "***************************\n",
      "Episode 3908 finished (timesteps: 9613245/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 9.87\n",
      "***************************\n",
      "Episode 3909 finished (timesteps: 9617126/10000000)\n",
      "Epsilon: 0.01, Episode reward: -1.0, Mean reward: 9.81\n",
      "***************************\n",
      "Episode 3910 finished (timesteps: 9621158/10000000)\n",
      "Epsilon: 0.01, Episode reward: -4.0, Mean reward: 9.63\n",
      "***************************\n",
      "Episode 3911 finished (timesteps: 9624823/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 9.59\n",
      "***************************\n",
      "Episode 3912 finished (timesteps: 9628193/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 9.61\n",
      "***************************\n",
      "Episode 3913 finished (timesteps: 9630784/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 9.61\n",
      "***************************\n",
      "Episode 3914 finished (timesteps: 9634755/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 9.53\n",
      "***************************\n",
      "Episode 3915 finished (timesteps: 9637723/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 9.52\n",
      "***************************\n",
      "Episode 3916 finished (timesteps: 9641403/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 9.47\n",
      "***************************\n",
      "Episode 3917 finished (timesteps: 9643546/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 9.51\n",
      "***************************\n",
      "Episode 3918 finished (timesteps: 9645986/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 9.54\n",
      "***************************\n",
      "Episode 3919 finished (timesteps: 9649227/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 9.52\n",
      "***************************\n",
      "Episode 3920 finished (timesteps: 9651524/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 9.61\n",
      "***************************\n",
      "Episode 3921 finished (timesteps: 9654561/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 9.72\n",
      "***************************\n",
      "Episode 3922 finished (timesteps: 9658047/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 9.87\n",
      "***************************\n",
      "Episode 3923 finished (timesteps: 9661678/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 9.81\n",
      "***************************\n",
      "Episode 3924 finished (timesteps: 9665793/10000000)\n",
      "Epsilon: 0.01, Episode reward: 4.0, Mean reward: 9.76\n",
      "***************************\n",
      "Episode 3925 finished (timesteps: 9669018/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 9.90\n",
      "***************************\n",
      "Episode 3926 finished (timesteps: 9671841/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 9.99\n",
      "***************************\n",
      "Episode 3927 finished (timesteps: 9674715/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 10.04\n",
      "***************************\n",
      "Episode 3928 finished (timesteps: 9678258/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 10.20\n",
      "***************************\n",
      "Episode 3929 finished (timesteps: 9681292/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 10.22\n",
      "***************************\n",
      "Episode 3930 finished (timesteps: 9684863/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 10.16\n",
      "***************************\n",
      "Episode 3931 finished (timesteps: 9689170/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 10.27\n",
      "***************************\n",
      "Episode 3932 finished (timesteps: 9691745/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 10.34\n",
      "***************************\n",
      "Episode 3933 finished (timesteps: 9694787/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 10.32\n",
      "***************************\n",
      "Episode 3934 finished (timesteps: 9697617/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 10.32\n",
      "***************************\n",
      "Episode 3935 finished (timesteps: 9701107/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 10.30\n",
      "***************************\n",
      "Episode 3936 finished (timesteps: 9704183/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 10.29\n",
      "***************************\n",
      "Episode 3937 finished (timesteps: 9707779/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 10.27\n",
      "***************************\n",
      "Episode 3938 finished (timesteps: 9710594/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 10.29\n",
      "***************************\n",
      "Episode 3939 finished (timesteps: 9714665/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 10.36\n",
      "***************************\n",
      "Episode 3940 finished (timesteps: 9719419/10000000)\n",
      "Epsilon: 0.01, Episode reward: -2.0, Mean reward: 10.21\n",
      "***************************\n",
      "Episode 3941 finished (timesteps: 9722934/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 10.48\n",
      "***************************\n",
      "Episode 3942 finished (timesteps: 9726903/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 10.38\n",
      "***************************\n",
      "Episode 3943 finished (timesteps: 9729967/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 10.40\n",
      "***************************\n",
      "Episode 3944 finished (timesteps: 9736078/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 10.32\n",
      "***************************\n",
      "Episode 3945 finished (timesteps: 9739321/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 10.28\n",
      "***************************\n",
      "Episode 3946 finished (timesteps: 9744438/10000000)\n",
      "Epsilon: 0.01, Episode reward: 8.0, Mean reward: 10.20\n",
      "***************************\n",
      "Episode 3947 finished (timesteps: 9748568/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 10.13\n",
      "***************************\n",
      "Episode 3948 finished (timesteps: 9751370/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 10.18\n",
      "***************************\n",
      "Episode 3949 finished (timesteps: 9754791/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 10.22\n",
      "***************************\n",
      "Episode 3950 finished (timesteps: 9758301/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 10.25\n",
      "***************************\n",
      "Episode 3951 finished (timesteps: 9761091/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 10.24\n",
      "***************************\n",
      "Episode 3952 finished (timesteps: 9764647/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 10.25\n",
      "***************************\n",
      "Episode 3953 finished (timesteps: 9768436/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 10.28\n",
      "***************************\n",
      "Episode 3954 finished (timesteps: 9771924/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 10.30\n",
      "***************************\n",
      "Episode 3955 finished (timesteps: 9775920/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 10.29\n",
      "***************************\n",
      "Episode 3956 finished (timesteps: 9779479/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 10.35\n",
      "***************************\n",
      "Episode 3957 finished (timesteps: 9783413/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 10.39\n",
      "***************************\n",
      "Episode 3958 finished (timesteps: 9787376/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 10.36\n",
      "***************************\n",
      "Episode 3959 finished (timesteps: 9790902/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 10.30\n",
      "***************************\n",
      "Episode 3960 finished (timesteps: 9794395/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 10.32\n",
      "***************************\n",
      "Episode 3961 finished (timesteps: 9796698/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 10.32\n",
      "***************************\n",
      "Episode 3962 finished (timesteps: 9799274/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 10.36\n",
      "***************************\n",
      "Episode 3963 finished (timesteps: 9801775/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 10.39\n",
      "***************************\n",
      "Episode 3964 finished (timesteps: 9804933/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 10.58\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3965 finished (timesteps: 9809712/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 10.58\n",
      "***************************\n",
      "Episode 3966 finished (timesteps: 9813075/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 10.70\n",
      "***************************\n",
      "Episode 3967 finished (timesteps: 9815519/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 10.70\n",
      "***************************\n",
      "Episode 3968 finished (timesteps: 9819586/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 10.70\n",
      "***************************\n",
      "Episode 3969 finished (timesteps: 9822907/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 10.83\n",
      "***************************\n",
      "Episode 3970 finished (timesteps: 9828529/10000000)\n",
      "Epsilon: 0.01, Episode reward: 3.0, Mean reward: 10.69\n",
      "***************************\n",
      "Episode 3971 finished (timesteps: 9832328/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 10.67\n",
      "***************************\n",
      "Episode 3972 finished (timesteps: 9834573/10000000)\n",
      "Epsilon: 0.01, Episode reward: 19.0, Mean reward: 10.79\n",
      "***************************\n",
      "Episode 3973 finished (timesteps: 9838404/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 10.81\n",
      "***************************\n",
      "Episode 3974 finished (timesteps: 9840922/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 10.82\n",
      "***************************\n",
      "Episode 3975 finished (timesteps: 9844894/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 10.82\n",
      "***************************\n",
      "Episode 3976 finished (timesteps: 9847682/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 10.76\n",
      "***************************\n",
      "Episode 3977 finished (timesteps: 9851109/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 10.66\n",
      "***************************\n",
      "Episode 3978 finished (timesteps: 9857207/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 10.74\n",
      "***************************\n",
      "Episode 3979 finished (timesteps: 9861280/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 10.91\n",
      "***************************\n",
      "Episode 3980 finished (timesteps: 9864978/10000000)\n",
      "Epsilon: 0.01, Episode reward: 10.0, Mean reward: 10.89\n",
      "***************************\n",
      "Episode 3981 finished (timesteps: 9868864/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 10.90\n",
      "***************************\n",
      "Episode 3982 finished (timesteps: 9871192/10000000)\n",
      "Epsilon: 0.01, Episode reward: 18.0, Mean reward: 11.01\n",
      "***************************\n",
      "Episode 3983 finished (timesteps: 9875602/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 11.10\n",
      "***************************\n",
      "Episode 3984 finished (timesteps: 9879625/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 11.08\n",
      "***************************\n",
      "Episode 3985 finished (timesteps: 9883296/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 11.09\n",
      "***************************\n",
      "Episode 3986 finished (timesteps: 9888391/10000000)\n",
      "Epsilon: 0.01, Episode reward: 7.0, Mean reward: 11.02\n",
      "***************************\n",
      "Episode 3987 finished (timesteps: 9893504/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 10.99\n",
      "***************************\n",
      "Episode 3988 finished (timesteps: 9897916/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 11.02\n",
      "***************************\n",
      "Episode 3989 finished (timesteps: 9901527/10000000)\n",
      "Epsilon: 0.01, Episode reward: 5.0, Mean reward: 11.22\n",
      "***************************\n",
      "Episode 3990 finished (timesteps: 9905544/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 11.46\n",
      "***************************\n",
      "Episode 3991 finished (timesteps: 9908611/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 11.43\n",
      "***************************\n",
      "Episode 3992 finished (timesteps: 9912277/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 11.41\n",
      "***************************\n",
      "Episode 3993 finished (timesteps: 9916210/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 11.49\n",
      "***************************\n",
      "Episode 3994 finished (timesteps: 9918561/10000000)\n",
      "Epsilon: 0.01, Episode reward: 19.0, Mean reward: 11.71\n",
      "***************************\n",
      "Episode 3995 finished (timesteps: 9921458/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 11.76\n",
      "***************************\n",
      "Episode 3996 finished (timesteps: 9924366/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 11.78\n",
      "***************************\n",
      "Episode 3997 finished (timesteps: 9927251/10000000)\n",
      "Epsilon: 0.01, Episode reward: -7.0, Mean reward: 11.52\n",
      "***************************\n",
      "Episode 3998 finished (timesteps: 9930644/10000000)\n",
      "Epsilon: 0.01, Episode reward: 17.0, Mean reward: 11.49\n",
      "***************************\n",
      "Episode 3999 finished (timesteps: 9934249/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 11.43\n",
      "***************************\n",
      "Episode 4000 finished (timesteps: 9936384/10000000)\n",
      "Epsilon: 0.01, Episode reward: -15.0, Mean reward: 11.17\n",
      "Saving checkpoint...\n",
      "Checkpoint saved into runs/checkpoints/CNN_DDQN_Pong-v5_11-04-2023_16:28:30\n",
      "***************************\n",
      "Episode 4001 finished (timesteps: 9940599/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 11.19\n",
      "***************************\n",
      "Episode 4002 finished (timesteps: 9943385/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 11.17\n",
      "***************************\n",
      "Episode 4003 finished (timesteps: 9946052/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 11.22\n",
      "***************************\n",
      "Episode 4004 finished (timesteps: 9951058/10000000)\n",
      "Epsilon: 0.01, Episode reward: 1.0, Mean reward: 11.09\n",
      "***************************\n",
      "Episode 4005 finished (timesteps: 9953218/10000000)\n",
      "Epsilon: 0.01, Episode reward: 20.0, Mean reward: 11.12\n",
      "***************************\n",
      "Episode 4006 finished (timesteps: 9956828/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 11.27\n",
      "***************************\n",
      "Episode 4007 finished (timesteps: 9961019/10000000)\n",
      "Epsilon: 0.01, Episode reward: 16.0, Mean reward: 11.38\n",
      "***************************\n",
      "Episode 4008 finished (timesteps: 9965065/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 11.35\n",
      "***************************\n",
      "Episode 4009 finished (timesteps: 9969310/10000000)\n",
      "Epsilon: 0.01, Episode reward: 6.0, Mean reward: 11.42\n",
      "***************************\n",
      "Episode 4010 finished (timesteps: 9973046/10000000)\n",
      "Epsilon: 0.01, Episode reward: 9.0, Mean reward: 11.55\n",
      "***************************\n",
      "Episode 4011 finished (timesteps: 9975826/10000000)\n",
      "Epsilon: 0.01, Episode reward: 15.0, Mean reward: 11.62\n",
      "***************************\n",
      "Episode 4012 finished (timesteps: 9979336/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 11.61\n",
      "***************************\n",
      "Episode 4013 finished (timesteps: 9983714/10000000)\n",
      "Epsilon: 0.01, Episode reward: 2.0, Mean reward: 11.53\n",
      "***************************\n",
      "Episode 4014 finished (timesteps: 9987380/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 11.62\n",
      "***************************\n",
      "Episode 4015 finished (timesteps: 9990748/10000000)\n",
      "Epsilon: 0.01, Episode reward: 11.0, Mean reward: 11.60\n",
      "***************************\n",
      "Episode 4016 finished (timesteps: 9995331/10000000)\n",
      "Epsilon: 0.01, Episode reward: 12.0, Mean reward: 11.63\n",
      "***************************\n",
      "Episode 4017 finished (timesteps: 9999310/10000000)\n",
      "Epsilon: 0.01, Episode reward: 13.0, Mean reward: 11.59\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-10000000.mp4.\n",
      "Moviepy - Writing video runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-10000000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/CNN_DDQN_Pong-v5_11-04-2023_16:28:30/rl-video-step-10000000.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4018 finished (timesteps: 10001915/10000000)\n",
      "Epsilon: 0.01, Episode reward: 14.0, Mean reward: 11.59\n",
      "***************************\n",
      "Saving checkpoint...\n",
      "Checkpoint saved into runs/checkpoints/CNN_DDQN_Pong-v5_11-04-2023_16:28:30\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent()\n",
    "agent.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
