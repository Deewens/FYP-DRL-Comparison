{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#from keras import Sequential\n",
    "#from keras.layers import Conv2D, Dense, Dropout, MaxPooling2D, Activation, Flatten, Input\n",
    "#from keras.activations import relu\n",
    "#from keras.callbacks import TensorBoard\n",
    "#from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import GrayScaleObservation, ResizeObservation, FrameStack\n",
    "#import matplotlib.pyplot as plt\n",
    "from collections import deque, namedtuple\n",
    "from typing import NamedTuple, Type\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.0+919230b)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('ALE/Freeway-v5', render_mode='human')\n",
    "#env.reset()\n",
    "#env.render()\n",
    "#env = GrayScaleObservation(gym.make('ALE/Breakout-v5', render_mode='human'))\n",
    "#env = ResizeObservation(env, (84, 84))\n",
    "#env = FrameStack(env, 4)\n",
    "\n",
    "#observation, info = env.reset()\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess image utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the buffer of frame to give to the Neural Network\n",
    "num_frames = 4 # (Tau)\n",
    "state_buffer = deque(maxlen=num_frames)\n",
    "next_state_buffer = deque(maxlen=num_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    def __init__(self, memory_size=50000, burn_in=10000):\n",
    "        \"\"\"\n",
    "        Construct a new buffer of replay experience\n",
    "        :param memory_size: maximum size of the memory\n",
    "        :param burn_in: used to initialise the memory.\n",
    "                        At tbe beginning of training, the agent will take a max 'burn_in' number of completely random step to populate the buffer with.\n",
    "                        This allows to have sufficient values to train on. E.g.: if burn_in is 10,000, then the memory will be filled with 10,000 random steps.\n",
    "                        The burn_in cannot be lower than memory size\n",
    "        \"\"\"\n",
    "        self.memory_size = memory_size\n",
    "        self.burn_in = burn_in\n",
    "        self.Buffer = namedtuple('Buffer', ['state', 'action', 'reward', 'done', 'next_state',])\n",
    "        self.replay_memory = deque(maxlen=memory_size) # Store namedtuple Buffer\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        \"\"\"\n",
    "        Randomly selects a batch of data stored in the memory\n",
    "        :param batch_size: the number of data to select\n",
    "        :return: I am not sure\n",
    "        \"\"\"\n",
    "        # This return a random list of indexes from the buffer\n",
    "        samples = np.random.choice(len(self.replay_memory), batch_size, replace=False)\n",
    "\n",
    "        # The below line of code is really not clear at all, I don't know who said python is simpler to read but seriously, this is horrible\n",
    "        # Basically, the part '*expression' unpack the value in the iterable container (the deque)\n",
    "        # Which creates a 'list' of arguments that can be passed to a function.\n",
    "        # e.g.:\n",
    "        # assume an array of tuple [(5, 2), (6, 2), (3, 7)]\n",
    "        # If we unpack this tuple '*[(5, 2), (6, 2), (3, 7)]', this will create a list of argument to be passed to a function, so:\n",
    "        # zip(*[(5, 2), (6, 2), (3, 7)]) is similar to zip((5, 2), (6, 2), (3, 7)). We are basically removing the stuff related to the array like [].\n",
    "        batch = zip(*[self.replay_memory[i] for i in samples])\n",
    "        # zip returns an iterator of tuple where the first item in each passed iterator is paired together, and then the second items are paired together, etc...\n",
    "        # https://www.w3schools.com/python/ref_func_zip.asp\n",
    "        return batch\n",
    "\n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        \"\"\"\n",
    "        Add data to the memory buffer\n",
    "        :param state: the current observation state (as ima array)\n",
    "        :param action: the action that is taken according to the current state\n",
    "        :param reward: the reward following the taken action (number)\n",
    "        :param done: if the current step is terminated following the action\n",
    "        :param next_state: the next state that follows the current one after the taken action\n",
    "        \"\"\"\n",
    "        self.replay_memory.append(\n",
    "            self.Buffer(state, action, reward, done, next_state))\n",
    "\n",
    "    def burn_in_capacity(self):\n",
    "        \"\"\"\n",
    "        Calculate the ratio between the data in the buffer and the burn in limit\n",
    "        :return: Return the ratio from 0 to 1. value > 1 means that the burn in limit has been reached\n",
    "        \"\"\"\n",
    "        return len(self.replay_memory) / self.burn_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "        self.max_steps_per_episode = 10000\n",
    "\n",
    "        # Get trained every step (.fit())\n",
    "        self.model = self.create_q_model()\n",
    "        # The nodel we .predict() against every step\n",
    "        self.target_model = self.create_q_model()\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "        # Experience replay buffers\n",
    "        self.action_history = []\n",
    "        self.state_history = []\n",
    "        self.state_next_history = []\n",
    "        self.rewards_history = []\n",
    "        self.done_history = []\n",
    "        self.episode_reward_history = []\n",
    "        # Maximum replay length\n",
    "        # Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "        self.MAX_MEMORY_LENGTH = 100000\n",
    "\n",
    "        self.running_reward = 0\n",
    "        self.episode_count = 0\n",
    "        self.frame_count = 0\n",
    "\n",
    "        # Number of frames to take random action and observe output\n",
    "        self.EPSILON_RANDOM_FRAMES = 50000\n",
    "        # Number of frames for exploration\n",
    "        self.EPSILON_GREEDY_FRAMES = 1000000.0\n",
    "\n",
    "        self.epsilon = 1 # Current exploration rate\n",
    "        self.EPSILON_MIN = 0.1 # Minimum exploration rate\n",
    "        self.EPSILON_MAX = 1.0 # Maximum exploration rate\n",
    "        self.epsilon_decay = (self.EPSILON_MAX - self.EPSILON_MIN) # Rate of decay for the exploration rate\n",
    "\n",
    "        self.UPDATE_AFTER_ACTIONS = 4\n",
    "        # How often to update the target network\n",
    "        self.UPDATE_TARGET_NETWORK = 10000\n",
    "\n",
    "        self.discount_rate = 0.99\n",
    "\n",
    "    def create_q_model(self):\n",
    "        # Network defined by the Deepmind paper\n",
    "        inputs = Input(shape=(84, 84, 4,))\n",
    "\n",
    "        # Convolutions on the frames on the screen\n",
    "        layer1 = Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
    "        layer2 = Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "        layer3 = Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "\n",
    "        layer4 = Flatten()(layer3)\n",
    "\n",
    "        layer5 = Dense(512, activation=\"relu\")(layer4)\n",
    "        action = Dense(self.env.action_space.n, activation=\"linear\")(layer5)\n",
    "\n",
    "        return tf.keras.Model(inputs=inputs, outputs=action)\n",
    "\n",
    "    def train(self, batch_size=32):\n",
    "        self.initialise_replay_memory()\n",
    "\n",
    "        self.episode_count = 0\n",
    "\n",
    "        while True:\n",
    "            state, info = self.env.reset()\n",
    "            episode_reward = 0\n",
    "\n",
    "            for timestep in range(1, self.max_steps_per_episode):\n",
    "                self.frame_count += 1\n",
    "\n",
    "                action = self.get_action(state)\n",
    "\n",
    "                # Decay probability of taking random action to balance exploitation vs exploration\n",
    "                self.epsilon -= self.epsilon_decay / self.EPSILON_GREEDY_FRAMES\n",
    "                self.epsilon = max(self.epsilon, self.EPSILON_MIN)\n",
    "\n",
    "                # Apply the action in the environment\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                episode_reward += reward\n",
    "\n",
    "                # Save actions and states in replay buffer\n",
    "                self.action_history.append(action)\n",
    "                self.state_history.append(state)\n",
    "                self.state_next_history.append(next_state)\n",
    "                self.done_history.append(done)\n",
    "                self.rewards_history.append(reward)\n",
    "                state = next_state\n",
    "\n",
    "                # Update every fourth frame\n",
    "                if self.frame_count % self.UPDATE_AFTER_ACTIONS == 0 and len(self.done_history) > batch_size:\n",
    "                    batch = self.replay_memory.sample_batch()\n",
    "                    states_sample, actions_sample, rewards_sample, dones_sample, next_states_sample = np.array([i for i in batch])\n",
    "\n",
    "                    future_rewards = self.target_model.predict(next_states_sample)\n",
    "                    updated_q_values = rewards_sample + self.discount_rate * tf.reduce_max(future_rewards, axis=1)\n",
    "                    \n",
    "                    # if final frame set the last value to -1\n",
    "                    updated_q_values = updated_q_values * (1 - dones_sample) - dones_sample\n",
    "                    \n",
    "                    # Create a mask to only calculate loss on the updated Q Values\n",
    "                    masks = tf.one_hot(actions_sample, self.env.action_space.n)\n",
    "                    \n",
    "                    with tf.GradientTape() as tape:\n",
    "                        # Train the model on the states and updated Q-values\n",
    "                        q_values = self.model(states_sample)\n",
    "                        \n",
    "                        # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                        q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                        # Calculate loss between new Q-value and old Q-value\n",
    "                        loss = tf.keras.losses.Huber(updated_q_values, q_action)\n",
    "\n",
    "                    # Backpropagation\n",
    "                    grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "                    self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "                if frame_count % self.UPDATE_TARGET_NETWORK == 0:\n",
    "                    self.target_model.set_weights(self.model.get_weights())\n",
    "                    template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "                    print(template.format(self.rewards, self.episode_count, frame_count))\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "\n",
    "            # Update running reward to check condition for solving\n",
    "            self.episode_reward_history.append(episode_reward)\n",
    "            if len(self.episode_reward_history) > 100:\n",
    "                del self.episode_reward_history[:1]\n",
    "            self.rewards = np.mean(self.episode_reward_history)\n",
    "\n",
    "            self.episode_count += 1\n",
    "\n",
    "            if self.rewards > 40:  # Condition to consider the task solved\n",
    "                print(\"Solved at episode {}!\".format(self.episode_count))\n",
    "                break\n",
    "\n",
    "    def initialise_replay_memory(self):\n",
    "        self.env.reset()\n",
    "        state_img = self.env.render()\n",
    "        state = preprocess_observation(state_img)\n",
    "\n",
    "        # Fill the memory replay buffer\n",
    "        while self.replay_memory.burn_in_capacity() < 1:\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            state_img = self.env.render()\n",
    "            next_state = preprocess_observation(state_img)\n",
    "\n",
    "            done = terminated or truncated\n",
    "\n",
    "            self.rewards += reward\n",
    "            self.replay_memory.append(state, action, reward, done, next_state)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                self.env.reset()\n",
    "                state_img = self.env.render()\n",
    "                state = preprocess_observation(state_img)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            # Take random action\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Predict action Q-values from env state\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probabilities = self.model(state_tensor, trainable=False)\n",
    "            action = tf.argmax(action_probabilities[0]).numpy()\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (5, 32) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m agent \u001B[38;5;241m=\u001B[39m DQNAgent(env, ExperienceReplay())\n\u001B[0;32m----> 2\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[25], line 87\u001B[0m, in \u001B[0;36mDQNAgent.train\u001B[0;34m(self, batch_size)\u001B[0m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m frame_count \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mUPDATE_AFTER_ACTIONS \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     86\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreplay_memory\u001B[38;5;241m.\u001B[39msample_batch()\n\u001B[0;32m---> 87\u001B[0m     states_sample, actions_sample, rewards_sample, dones_sample, next_states_sample \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     89\u001B[0m     future_rewards \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_model\u001B[38;5;241m.\u001B[39mpredict(next_states_sample)\n\u001B[1;32m     90\u001B[0m     updated_q_values \u001B[38;5;241m=\u001B[39m rewards_sample \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdiscount_rate \u001B[38;5;241m*\u001B[39m tf\u001B[38;5;241m.\u001B[39mreduce_max(future_rewards, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (5, 32) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(env, ExperienceReplay())\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
