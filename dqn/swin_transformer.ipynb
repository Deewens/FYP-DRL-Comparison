{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-30 11:31:11.348318: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-30 11:31:15.224694: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, TFSwinModel, TFSwinForImageClassification, SwinConfig\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: cats-image/image\n",
      "Found cached dataset cats-image (/home/deewens/.cache/huggingface/datasets/huggingface___cats-image/image/1.9.0/68fbc793fb10cd165e490867f5d61fa366086ea40c73e549a020103dcb4f597e)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cebbec517ddf4d4d9e874718f2f82419"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"huggingface/cats-image\")\n",
    "image = dataset[\"test\"][\"image\"][0]\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
    "inputs = image_processor(image, return_tensors=\"tf\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "data": {
      "text/plain": "{'pixel_values': <tf.Tensor: shape=(1, 3, 224, 224), dtype=float32, numpy=\n array([[[[ 0.31381115,  0.43368444,  0.4850587 , ..., -0.35405433,\n           -0.33692956, -0.35405433],\n          [ 0.3651854 ,  0.43368444,  0.46793392, ..., -0.35405433,\n           -0.35405433, -0.38830382],\n          [ 0.31381115,  0.39943492,  0.41655967, ..., -0.45680285,\n           -0.42255333, -0.38830382],\n          ...,\n          [ 1.9064132 ,  1.7865399 ,  1.6495419 , ...,  1.6152923 ,\n            1.4954191 ,  1.4440448 ],\n          [ 1.8721637 ,  1.8036647 ,  1.7522904 , ...,  1.4097953 ,\n            1.1357993 ,  0.9816765 ],\n          [ 1.8721637 ,  1.7180408 ,  1.7351656 , ...,  0.12543888,\n           -0.16568205, -0.4739276 ]],\n \n         [[-1.6155462 , -1.6155462 , -1.6155462 , ..., -1.7906162 ,\n           -1.7906162 , -1.8081232 ],\n          [-1.5630252 , -1.5630252 , -1.5630252 , ..., -1.7731092 ,\n           -1.7556022 , -1.7731092 ],\n          [-1.6330532 , -1.5980392 , -1.5630252 , ..., -1.8081232 ,\n           -1.7906162 , -1.7906162 ],\n          ...,\n          [-0.39005598, -0.53011197, -0.635154  , ..., -0.740196  ,\n           -0.810224  , -0.862745  ],\n          [-0.39005598, -0.44257697, -0.565126  , ..., -0.84523803,\n           -1.0028011 , -1.0728291 ],\n          [-0.42507   , -0.565126  , -0.582633  , ..., -1.4929972 ,\n           -1.5980392 , -1.7205882 ]],\n \n         [[-0.79355115, -0.60182995, -0.6541175 , ..., -1.229281  ,\n           -1.1247058 , -1.1595641 ],\n          [-0.8458387 , -0.7238344 , -0.68897593, ..., -1.229281  ,\n           -1.1595641 , -1.229281  ],\n          [-0.74126357, -0.63668835, -0.60182995, ..., -1.2467101 ,\n           -1.2641394 , -1.2815686 ],\n          ...,\n          [ 1.6813945 ,  1.6639653 ,  1.3850982 , ...,  1.4373858 ,\n            1.3153814 ,  1.1759479 ],\n          [ 1.6465361 ,  1.454815  ,  1.5593902 , ...,  1.1585187 ,\n            0.9319392 ,  0.77507645],\n          [ 1.6988237 ,  1.6988237 ,  1.5593902 , ..., -0.14867094,\n           -0.67154676, -0.8981263 ]]]], dtype=float32)>}"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFSwinForImageClassification.\n",
      "\n",
      "Some layers of TFSwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized: ['swin/encoder/layers.2/blocks.2/attention/self/relative_position_index:0', 'swin/encoder/layers.2/blocks.3/attention/self/relative_position_index:0', 'swin/encoder/layers.3/blocks.0/attention/self/relative_position_index:0', 'swin/encoder/layers.1/blocks.0/attention/self/relative_position_index:0', 'swin/encoder/layers.2/blocks.1/attention/self/relative_position_index:0', 'swin/encoder/layers.3/blocks.1/attention/self/relative_position_index:0', 'swin/encoder/layers.2/blocks.4/attention/self/relative_position_index:0', 'swin/encoder/layers.2/blocks.5/attention/self/relative_position_index:0', 'swin/encoder/layers.1/blocks.1/attention/self/relative_position_index:0', 'swin/encoder/layers.0/blocks.0/attention/self/relative_position_index:0', 'swin/encoder/layers.2/blocks.0/attention/self/relative_position_index:0', 'swin/encoder/layers.0/blocks.1/attention/self/relative_position_index:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<transformers.models.swin.modeling_tf_swin.TFSwinForImageClassification at 0x7fbf31f47760>"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TFSwinForImageClassification.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
    "model.config.num_channels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: cats-image/image\n",
      "Found cached dataset cats-image (/home/deewens/.cache/huggingface/datasets/huggingface___cats-image/image/1.9.0/68fbc793fb10cd165e490867f5d61fa366086ea40c73e549a020103dcb4f597e)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad36eee7ef8e430692dd587a1d3b5c30"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "All model checkpoint layers were used when initializing TFSwinForImageClassification.\n",
      "\n",
      "Some layers of TFSwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized: ['swin/encoder/layers.2/blocks.2/attention/self/relative_position_index:0', 'swin/encoder/layers.2/blocks.3/attention/self/relative_position_index:0', 'swin/encoder/layers.3/blocks.0/attention/self/relative_position_index:0', 'swin/encoder/layers.1/blocks.0/attention/self/relative_position_index:0', 'swin/encoder/layers.2/blocks.1/attention/self/relative_position_index:0', 'swin/encoder/layers.3/blocks.1/attention/self/relative_position_index:0', 'swin/encoder/layers.2/blocks.4/attention/self/relative_position_index:0', 'swin/encoder/layers.2/blocks.5/attention/self/relative_position_index:0', 'swin/encoder/layers.1/blocks.1/attention/self/relative_position_index:0', 'swin/encoder/layers.0/blocks.0/attention/self/relative_position_index:0', 'swin/encoder/layers.2/blocks.0/attention/self/relative_position_index:0', 'swin/encoder/layers.0/blocks.1/attention/self/relative_position_index:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tabby, tabby cat\n"
     ]
    }
   ],
   "source": [
    "logits = model(**inputs).logits\n",
    "\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_label = int(tf.math.argmax(logits, axis=-1))\n",
    "print(model.config.id2label[predicted_label])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "TensorShape([1, 1000])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_swin_for_image_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " swin (TFSwinMainLayer)      multiple                  27548166  \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  769000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,317,166\n",
      "Trainable params: 28,288,354\n",
      "Non-trainable params: 28,812\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 15\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# custom_config.save_pretrained(save_directory=\"./swin_config\") # Save the config in a json file\u001B[39;00m\n\u001B[1;32m     14\u001B[0m swin_model \u001B[38;5;241m=\u001B[39m TFSwinModel(config\u001B[38;5;241m=\u001B[39mcustom_config)\n\u001B[0;32m---> 15\u001B[0m \u001B[43mswin_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msummary\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py:3229\u001B[0m, in \u001B[0;36mModel.summary\u001B[0;34m(self, line_length, positions, print_fn, expand_nested, show_trainable, layer_range)\u001B[0m\n\u001B[1;32m   3198\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Prints a string summary of the network.\u001B[39;00m\n\u001B[1;32m   3199\u001B[0m \n\u001B[1;32m   3200\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3226\u001B[0m \u001B[38;5;124;03m    ValueError: if `summary()` is called before the model is built.\u001B[39;00m\n\u001B[1;32m   3227\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3228\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuilt:\n\u001B[0;32m-> 3229\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   3230\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis model has not yet been built. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3231\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBuild the model first by calling `build()` or by calling \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3232\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe model on a batch of data.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3233\u001B[0m     )\n\u001B[1;32m   3234\u001B[0m layer_utils\u001B[38;5;241m.\u001B[39mprint_summary(\n\u001B[1;32m   3235\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   3236\u001B[0m     line_length\u001B[38;5;241m=\u001B[39mline_length,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3241\u001B[0m     layer_range\u001B[38;5;241m=\u001B[39mlayer_range,\n\u001B[1;32m   3242\u001B[0m )\n",
      "\u001B[0;31mValueError\u001B[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
     ]
    }
   ],
   "source": [
    "default_config = SwinConfig()\n",
    "custom_config = SwinConfig(\n",
    "    image_size=84,\n",
    "    patch_size=3,\n",
    "    num_channels=4,\n",
    "    embed_dim=96,\n",
    "    depths=[2, 3, 2],\n",
    "    num_heads=[3, 3, 6],\n",
    "    window_size=7,\n",
    "    mlp_ratio=4.0,\n",
    "    drop_path_rate=0.1,\n",
    ")\n",
    "# custom_config.save_pretrained(save_directory=\"./swin_config\") # Save the config in a json file\n",
    "swin_model = TFSwinModel(config=custom_config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<keras.layers.convolutional.conv2d.Conv2D object at 0x7fbf3c322760>,)\n",
      "True\n",
      "<class 'transformers.models.swin.modeling_tf_swin.TFSwinModel'>\n"
     ]
    }
   ],
   "source": [
    "input_layer = tf.keras.layers.Input(shape=(4, 84, 84))\n",
    "conv = tf.keras.layers.Conv2D(32, 8, strides=4, activation=\"relu\", data_format=\"channels_first\"),\n",
    "\n",
    "\n",
    "print(conv)\n",
    "print(issubclass(tf.keras.layers.Conv2D, tf.keras.layers.Layer))\n",
    "print(type(model))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[[ 52,  52,  52, ...,  87,  87,  87],\n         [ 87,  87,  87, ...,  87,  87,  87],\n         [ 87,  87,  87, ...,  87,  87,  87],\n         ...,\n         [236, 236, 236, ..., 236, 236, 236],\n         [236, 236, 236, ..., 236, 236, 236],\n         [236, 236, 236, ..., 236, 236, 236]],\n\n        [[ 52,  52,  52, ...,  87,  87,  87],\n         [ 87,  87,  87, ...,  87,  87,  87],\n         [ 87,  87,  87, ...,  87,  87,  87],\n         ...,\n         [236, 236, 236, ..., 236, 236, 236],\n         [236, 236, 236, ..., 236, 236, 236],\n         [236, 236, 236, ..., 236, 236, 236]],\n\n        [[ 52,  52,  52, ...,  87,  87,  87],\n         [ 87,  87,  87, ...,  87,  87,  87],\n         [ 87,  87,  87, ...,  87,  87,  87],\n         ...,\n         [236, 236, 236, ..., 236, 236, 236],\n         [236, 236, 236, ..., 236, 236, 236],\n         [236, 236, 236, ..., 236, 236, 236]],\n\n        [[ 52,  52,  52, ...,  87,  87,  87],\n         [ 87,  87,  87, ...,  87,  87,  87],\n         [ 87,  87,  87, ...,  87,  87,  87],\n         ...,\n         [236, 236, 236, ..., 236, 236, 236],\n         [236, 236, 236, ..., 236, 236, 236],\n         [236, 236, 236, ..., 236, 236, 236]]]], dtype=uint8)"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStack\n",
    "\n",
    "\n",
    "def make_env(env_name=\"ALE/Pong-v5\", seed=42):\n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\", full_action_space=False, frameskip=1)\n",
    "    env = AtariPreprocessing(env)\n",
    "    # env = RecordEpisodeStatistics(env)\n",
    "    env = FrameStack(env, 4)\n",
    "    env.observation_space.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "\n",
    "    return env\n",
    "\n",
    "env = make_env()\n",
    "state, _ = env.reset()\n",
    "state = np.array(state)\n",
    "state = np.expand_dims(state, 0)\n",
    "# state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preprocess = tf.keras.Sequential([\n",
    "    #tf.keras.layers.Input(shape=(4, 84, 84)),\n",
    "    tf.keras.layers.Rescaling(scale=1.0 / 255),\n",
    "])\n",
    "state = preprocess(state)\n",
    "state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "class SwinTransformerAtariBlock(tf.keras.Model):\n",
    "    def __init__(self, num_actions):\n",
    "        super(SwinTransformerAtariBlock, self).__init__()\n",
    "\n",
    "        # Preprocessing phase\n",
    "        self.rescaling = tf.keras.layers.Rescaling(scale=1.0 / 255)\n",
    "        self.swin = TFSwinModel(config=configuration)\n",
    "        self.action_outputs = tf.keras.layers.Dense(num_actions, name=\"action\", activation=\"linear\")\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        swin_outputs = self.swin(pixel_values=inputs, training=training)\n",
    "        pooled_output = swin_outputs[1]\n",
    "        logits = self.action_outputs(pooled_output)\n",
    "\n",
    "\n",
    "        return logits\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\narray([[-0.28238356,  0.78094923,  0.49464095,  0.3961748 , -1.7232966 ,\n        -1.5269284 ]], dtype=float32)>"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SwinTransformerAtariBlock(6)\n",
    "model(state)\n",
    "\n",
    "# inputs = tf.keras.layers.Input(shape=(4, 84, 84))\n",
    "# x = tf.keras.layers.Rescaling(scale=1.0 / 255)(inputs)\n",
    "# x = TFSwinModel(config=configuration)(x)\n",
    "#action = tf.keras.layers.Dense(6, activation=\"linear\")(x.pooler_output)\n",
    "#rl_model = tf.keras.Model(inputs=inputs, outputs=action)\n",
    "\n",
    "# model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
