{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-06T14:46:48.599692Z",
     "end_time": "2023-04-06T14:46:50.208129Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import PixelObservationWrapper, TransformObservation, FrameStack, GrayScaleObservation, ResizeObservation\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "        # Formula to calculate output of conv layer:\n",
    "        $$H_{out} = \\frac{(H_{in} + 2 * padding[0] - dilation[0] * (kernelSize[0] - 1) - 1}{stride[0]}+ 1$$\n",
    "        $$W_{out} = \\frac{(W_{in} + 2 * padding[1] - dilation[1] * (kernelSize[1] - 1) - 1}{stride[1]}+ 1$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DQNAtari(nn.Module):\n",
    "    def __init__(self, num_channels, num_actions):\n",
    "        super(DQNAtari, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_channels, 32, stride=4, kernel_size=8)\n",
    "        self.conv2 = nn.Conv2d(32, 64, stride=2, kernel_size=4)\n",
    "        self.conv3 = nn.Conv2d(64, 64, stride=1, kernel_size=3)\n",
    "        # 64 * 7 * 7 is the output of last conv layer because of the formula above, for an input of 84*84\n",
    "        self.linear = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.output = nn.Linear(512, num_actions) # Head layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float() / 255 # Rescale input from [0, 255] to [0, 1]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.linear(x.view(x.size(0), -1)))\n",
    "        return self.output(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-06T14:46:53.622742Z",
     "end_time": "2023-04-06T14:46:53.647783Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DQNCartPole(nn.Module):\n",
    "    def __init__(self, num_channels, num_actions):\n",
    "        super(DQNCartPole, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_channels, 64, stride=3, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(64, 64, stride=2, kernel_size=4)\n",
    "        self.conv3 = nn.Conv2d(64, 64, stride=1, kernel_size=3)\n",
    "        self.flatten = nn.Flatten()\n",
    "        #self.linear1 = nn.Linear(64 * 23 * 36, 512)\n",
    "        self.linear1 = nn.Linear(6400, 512) # 84x84 input shape\n",
    "        self.linear2 = nn.Linear(512, 256)\n",
    "        self.linear3 = nn.Linear(256, 64)\n",
    "        self.head = nn.Linear(64, num_actions) # Head layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / 255.0 # Rescale input from [0, 255] to [0, 1]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        return self.head(x)\n",
    "\n",
    "model = DQNCartPole(4, 2).to(device)\n",
    "print(model)\n",
    "#img = torch.rand([1, 4, 160, 240], device=device)\n",
    "img = torch.rand([1, 4, 84, 84], device=device)\n",
    "pred = model(img)\n",
    "pred"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-06T14:51:25.332124Z",
     "end_time": "2023-04-06T14:51:25.387422Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(4, 32, stride=4, kernel_size=8)\n",
    "conv2 = nn.Conv2d(32, 64, stride=2, kernel_size=4)\n",
    "conv3 = nn.Conv2d(64, 64, stride=1, kernel_size=3)\n",
    "img = torch.rand([4, 84, 84])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-06T13:33:25.957830Z",
     "end_time": "2023-04-06T13:33:25.963964Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conv21 = nn.Conv2d(4, 64, stride=3, kernel_size=5)\n",
    "conv22 = nn.Conv2d(64, 64, stride=2, kernel_size=4)\n",
    "conv23 = nn.Conv2d(64, 64, stride=1, kernel_size=3)\n",
    "img2 = torch.rand([4, 160, 240])\n",
    "output2 = conv23(conv22(conv21(img2)))\n",
    "print(output2.shape)\n",
    "\n",
    "output2.view(output2.size(0), -1).shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-06T13:33:25.962942Z",
     "end_time": "2023-04-06T13:33:26.101062Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_env(env_name=\"CartPole-v1\", seed=0, obs_resize_shape=(160, 240)):\n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "    env = PixelObservationWrapper(env)\n",
    "\n",
    "    # PixelObservationWrapper replace Box space to Dict space env, we don't want because it won't work with following wrappers, so we directly returns the Pixels value\n",
    "    env = TransformObservation(env, lambda obs: obs[\"pixels\"])\n",
    "    # Need to change the observation space to keep it true with observation after the TransformObservation wrapper\n",
    "    env.observation_space = env.observation_space['pixels']\n",
    "\n",
    "    env = GrayScaleObservation(env)\n",
    "    env = ResizeObservation(env, obs_resize_shape)\n",
    "    env = FrameStack(env, 4)\n",
    "    env.observation_space.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "\n",
    "    return env"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-06T14:51:31.839331Z",
     "end_time": "2023-04-06T14:51:31.851076Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 9))\n",
    "def plot_model(self, score, episode):\n",
    "    self.scores.append(score)\n",
    "    self.episodes.append(episode)\n",
    "    self.averages.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
    "\n",
    "    plt.plot(self.episodes, self.averages, 'r')\n",
    "    plt.plot(self.episodes, self.scores, 'b')\n",
    "    plt.ylabel('Score', fontsize=18)\n",
    "    plt.xlabel('Steps', fontsize=18)\n",
    "\n",
    "    dqn = \"DQN_\"\n",
    "    soft_update = \"\"\n",
    "    dueling = \"\"\n",
    "    greedy = \"\"\n",
    "    PER = \"\"\n",
    "\n",
    "    if self.ddqn: dqn = 'DDQN_'\n",
    "    if self.soft_update: soft_update = '_soft'\n",
    "    if self.dueling: dueling = '_Dueling'\n",
    "    if self.epsilon_greedy: greedy = '_Greedy'\n",
    "    if self.use_PER: PER = \"_PER\"\n",
    "\n",
    "    try:\n",
    "        plt.savefig(dqn + \"CartPole-v1_CNN\" + soft_update + dueling + greedy + PER + \".png\")\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    return str(self.averages[-1])[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-06T14:51:39.500614Z",
     "end_time": "2023-04-06T14:51:39.516970Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.env = make_env(obs_resize_shape=(84, 84))\n",
    "\n",
    "        self.env.spec.max_episode_steps = 4000\n",
    "        self.MAX_EPISODES = 2000\n",
    "        # self.MAX_EPISODES = 1\n",
    "\n",
    "        self.GAMMA = 0.95\n",
    "        self.TAU = 0.1\n",
    "        self.BATCH_SIZE = 32\n",
    "\n",
    "        self.epsilon = 1.0\n",
    "        self.EPSILON_START = 1.0\n",
    "        self.EPSILON_END = 0.01\n",
    "        self.EPSILON_DECAY = 0.0005\n",
    "\n",
    "        self.TRAIN_FREQUENCY = 4\n",
    "\n",
    "        self.LEARNING_RATE = 0.00025\n",
    "\n",
    "        self.memory = ReplayBuffer(\n",
    "            buffer_size=10_000,\n",
    "            observation_space=self.env.observation_space,\n",
    "            action_space=self.env.action_space,\n",
    "            device=device,\n",
    "            optimize_memory_usage=True,\n",
    "            handle_timeout_termination=False\n",
    "        )\n",
    "\n",
    "        self.scores = []\n",
    "        self.episodes = []\n",
    "        self.means = []\n",
    "\n",
    "        self.q_network = DQNCartPole(4, self.env.action_space.n).to(device)\n",
    "        self.target_network = DQNCartPole(4, self.env.action_space.n).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.LEARNING_RATE)\n",
    "\n",
    "    def act(self, state, current_step):\n",
    "        \"\"\"\n",
    "        Choose an action either random or predicted by q_network\n",
    "\n",
    "        :param state: current observation state\n",
    "        :param current_step: global step\n",
    "        :return: an action as uint Tensor (use .item() to retrieve the value)\n",
    "        \"\"\"\n",
    "        # Epsilon-Greedy strategy\n",
    "        self.epsilon = self.EPSILON_END + (self.EPSILON_START - self.EPSILON_END) * np.exp(-self.EPSILON_DECAY * current_step)\n",
    "\n",
    "        if self.epsilon > np.random.rand():\n",
    "            # Random action\n",
    "            return np.array(self.env.action_space.sample())\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor(np.array(state), device=device).unsqueeze(0)\n",
    "                q_values = self.q_network(state_tensor)\n",
    "                return q_values.argmax(dim=1)[0].cpu().numpy()\n",
    "\n",
    "    def remember(self, state, action, next_state, reward, done, infos):\n",
    "        self.memory.add(state, next_state, action, reward, done, infos)\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # Soft update of the target netwok's weights\n",
    "        target_net_state_dict = self.target_network.state_dict()\n",
    "        policy_net_state_dict = self.q_network.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key] * self.TAU + target_net_state_dict[key] * (1 - self.TAU)\n",
    "        self.target_network.load_state_dict(target_net_state_dict)\n",
    "\n",
    "    def optimize_model(self):\n",
    "        minibatch = self.memory.sample(self.BATCH_SIZE)\n",
    "\n",
    "        # Calculate Q values for current states\n",
    "        # For each q_values, get the action according to the minibatch\n",
    "        q_values = self.q_network(minibatch.observations).gather(1, minibatch.actions)\n",
    "\n",
    "        # Then, calculate the best actions for the next states, and return its indices\n",
    "        with torch.no_grad():\n",
    "            best_next_actions = self.q_network(minibatch.next_observations).argmax(1).unsqueeze(1)\n",
    "\n",
    "        # Calculate the Q values for the next states using the target network, and return the action according to the best next action returned by the q network\n",
    "        target_next_q_values = self.target_network(minibatch.next_observations).gather(1, best_next_actions)\n",
    "\n",
    "        # Calculate the target Q values using Double DQN\n",
    "        target_q_values = minibatch.rewards + (1 - minibatch.dones) * self.GAMMA * target_next_q_values\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = F.mse_loss(q_values, target_q_values)\n",
    "\n",
    "        # Optimise Q network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def plot_data(self, score, episode, show_result=False):\n",
    "        self.scores.append(score)\n",
    "        self.episodes.append(episode)\n",
    "        self.means.append(sum(self.scores[-100:]) / len(self.scores[-100:]))\n",
    "\n",
    "        plt.figure(1)\n",
    "        if show_result:\n",
    "            plt.title(\"Result\")\n",
    "        else:\n",
    "            plt.clf()\n",
    "            plt.title(\"Training...\")\n",
    "\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.plot(self.episodes, self.means, 'r')\n",
    "        plt.plot(self.episodes, self.scores, 'b')\n",
    "\n",
    "        plt.pause(0.001) # Pause a bit so that plots are updated\n",
    "        if is_ipython:\n",
    "            if not show_result:\n",
    "                print(f\"Episode: {episode}/{self.MAX_EPISODES}, score: {self.scores[-1:]}, epsilon: {self.epsilon:.2}, Mean score: {self.means[-1:][0]}\")\n",
    "                print(\"--------------------------\")\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "            else:\n",
    "                display.display(plt.gcf())\n",
    "\n",
    "    def run(self):\n",
    "        global_step = 0\n",
    "        episode_reward_history = []\n",
    "        for episode in range(self.MAX_EPISODES):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            steps_alive = 0\n",
    "            while not done:\n",
    "                global_step += 1\n",
    "                action = self.act(state, global_step)\n",
    "                next_state, reward, terminated, truncated, infos = self.env.step(action.item())\n",
    "                done = terminated or truncated\n",
    "\n",
    "                if not done or steps_alive == self.env.spec.max_episode_steps - 1:\n",
    "                    reward = reward\n",
    "                else:\n",
    "                    reward = -100\n",
    "\n",
    "                self.remember(state, action, next_state, reward, done, infos)\n",
    "\n",
    "                state = next_state\n",
    "                steps_alive += 1\n",
    "\n",
    "                if done:\n",
    "                    if episode % 4 == 0:\n",
    "                        self.update_target_model()\n",
    "\n",
    "                    self.plot_data(steps_alive, episode)\n",
    "\n",
    "                self.optimize_model()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-06T14:51:40.420672Z",
     "end_time": "2023-04-06T14:51:40.451397Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent = DQNAgent()\n",
    "agent.run()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-06T13:35:49.173416Z",
     "end_time": "2023-04-06T13:35:50.050925Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
