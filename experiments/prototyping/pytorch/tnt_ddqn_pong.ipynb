{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-12T17:03:58.895481Z",
     "end_time": "2023-04-12T17:03:58.936057Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStack, RecordEpisodeStatistics, RecordVideo\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from stable_baselines3.common.save_util import load_from_pkl, save_to_pkl\n",
    "\n",
    "from algorithms.common.typing import HParams\n",
    "from algorithms.pytorch.common.tnt import TNT\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-12T17:05:20.528048Z",
     "end_time": "2023-04-12T17:05:20.632787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Box(0, 255, (4, 84, 84), uint8)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step_trigger(step: int):\n",
    "    return step % 400_000 == 0\n",
    "\n",
    "def make_env(env_name=\"ALE/Pong-v5\", seed=42):\n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\", full_action_space=False, frameskip=1)\n",
    "    env = AtariPreprocessing(env)\n",
    "    env = FrameStack(env, 4)\n",
    "    env = RecordEpisodeStatistics(env)\n",
    "    # A video will be recorded every 400,000 steps.\n",
    "    # (Can't use lambda expression because it is not supported by cloud pickle when saving checkpoint model)\n",
    "    env = RecordVideo(env, \"runs/videos/\", step_trigger=step_trigger, video_length=1000)\n",
    "\n",
    "    env.observation_space.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "\n",
    "    return env\n",
    "\n",
    "env = make_env()\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-12T17:05:20.631912Z",
     "end_time": "2023-04-12T17:05:20.720002Z"
    }
   },
   "outputs": [],
   "source": [
    "class TNTDQN(nn.Module):\n",
    "    def __init__(self, num_channels, num_actions):\n",
    "        super(TNTDQN, self).__init__()\n",
    "\n",
    "        self.tnt = TNT(\n",
    "            img_size=84,\n",
    "            in_chans=num_channels,\n",
    "            patch_size=4,\n",
    "            outer_dim=256, # embedding dimension\n",
    "            inner_num_heads=1,\n",
    "            outer_num_heads=4,\n",
    "            attn_drop_rate=0.0,\n",
    "            drop_rate=0.1,\n",
    "            num_classes=num_actions,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float() / 255  # Rescale input from [0, 255] to [0, 1]\n",
    "        return self.tnt(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **policy network**: $Q^{A}_{\\theta}$\n",
    "- **target network**: $Q^{B}_{\\theta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-12T17:05:21.929279Z",
     "end_time": "2023-04-12T17:05:21.937838Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def linear_schedule(start_epsilon: float, end_epsilon: float, duration: int, timestep: int):\n",
    "    slope = (end_epsilon - start_epsilon) / duration\n",
    "    return max(slope * timestep + start_epsilon, end_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-12T17:05:37.070617Z",
     "end_time": "2023-04-12T17:05:37.112124Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_latest_checkpoint_file(files: List[str]) -> any:\n",
    "    \"\"\"\n",
    "    Return the most recent checkpoint file from the passed list of files.\n",
    "\n",
    "    If multiple files with same datetime are passed, only the first is returned\n",
    "\n",
    "    :param files: list of file names containing a formatted datetime (=> %d-%m-%Y_%H:%M:%S)\n",
    "    :return: the file with the most recent date time or ``None`` if no files were found (because of the lack of correctly formatted date in the file name)\n",
    "    \"\"\"\n",
    "    datetime_regex = r\"\\d{2}-\\d{2}-\\d{4}_\\d{2}:\\d{2}:\\d{2}\"\n",
    "\n",
    "    latest_file = None\n",
    "    latest_datetime = datetime.min\n",
    "    for file in files:\n",
    "        match = re.search(datetime_regex, file)\n",
    "        if not match: continue # Go to next element in list if no match is found\n",
    "\n",
    "        file_datetime = datetime.strptime(match.group(), \"%d-%m-%Y_%H:%M:%S\")\n",
    "        if file_datetime > latest_datetime:\n",
    "            latest_datetime = file_datetime\n",
    "            latest_file = file\n",
    "\n",
    "    return latest_file\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, name, env_id, hparams: HParams):\n",
    "        self.name = name\n",
    "        self.env = make_env(env_id, seed=hparams[\"env_seed\"])\n",
    "\n",
    "        self.start_datetime = None\n",
    "        self.start_time = None\n",
    "\n",
    "        self.MAX_TIMESTEPS = hparams[\"max_timesteps\"]  # Maximum number of total steps\n",
    "        self.TARGET_UPDATE_INTERVAL = hparams[\"target_update_interval\"]  # Number of steps between the synchronisation of q and target network\n",
    "        self.LEARNING_STARTS = hparams[\"learning_starts\"]  # The number of steps to wait before we start the training, so the agent can explore and store its experience in the replay buffer\n",
    "\n",
    "        self.TRAIN_FREQUENCY = hparams[\"train_frequency\"] # Training is done each 4 steps\n",
    "\n",
    "        self.CHECKPOINT_INTERVAL_EPISODE = 1000 # Checkpoint saving interval per episode (a checkpoint will be saved each X episodes)\n",
    "\n",
    "        self.REPLAY_SIZE = hparams[\"replay_buffer_size\"]\n",
    "        self.BATCH_SIZE = hparams[\"batch_size\"]\n",
    "\n",
    "        self.GAMMA = hparams[\"discount_rate\"]  # Discount rate\n",
    "\n",
    "        self.EXPLORATION_FRACTION = hparams[\"exploration_fraction\"]  # The fraction of 'TOTAL_TIMESTEPS' it takes from 'EPSILON_START' to 'EPSILON_END'.\n",
    "        self.EPSILON_INITIAL = 1.0\n",
    "        self.EPSILON_FINAL = hparams[\"exploration_final\"]\n",
    "\n",
    "        self.epsilon = self.EPSILON_INITIAL  # Exploration probability\n",
    "\n",
    "        self.memory = ReplayBuffer(\n",
    "            buffer_size=self.REPLAY_SIZE,\n",
    "            observation_space=self.env.observation_space,\n",
    "            action_space=self.env.action_space,\n",
    "            device=device,\n",
    "            optimize_memory_usage=True,\n",
    "            handle_timeout_termination=False\n",
    "        )\n",
    "\n",
    "        self.timesteps = 0\n",
    "\n",
    "        self.policy_network = TNTDQN(4, self.env.action_space.n).to(device)\n",
    "        self.target_network = TNTDQN(4, self.env.action_space.n).to(device)\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.policy_network.parameters(), lr=hparams[\"learning_rate\"])\n",
    "        self.loss_fn = hparams[\"loss\"]\n",
    "\n",
    "        # Metrics/Logs\n",
    "        self.PATH = \"runs\"\n",
    "        if not os.path.exists(self.PATH):\n",
    "            os.makedirs(self.PATH)\n",
    "\n",
    "        self.CHECKPOINTS_PATH = f\"{self.PATH}/checkpoints\"\n",
    "        self.LOGS_PATH = f\"{self.PATH}/logs\"\n",
    "        self.VIDEO_PATH = f\"{self.PATH}/videos\"\n",
    "\n",
    "        self.is_loaded_from_checkpoint = False\n",
    "\n",
    "        self.writer = None\n",
    "\n",
    "    def remember(self, observation, next_observation, action, reward, done, infos):\n",
    "        self.memory.add(observation, next_observation, action, reward, done, infos)\n",
    "\n",
    "    def act(self, state):\n",
    "        # Reduce epsilon when learning started\n",
    "        if self.timesteps >= self.LEARNING_STARTS:\n",
    "            # Minus LEARNING_STARTS to takes into account that learning only started after LEARNING_STARTS,\n",
    "            # and so we want to start reducing epsilon only when learning start\n",
    "            self.epsilon = linear_schedule(\n",
    "                self.EPSILON_INITIAL,\n",
    "                self.EPSILON_FINAL,\n",
    "                int(self.EXPLORATION_FRACTION * self.MAX_TIMESTEPS),\n",
    "                self.timesteps - self.LEARNING_STARTS\n",
    "            )\n",
    "\n",
    "        if self.timesteps < self.LEARNING_STARTS or np.random.rand() < self.epsilon:\n",
    "            # Random action\n",
    "            return np.array(self.env.action_space.sample())\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor(np.array(state), device=device).unsqueeze(0)\n",
    "                q_values = self.policy_network(state_tensor)\n",
    "                return q_values.argmax(dim=1)[0].cpu().numpy()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "\n",
    "    def optimize_model(self):\n",
    "        minibatch = self.memory.sample(self.BATCH_SIZE)\n",
    "\n",
    "        # Calculate Q values for current states\n",
    "        # For each q_values, get the action according to the minibatch\n",
    "        q_values = self.policy_network(minibatch.observations).gather(1, minibatch.actions)\n",
    "\n",
    "        # Then, calculate the best actions for the next states, and return its indices\n",
    "        with torch.no_grad():\n",
    "            best_next_actions = self.policy_network(minibatch.next_observations).argmax(1).unsqueeze(1)\n",
    "\n",
    "        # Calculate the Q values for the next states using the target network, and return the action according to the best next action returned by the q network\n",
    "        target_next_q_values = self.target_network(minibatch.next_observations).gather(1, best_next_actions)\n",
    "\n",
    "        # Calculate the target Q values using Double DQN\n",
    "        target_q_values = minibatch.rewards + (1 - minibatch.dones) * self.GAMMA * target_next_q_values\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = self.loss_fn(q_values, target_q_values)\n",
    "\n",
    "        # Compute metrics for loss\n",
    "        if self.timesteps % 100 == 0:\n",
    "            self.writer.add_scalar(\"train/loss\", loss, self.timesteps)\n",
    "            self.writer.add_scalar(\"train/q_values\", q_values.squeeze().mean().item(), self.timesteps)\n",
    "            steps_per_second = int(self.timesteps / (time.time() - self.start_time))\n",
    "            #print(\"Steps per second: \", steps_per_second)\n",
    "            self.writer.add_scalar(\"train/steps_per_second\", steps_per_second, self.timesteps)\n",
    "\n",
    "\n",
    "        # Optimise Q network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        if self.start_datetime is None:\n",
    "            print(\"SAVE_CHECKPOINT_ERROR: Training need to have started to save a checkpoint.\")\n",
    "            return\n",
    "\n",
    "        print(\"Saving checkpoint...\")\n",
    "        current_datetime_str = datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
    "        start_datetime_str = self.start_datetime.strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
    "\n",
    "        save_parent_directory = f\"{self.CHECKPOINTS_PATH}/{self.name}_{start_datetime_str}\"\n",
    "        save_path = save_parent_directory + \"/chkpt_\" + current_datetime_str + \".tar\"\n",
    "        replay_buffer_path = save_parent_directory + \"/replay_buffer_\" + current_datetime_str\n",
    "\n",
    "        if not os.path.exists(save_parent_directory):\n",
    "            os.makedirs(save_parent_directory)\n",
    "\n",
    "        checkpoint = {\n",
    "            \"env\": self.env,\n",
    "            \"timesteps\": self.timesteps,\n",
    "            \"start_datetime\": self.start_datetime,\n",
    "            \"epsilon\": self.epsilon,\n",
    "            \"policy_network\": self.policy_network.state_dict(),\n",
    "            \"target_network\": self.target_network.state_dict(),\n",
    "            \"optimizer\": self.optimizer.state_dict(),\n",
    "        }\n",
    "\n",
    "        torch.save(checkpoint, save_path)\n",
    "        # Saving the replay buffer will takes time! But it is needed to properly resume training\n",
    "        save_to_pkl(replay_buffer_path, self.memory, verbose=1)\n",
    "\n",
    "        print(f\"Checkpoint saved into {save_parent_directory}\")\n",
    "\n",
    "    def load_last_checkpoint(self, path):\n",
    "        \"\"\"\n",
    "        Load the last saved checkpoint found in the given ``path``\n",
    "\n",
    "        :param path: the path to the directory containing the checkpoint(s)\n",
    "        \"\"\"\n",
    "        print(f\"Loading most recent checkpoint from {path}\")\n",
    "        self.is_loaded_from_checkpoint = True\n",
    "\n",
    "        # Using list comprehension to filter directories and only get the files\n",
    "        files = [file for file in os.listdir(path) if os.path.isfile(os.path.join(path, file))]\n",
    "\n",
    "        checkpoint_files = [chkpt_file for chkpt_file in files if \"chkpt\" in chkpt_file]\n",
    "        replay_buffer_files = [chkpt_file for chkpt_file in files if \"replay_buffer\" in chkpt_file]\n",
    "\n",
    "        checkpoint_file = get_latest_checkpoint_file(checkpoint_files)\n",
    "        replay_buffer_file = get_latest_checkpoint_file(replay_buffer_files)\n",
    "\n",
    "        checkpoint: Dict[str, any] = torch.load(path + \"/\" + checkpoint_file)\n",
    "\n",
    "        self.env = checkpoint[\"env\"]\n",
    "        self.timesteps = checkpoint[\"timesteps\"]\n",
    "        self.start_datetime: datetime = checkpoint[\"start_datetime\"]\n",
    "        self.start_time = self.start_datetime.timestamp()\n",
    "\n",
    "        self.epsilon = checkpoint[\"epsilon\"]\n",
    "\n",
    "        self.policy_network.load_state_dict(checkpoint[\"policy_network\"])\n",
    "        self.target_network.load_state_dict(checkpoint[\"target_network\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "        self.memory: ReplayBuffer = load_from_pkl(path + \"/\" + replay_buffer_file)\n",
    "        print(\"Checkpoint successfully loaded, you can resume the training now.\")\n",
    "\n",
    "    def run(self):\n",
    "        # Either create a new SummaryWriter or resume from previous one\n",
    "        if not self.is_loaded_from_checkpoint:\n",
    "            current_datetime = datetime.now()\n",
    "            self.start_datetime = current_datetime\n",
    "            self.start_time = current_datetime.timestamp()\n",
    "\n",
    "        start_datetime_str = self.start_datetime.strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
    "        self.writer = SummaryWriter(f\"{self.LOGS_PATH}/{self.name}_{start_datetime_str}\")\n",
    "\n",
    "        video_folder_path = f\"{self.VIDEO_PATH}/{self.name}_{start_datetime_str}\"\n",
    "        if not os.path.exists(video_folder_path):\n",
    "            os.makedirs(video_folder_path)\n",
    "        self.env.video_folder = video_folder_path\n",
    "\n",
    "        while self.timesteps < self.MAX_TIMESTEPS:\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                self.timesteps += 1\n",
    "\n",
    "                action = self.act(state)\n",
    "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                self.remember(state, next_state, action, reward, terminated, info)\n",
    "\n",
    "                if self.timesteps >= self.LEARNING_STARTS and self.timesteps % self.TRAIN_FREQUENCY == 0:\n",
    "                    self.optimize_model()\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    mean_reward = np.mean(self.env.return_queue)\n",
    "                    mean_length = np.mean(self.env.length_queue)\n",
    "\n",
    "                    # Get episode statistics from info (\"episode\" key only exist when episode is done)\n",
    "                    episode_reward = info[\"episode\"][\"r\"]\n",
    "                    self.writer.add_scalar(\"rollout/episodic_return\", episode_reward, self.timesteps)\n",
    "                    self.writer.add_scalar(\"rollout/episodic_length\", info[\"episode\"][\"l\"], self.timesteps)\n",
    "\n",
    "                    self.writer.add_scalar(\"rollout/ep_rew_mean\", mean_reward, self.timesteps)\n",
    "                    self.writer.add_scalar(\"rollout/ep_len_mean\", mean_length, self.timesteps)\n",
    "\n",
    "                    self.writer.add_scalar(\"rollout/exploration_rate\", self.epsilon, self.timesteps)\n",
    "\n",
    "\n",
    "                    print(f\"Episode {self.env.episode_count} finished (timesteps: {self.timesteps}/{self.MAX_TIMESTEPS})\\n\"\n",
    "                          f\"Epsilon: {self.epsilon:.2f}, Episode reward: {episode_reward.item()}, Mean reward: {mean_reward:.2f}\")\n",
    "\n",
    "                    if self.env.episode_count % self.CHECKPOINT_INTERVAL_EPISODE == 0:\n",
    "                        self.save_checkpoint()\n",
    "                    print(\"***************************\")\n",
    "\n",
    "                if self.timesteps >= self.LEARNING_STARTS and self.timesteps % self.TARGET_UPDATE_INTERVAL == 0:\n",
    "                    self.update_target_network()\n",
    "                    #print(\"Target model updated.\")\n",
    "\n",
    "        self.save_checkpoint() # Save last checkpoint at the end of training\n",
    "\n",
    "        self.writer.flush()\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# I use the same hyperparameters as this model: https://huggingface.co/sb3/dqn-PongNoFrameskip-v4\n",
    "hparams_config: HParams = {\n",
    "    \"env_seed\": 42,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"loss\": torch.nn.SmoothL1Loss(),\n",
    "    \"max_timesteps\": 10_000_000,\n",
    "    \"target_update_interval\": 1000,\n",
    "    \"learning_starts\": 100_000,\n",
    "    \"train_frequency\": 4,\n",
    "    \"replay_buffer_size\": 10_000,\n",
    "    \"batch_size\": 32,\n",
    "    \"discount_rate\": 0.99,\n",
    "    \"exploration_fraction\": 0.1,\n",
    "    \"exploration_final\": 0.01,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T17:05:38.267194Z",
     "end_time": "2023-04-12T17:05:38.274346Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T19:37:01.291338Z",
     "end_time": "2023-04-12T03:13:14.524312Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deewens/miniconda3/envs/pytorch/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:364: UserWarning: \u001B[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001B[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished (timesteps: 821/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -21.00\n",
      "***************************\n",
      "Moviepy - Building video runs/videos/TNT_Double_DQN_Pong-v5_12-04-2023_17:05:41/rl-video-step-0.mp4.\n",
      "Moviepy - Writing video runs/videos/TNT_Double_DQN_Pong-v5_12-04-2023_17:05:41/rl-video-step-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready runs/videos/TNT_Double_DQN_Pong-v5_12-04-2023_17:05:41/rl-video-step-0.mp4\n",
      "Episode 2 finished (timesteps: 1937/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.00\n",
      "***************************\n",
      "Episode 3 finished (timesteps: 2989/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.00\n",
      "***************************\n",
      "Episode 4 finished (timesteps: 3864/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.25\n",
      "***************************\n",
      "Episode 5 finished (timesteps: 4716/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.40\n",
      "***************************\n",
      "Episode 6 finished (timesteps: 5510/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.50\n",
      "***************************\n",
      "Episode 7 finished (timesteps: 6299/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.57\n",
      "***************************\n",
      "Episode 8 finished (timesteps: 7225/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.62\n",
      "***************************\n",
      "Episode 9 finished (timesteps: 8338/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.44\n",
      "***************************\n",
      "Episode 10 finished (timesteps: 9155/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.50\n",
      "***************************\n",
      "Episode 11 finished (timesteps: 10136/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.45\n",
      "***************************\n",
      "Episode 12 finished (timesteps: 10988/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.50\n",
      "***************************\n",
      "Episode 13 finished (timesteps: 11801/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.54\n",
      "***************************\n",
      "Episode 14 finished (timesteps: 12754/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.50\n",
      "***************************\n",
      "Episode 15 finished (timesteps: 13747/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.47\n",
      "***************************\n",
      "Episode 16 finished (timesteps: 14565/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.50\n",
      "***************************\n",
      "Episode 17 finished (timesteps: 15513/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.47\n",
      "***************************\n",
      "Episode 18 finished (timesteps: 16271/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.50\n",
      "***************************\n",
      "Episode 19 finished (timesteps: 17224/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.47\n",
      "***************************\n",
      "Episode 20 finished (timesteps: 18242/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.45\n",
      "***************************\n",
      "Episode 21 finished (timesteps: 19155/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.43\n",
      "***************************\n",
      "Episode 22 finished (timesteps: 20126/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.41\n",
      "***************************\n",
      "Episode 23 finished (timesteps: 20973/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.43\n",
      "***************************\n",
      "Episode 24 finished (timesteps: 21813/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.46\n",
      "***************************\n",
      "Episode 25 finished (timesteps: 22937/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.44\n",
      "***************************\n",
      "Episode 26 finished (timesteps: 23787/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.46\n",
      "***************************\n",
      "Episode 27 finished (timesteps: 24778/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.44\n",
      "***************************\n",
      "Episode 28 finished (timesteps: 25679/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.46\n",
      "***************************\n",
      "Episode 29 finished (timesteps: 26627/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.45\n",
      "***************************\n",
      "Episode 30 finished (timesteps: 27450/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.47\n",
      "***************************\n",
      "Episode 31 finished (timesteps: 28416/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.48\n",
      "***************************\n",
      "Episode 32 finished (timesteps: 29445/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.47\n",
      "***************************\n",
      "Episode 33 finished (timesteps: 30208/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.48\n",
      "***************************\n",
      "Episode 34 finished (timesteps: 31095/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.47\n",
      "***************************\n",
      "Episode 35 finished (timesteps: 32012/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.46\n",
      "***************************\n",
      "Episode 36 finished (timesteps: 32876/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.44\n",
      "***************************\n",
      "Episode 37 finished (timesteps: 33844/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.43\n",
      "***************************\n",
      "Episode 38 finished (timesteps: 34792/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.42\n",
      "***************************\n",
      "Episode 39 finished (timesteps: 35722/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.41\n",
      "***************************\n",
      "Episode 40 finished (timesteps: 36479/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.42\n",
      "***************************\n",
      "Episode 41 finished (timesteps: 37302/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.44\n",
      "***************************\n",
      "Episode 42 finished (timesteps: 38321/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.43\n",
      "***************************\n",
      "Episode 43 finished (timesteps: 39362/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.42\n",
      "***************************\n",
      "Episode 44 finished (timesteps: 40630/10000000)\n",
      "Epsilon: 1.00, Episode reward: -17.0, Mean reward: -20.34\n",
      "***************************\n",
      "Episode 45 finished (timesteps: 41926/10000000)\n",
      "Epsilon: 1.00, Episode reward: -18.0, Mean reward: -20.29\n",
      "***************************\n",
      "Episode 46 finished (timesteps: 42747/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.30\n",
      "***************************\n",
      "Episode 47 finished (timesteps: 43689/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.30\n",
      "***************************\n",
      "Episode 48 finished (timesteps: 44800/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.27\n",
      "***************************\n",
      "Episode 49 finished (timesteps: 45680/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.29\n",
      "***************************\n",
      "Episode 50 finished (timesteps: 46486/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.30\n",
      "***************************\n",
      "Episode 51 finished (timesteps: 47271/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.31\n",
      "***************************\n",
      "Episode 52 finished (timesteps: 48119/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 53 finished (timesteps: 48935/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.34\n",
      "***************************\n",
      "Episode 54 finished (timesteps: 49851/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.33\n",
      "***************************\n",
      "Episode 55 finished (timesteps: 50673/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.35\n",
      "***************************\n",
      "Episode 56 finished (timesteps: 51463/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.36\n",
      "***************************\n",
      "Episode 57 finished (timesteps: 52399/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.35\n",
      "***************************\n",
      "Episode 58 finished (timesteps: 53222/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.36\n",
      "***************************\n",
      "Episode 59 finished (timesteps: 54032/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.37\n",
      "***************************\n",
      "Episode 60 finished (timesteps: 54857/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.38\n",
      "***************************\n",
      "Episode 61 finished (timesteps: 55883/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.39\n",
      "***************************\n",
      "Episode 62 finished (timesteps: 56976/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.40\n",
      "***************************\n",
      "Episode 63 finished (timesteps: 57766/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.41\n",
      "***************************\n",
      "Episode 64 finished (timesteps: 58588/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.42\n",
      "***************************\n",
      "Episode 65 finished (timesteps: 59519/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.43\n",
      "***************************\n",
      "Episode 66 finished (timesteps: 60507/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.41\n",
      "***************************\n",
      "Episode 67 finished (timesteps: 61614/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.39\n",
      "***************************\n",
      "Episode 68 finished (timesteps: 62638/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.38\n",
      "***************************\n",
      "Episode 69 finished (timesteps: 63538/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.38\n",
      "***************************\n",
      "Episode 70 finished (timesteps: 64385/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.39\n",
      "***************************\n",
      "Episode 71 finished (timesteps: 65233/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.39\n",
      "***************************\n",
      "Episode 72 finished (timesteps: 66130/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.40\n",
      "***************************\n",
      "Episode 73 finished (timesteps: 67013/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.40\n",
      "***************************\n",
      "Episode 74 finished (timesteps: 67975/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.39\n",
      "***************************\n",
      "Episode 75 finished (timesteps: 68766/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.40\n",
      "***************************\n",
      "Episode 76 finished (timesteps: 69620/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.41\n",
      "***************************\n",
      "Episode 77 finished (timesteps: 70617/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.42\n",
      "***************************\n",
      "Episode 78 finished (timesteps: 71546/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.41\n",
      "***************************\n",
      "Episode 79 finished (timesteps: 72488/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.42\n",
      "***************************\n",
      "Episode 80 finished (timesteps: 73461/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.41\n",
      "***************************\n",
      "Episode 81 finished (timesteps: 74362/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.42\n",
      "***************************\n",
      "Episode 82 finished (timesteps: 75276/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.41\n",
      "***************************\n",
      "Episode 83 finished (timesteps: 76223/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.41\n",
      "***************************\n",
      "Episode 84 finished (timesteps: 77064/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.42\n",
      "***************************\n",
      "Episode 85 finished (timesteps: 78099/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.41\n",
      "***************************\n",
      "Episode 86 finished (timesteps: 78949/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.42\n",
      "***************************\n",
      "Episode 87 finished (timesteps: 79909/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.43\n",
      "***************************\n",
      "Episode 88 finished (timesteps: 80709/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.43\n",
      "***************************\n",
      "Episode 89 finished (timesteps: 81518/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.44\n",
      "***************************\n",
      "Episode 90 finished (timesteps: 82463/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.44\n",
      "***************************\n",
      "Episode 91 finished (timesteps: 83552/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.45\n",
      "***************************\n",
      "Episode 92 finished (timesteps: 84420/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.46\n",
      "***************************\n",
      "Episode 93 finished (timesteps: 85410/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.45\n",
      "***************************\n",
      "Episode 94 finished (timesteps: 86429/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.45\n",
      "***************************\n",
      "Episode 95 finished (timesteps: 87314/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.45\n",
      "***************************\n",
      "Episode 96 finished (timesteps: 88234/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.46\n",
      "***************************\n",
      "Episode 97 finished (timesteps: 89229/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.44\n",
      "***************************\n",
      "Episode 98 finished (timesteps: 90009/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.45\n",
      "***************************\n",
      "Episode 99 finished (timesteps: 90830/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.45\n",
      "***************************\n",
      "Episode 100 finished (timesteps: 91591/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.46\n",
      "***************************\n",
      "Episode 101 finished (timesteps: 92400/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.46\n",
      "***************************\n",
      "Episode 102 finished (timesteps: 93247/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.48\n",
      "***************************\n",
      "Episode 103 finished (timesteps: 94407/10000000)\n",
      "Epsilon: 1.00, Episode reward: -19.0, Mean reward: -20.47\n",
      "***************************\n",
      "Episode 104 finished (timesteps: 95233/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.47\n",
      "***************************\n",
      "Episode 105 finished (timesteps: 96379/10000000)\n",
      "Epsilon: 1.00, Episode reward: -20.0, Mean reward: -20.46\n",
      "***************************\n",
      "Episode 106 finished (timesteps: 97175/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.46\n",
      "***************************\n",
      "Episode 107 finished (timesteps: 98087/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.46\n",
      "***************************\n",
      "Episode 108 finished (timesteps: 99161/10000000)\n",
      "Epsilon: 1.00, Episode reward: -21.0, Mean reward: -20.46\n",
      "***************************\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 5.80 GiB total capacity; 1.49 GiB already allocated; 103.88 MiB free; 1.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m agent \u001B[38;5;241m=\u001B[39m DQNAgent(name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTNT_Double_DQN_Pong-v5\u001B[39m\u001B[38;5;124m\"\u001B[39m, env_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mALE/Pong-v5\u001B[39m\u001B[38;5;124m\"\u001B[39m, hparams\u001B[38;5;241m=\u001B[39mhparams_config)\n\u001B[0;32m----> 2\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[22], line 240\u001B[0m, in \u001B[0;36mDQNAgent.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    237\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mremember(state, next_state, action, reward, terminated, info)\n\u001B[1;32m    239\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimesteps \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mLEARNING_STARTS \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimesteps \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mTRAIN_FREQUENCY \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 240\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimize_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    242\u001B[0m state \u001B[38;5;241m=\u001B[39m next_state\n\u001B[1;32m    244\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m done:\n",
      "Cell \u001B[0;32mIn[22], line 116\u001B[0m, in \u001B[0;36mDQNAgent.optimize_model\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    112\u001B[0m minibatch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmemory\u001B[38;5;241m.\u001B[39msample(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mBATCH_SIZE)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;66;03m# Calculate Q values for current states\u001B[39;00m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;66;03m# For each q_values, get the action according to the minibatch\u001B[39;00m\n\u001B[0;32m--> 116\u001B[0m q_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy_network\u001B[49m\u001B[43m(\u001B[49m\u001B[43mminibatch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobservations\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mgather(\u001B[38;5;241m1\u001B[39m, minibatch\u001B[38;5;241m.\u001B[39mactions)\n\u001B[1;32m    118\u001B[0m \u001B[38;5;66;03m# Then, calculate the best actions for the next states, and return its indices\u001B[39;00m\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n",
      "File \u001B[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[19], line 19\u001B[0m, in \u001B[0;36mTNTDQN.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m     18\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mfloat() \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m255\u001B[39m  \u001B[38;5;66;03m# Rescale input from [0, 255] to [0, 1]\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtnt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Documents/Studies/Fourth Year Project/Sources/RL-and-NN-Experiments/algorithms/pytorch/common/tnt.py:317\u001B[0m, in \u001B[0;36mTNT.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    316\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m--> 317\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    318\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead(x)\n\u001B[1;32m    319\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/Documents/Studies/Fourth Year Project/Sources/RL-and-NN-Experiments/algorithms/pytorch/common/tnt.py:311\u001B[0m, in \u001B[0;36mTNT.forward_features\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    308\u001B[0m outer_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpos_drop(outer_tokens)\n\u001B[1;32m    310\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m blk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocks:\n\u001B[0;32m--> 311\u001B[0m     inner_tokens, outer_tokens \u001B[38;5;241m=\u001B[39m \u001B[43mblk\u001B[49m\u001B[43m(\u001B[49m\u001B[43minner_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mouter_tokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    313\u001B[0m outer_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(outer_tokens)\n\u001B[1;32m    314\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outer_tokens[:, \u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Documents/Studies/Fourth Year Project/Sources/RL-and-NN-Experiments/algorithms/pytorch/common/tnt.py:188\u001B[0m, in \u001B[0;36mBlock.forward\u001B[0;34m(self, inner_tokens, outer_tokens)\u001B[0m\n\u001B[1;32m    186\u001B[0m     outer_tokens \u001B[38;5;241m=\u001B[39m outer_tokens \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdrop_path(tmp_ \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mse_layer(tmp_))\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 188\u001B[0m     outer_tokens \u001B[38;5;241m=\u001B[39m outer_tokens \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdrop_path(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mouter_attn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mouter_norm1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mouter_tokens\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    189\u001B[0m     outer_tokens \u001B[38;5;241m=\u001B[39m outer_tokens \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdrop_path(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mouter_mlp(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mouter_norm2(outer_tokens)))\n\u001B[1;32m    190\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m inner_tokens, outer_tokens\n",
      "File \u001B[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Documents/Studies/Fourth Year Project/Sources/RL-and-NN-Experiments/algorithms/pytorch/common/tnt.py:129\u001B[0m, in \u001B[0;36mAttention.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    126\u001B[0m q, k \u001B[38;5;241m=\u001B[39m qk[\u001B[38;5;241m0\u001B[39m], qk[\u001B[38;5;241m1\u001B[39m]  \u001B[38;5;66;03m# make torchscript happy (cannot use tensor as tuple)\u001B[39;00m\n\u001B[1;32m    127\u001B[0m v \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mv(x)\u001B[38;5;241m.\u001B[39mreshape(B, N, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m)\n\u001B[0;32m--> 129\u001B[0m attn \u001B[38;5;241m=\u001B[39m (\u001B[43mq\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m@\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtranspose\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m) \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscale\n\u001B[1;32m    130\u001B[0m attn \u001B[38;5;241m=\u001B[39m attn\u001B[38;5;241m.\u001B[39msoftmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    131\u001B[0m attn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn_drop(attn)\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 5.80 GiB total capacity; 1.49 GiB already allocated; 103.88 MiB free; 1.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(name=\"TNT_Double_DQN_Pong-v5\", env_id=\"ALE/Pong-v5\", hparams=hparams_config)\n",
    "agent.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
