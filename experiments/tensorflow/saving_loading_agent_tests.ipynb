{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5097,
     "status": "ok",
     "timestamp": 1675338967774,
     "user": {
      "displayName": "Adrien Dudon",
      "userId": "04934219595809892013"
     },
     "user_tz": 0
    },
    "id": "mV8GpLptU7_x",
    "outputId": "a2f1dcc9-16c9-42e3-f23c-6505c33891b4"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStack\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1675338967776,
     "user": {
      "displayName": "Adrien Dudon",
      "userId": "04934219595809892013"
     },
     "user_tz": 0
    },
    "id": "8VHex2iTUE7p"
   },
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "class ReplayMemory:\n",
    "  def __init__(self, capacity):\n",
    "    self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.buffer)\n",
    "  \n",
    "  def append(self, experience):\n",
    "      self.buffer.append(experience)\n",
    "    \n",
    "  def sample(self, batch_size=32):\n",
    "    indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "    states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "    return np.array(states), np.array(actions), np.array(rewards), np.array(dones), np.array(next_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Test saving of replay memory\n",
    "\n",
    "test_env = gym.make(\"ALE/Pong-v5\", full_action_space=False, frameskip=1)\n",
    "test_env = AtariPreprocessing(test_env)\n",
    "test_env = FrameStack(test_env, 4)\n",
    "\n",
    "memory = ReplayMemory(50)\n",
    "state = np.array(test_env.reset()[0])\n",
    "for _ in range(1, 500):\n",
    "  action = test_env.action_space.sample()\n",
    "\n",
    "  next_state, reward, terminated, truncated, info = test_env.step(action)\n",
    "  next_state = np.array(next_state)\n",
    "  done = terminated or truncated\n",
    "\n",
    "  memory.append(Experience(state, action, reward, done, next_state))\n",
    "\n",
    "with open('replay_memory_checkpoint', 'wb') as file:\n",
    "  #print(memory.buffer)\n",
    "  pickle.dump(memory.buffer, file)\n",
    "\n",
    "newMemory = ReplayMemory(50)\n",
    "\n",
    "with open('replay_memory_checkpoint', 'rb') as file:\n",
    "  backup = pickle.load(file)\n",
    "  newMemory.buffer = backup\n",
    "\n",
    "print(newMemory.buffer)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "memory = ReplayMemory(10000)\n",
    "with open('last_checkpoint/replay_memory', 'rb') as file:\n",
    "     memory.buffer = pickle.load(file)\n",
    "\n",
    "print(memory.buffer[10000 - 1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "with open('last_checkpoint/hyperparameters.json', 'r') as file:\n",
    "  data = json.load(file)\n",
    "  print(data.get('epsilon'))\n",
    "  print(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1675338967777,
     "user": {
      "displayName": "Adrien Dudon",
      "userId": "04934219595809892013"
     },
     "user_tz": 0
    },
    "id": "-dHeg9KnZWhu"
   },
   "outputs": [],
   "source": [
    "def create_q_model(num_actions):\n",
    "    # Network defined by the Deepmind paper\n",
    "    inputs = layers.Input(shape=(4, 84, 84, 1))\n",
    "\n",
    "    # Convolutions on the frames on the screen\n",
    "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "\n",
    "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
    "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNwicaAPX5sN"
   },
   "source": [
    "## Inputs\n",
    "**Network models**\n",
    "- θ = Initial network model\n",
    "- θ' = Target network model\n",
    "- N' = Target network replacement frequency\n",
    "\n",
    "**Replay memory buffer**\n",
    "- D = empty replay buffer\n",
    "- $N_r$ = replay buffer maximum size  \n",
    "\n",
    "- $N_b$ = training batch size\n",
    "\n",
    "$N_r$ = 10,000, $N_b$ = 32, N' = 1,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1675338967780,
     "user": {
      "displayName": "Adrien Dudon",
      "userId": "04934219595809892013"
     },
     "user_tz": 0
    },
    "id": "-q_cze-cVyG4"
   },
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "\n",
    "class DoubleDQNAgent:\n",
    "  def __init__(self, env):\n",
    "    self.env = env\n",
    "\n",
    "    # Constants\n",
    "    self.NUM_ACTIONS = self.env.action_space.n\n",
    "    self.DISCOUNT_FACTOR = 0.99 # Discount factor gamma used in the Q-learning update\n",
    "    self.REPLAY_START_SIZE = 50000 # The agent is run for this number of steps before the training start. The resulting experience is used to populate the replay memory\n",
    "    # self.REPLAY_START_SIZE = 50\n",
    "    self.FINAL_EXPLORATION_STEP = 1000000 # Number of frames over which the initial value of epsilon is linearly annealed to its final value. \n",
    "    self.INITIAL_EXPLORATION = 1.0 # Initial value of epsilon in Epsilon-Greedy exploration\n",
    "    self.FINAL_EXPLORATION = 0.1 # Final value of epsilon in Epsilon-Greedy exploration\n",
    "    self.REPLAY_MEMORY_SIZE = 100000\n",
    "    self.MINIBATCH_SIZE = 32\n",
    "    self.TARGET_NETWORK_UPDATE_FREQUENCY = 10000 # The frequency with which the tqrget netzork is updqted (measured in the number of parameter updates)\n",
    "    self.LEARNING_RATE = 1e-04\n",
    "    self.UPDATE_FREQUENCY = 4\n",
    "\n",
    "    self.buffer = ReplayMemory(self.REPLAY_MEMORY_SIZE)\n",
    "\n",
    "    self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.LEARNING_RATE, clipnorm=1.0)\n",
    "    self.loss_function = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    self.train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "    self.train_accuracy = tf.keras.metrics.MeanSquaredError('train_accuracy')\n",
    "\n",
    "    self.model = create_q_model(self.NUM_ACTIONS)\n",
    "    self.target_model = create_q_model(self.NUM_ACTIONS)\n",
    "    self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    #self.EPSILON_INTERVAL = (1.0 -self.FINAL_EXPLORATION)\n",
    "    #self.EPSILON_DECAY_FACTOR = 0.99\n",
    "\n",
    "    # self.running_reward = 0\n",
    "    # self.episode_reward_history = []\n",
    "\n",
    "    # self.max_episodes = 10000\n",
    "    # self.max_step_per_episodes = 100\n",
    "\n",
    "  def train(self):\n",
    "    epsilon = self.INITIAL_EXPLORATION\n",
    "    episode_reward_history = []\n",
    "\n",
    "    episode_idx = 0\n",
    "    timestep = 0\n",
    "    while True:\n",
    "      episode_idx += 1\n",
    "      episode_reward = 0\n",
    "      done = False\n",
    "\n",
    "      state = np.array(self.env.reset()[0])\n",
    "\n",
    "      while not done:\n",
    "        timestep += 1\n",
    "        action = self.choose_action(state, epsilon)\n",
    "\n",
    "        # Reduce epsilon  if training started\n",
    "        if timestep > self.REPLAY_START_SIZE:\n",
    "          epsilon -= (self.INITIAL_EXPLORATION - self.FINAL_EXPLORATION) / self.FINAL_EXPLORATION_STEP\n",
    "          epsilon = max(epsilon, self.FINAL_EXPLORATION)\n",
    "\n",
    "        next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        next_state = np.array(next_state)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        episode_reward += reward\n",
    "        self.buffer.append(Experience(state, action, reward, done, next_state))\n",
    "        \n",
    "        # Only train if done observing (buffer has been filled enough)\n",
    "        if timestep % self.UPDATE_FREQUENCY == 0 and timestep > self.REPLAY_START_SIZE:\n",
    "          states_sample, actions_sample, rewards_sample, dones_sample, next_states_sample = self.buffer.sample(self.MINIBATCH_SIZE)\n",
    "          \n",
    "          states_sample_tensor = tf.convert_to_tensor(states_sample)\n",
    "          next_states_sample_tensor = tf.convert_to_tensor(next_states_sample)\n",
    "          actions_sample_tensor = tf.convert_to_tensor(actions_sample)\n",
    "\n",
    "          # Perform experience replay\n",
    "          # Predict the target q value from the next sample sates\n",
    "          target_q_values = self.target_model.predict(next_states_sample_tensor)\n",
    "\n",
    "          # Calculate the target q values by discounting the discount rate from the Q Value predicted by the target model\n",
    "          # (1 - minibatch.done) will be 0 if this is the terminated state, and thus, won't update the q_learning target (because 0 * x = 0)\n",
    "          # reduce_max get the maximum Q_value for each list of q_values returned by the target model (because we gave a batch of 32 states to the model)\n",
    "          target_q_values = rewards_sample + (1 - dones_sample) * self.DISCOUNT_FACTOR * tf.reduce_max(target_q_values, axis=1)\n",
    "\n",
    "          # Create a mask on the action stores in the sampled minibatch \n",
    "          # This allows us to only calculate the loss on the updated Q-values\n",
    "          # WHAT IS A ONE_HOT Tensor?\n",
    "          # A one hot tensor is a matrix representation of a categorical variable, \n",
    "          # where the matrix has a single 1 in the column corresponding to the category and all other entries are 0. \n",
    "          # In other words, a one-hot tensor is a vector of length equal to the number of categories with a single 1 in the position corresponding to the category and all other values as 0.\n",
    "          masks = tf.one_hot(actions_sample_tensor, self.NUM_ACTIONS)\n",
    "\n",
    "          # Now, we need to calculate the gardient descent, using GradientTape to record the operation made during the training of the Q Function network (main model)\n",
    "          # As stated above, GradientTape just record the operation made inside it, such as model training or calculation\n",
    "          with tf.GradientTape() as tape:\n",
    "            # We train the main network and record the training into the tape\n",
    "            q_values = self.model(states_sample_tensor)\n",
    "\n",
    "            # Apply the masks to the Q-values to get the Q-value only for taken action from the minibatch\n",
    "            masked_q_values = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "            loss = self.loss_function(target_q_values, masked_q_values)\n",
    "\n",
    "          # We can then performed the back propagation on te taped operation made while training the network\n",
    "          # Backpropagation\n",
    "          gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "          self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\n",
    "          self.train_loss(loss)\n",
    "          self.train_accuracy(target_q_values, masked_q_values)\n",
    "\n",
    "          if timestep % self.TARGET_NETWORK_UPDATE_FREQUENCY == 0:\n",
    "            # update the the target network with weights from the main network\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            print(\"Target network updated\")\n",
    "\n",
    "          # Save model every 10,000 iterations\n",
    "          if timestep % 10000 == 0:\n",
    "            print(\"Saving model...\")\n",
    "            self.model.save('pong-model.h5', overwrite=True)\n",
    "\n",
    "          print(\"Timestep: {}, Epsilon: {}, Reward: {}, Loss: {}\".format(timestep, epsilon, reward, loss))\n",
    "\n",
    "      with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', self.train_loss.result(), step=episode_idx)\n",
    "        tf.summary.scalar('accuracy', self.train_accuracy.result(), step=episode_idx)\n",
    "\n",
    "      template = 'Epoch {}, Loss: {}, Accuracy: {}'\n",
    "      print (template.format(episode_idx+1,\n",
    "                             self.train_loss.result(),\n",
    "                             self.train_accuracy.result()*100))\n",
    "\n",
    "      self.train_loss.reset_state()\n",
    "      self.train_accuracy.reset_state()\n",
    "\n",
    "      episode_reward_history.append(episode_reward)\n",
    "      # Update running reward to check condition for solving\n",
    "      episode_reward_history.append(episode_reward)\n",
    "      if len(episode_reward_history) > 100:\n",
    "          del episode_reward_history[:1]\n",
    "      mean_reward = np.mean(episode_reward_history)\n",
    "\n",
    "      print(\"Episode {} finished!\".format(episode_idx))\n",
    "      print(\"Episode reward: {}, Mean reward: {}\".format(episode_reward, mean_reward))\n",
    "      print(\"******************\")\n",
    "\n",
    "  def choose_action(self, state, epsilon):\n",
    "    # Use epsilon greedy policy to select actions.\n",
    "    if np.random.random() < epsilon:\n",
    "      action = self.env.action_space.sample()\n",
    "    else:\n",
    "      # Predict action Q-values\n",
    "      state_tensor = tf.convert_to_tensor(state) # Convert the state numpy array to a tensor array because tensorflow only accept tensor\n",
    "      state_tensor = tf.expand_dims(state_tensor, 0) # Add one dimension to the state because this is a batch of only one state\n",
    "      q_values = self.model(state_tensor, training=False) # Call the model to predict the Q-Value according to the passed state\n",
    "\n",
    "      # Take best action from the returned q_values\n",
    "      # tf.argmax return the index with the largest q_values\n",
    "      action = tf.argmax(q_values[0]).numpy() # convert it back to np because it returns a Tensor\n",
    "\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lLvI3-XZZqOn",
    "outputId": "7238e65b-3266-4462-cfaf-4cf35acdeb0b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Pong-v5\", full_action_space=False, frameskip=1)\n",
    "env = AtariPreprocessing(env, grayscale_newaxis=True)\n",
    "env = FrameStack(env, 4)\n",
    "\n",
    "%tensorboard --logdir logs/gradient_tape\n",
    "agent = DoubleDQNAgent(env)\n",
    "agent.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yH893VxpOcjG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPjxUQNacmLyIsv93bUz4HA",
   "provenance": [
    {
     "file_id": "18HbejTFWAGcCODv5l3DO0cgiwzu87g9U",
     "timestamp": 1675280128956
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
