{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Save and Load Models\n",
    "Quick summary of this tutorial: https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class TheModelClass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TheModelClass, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T11:38:48.468260Z",
     "end_time": "2023-04-07T11:38:48.470235Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "model = TheModelClass()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "loss = nn.SmoothL1Loss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T11:52:06.765349Z",
     "end_time": "2023-04-07T11:52:06.782241Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "conv1.weight \t torch.Size([6, 3, 5, 5])\n",
      "conv1.bias \t torch.Size([6])\n",
      "conv2.weight \t torch.Size([16, 6, 5, 5])\n",
      "conv2.bias \t torch.Size([16])\n",
      "fc1.weight \t torch.Size([120, 400])\n",
      "fc1.bias \t torch.Size([120])\n",
      "fc2.weight \t torch.Size([84, 120])\n",
      "fc2.bias \t torch.Size([84])\n",
      "fc3.weight \t torch.Size([10, 84])\n",
      "fc3.bias \t torch.Size([10])\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}]\n"
     ]
    }
   ],
   "source": [
    "# Print model's sate_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T11:38:48.721815Z",
     "end_time": "2023-04-07T11:38:48.819691Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving & Loading Model for Inference (inference = put the model in production)\n",
    "### Save/Load `state_dict` (Recommended)\n",
    "When saving a model for inference, it is only necessary to save the trained model's learned parameters. Saving the model's *state_dict* with the `torch.save()` function will give you the most flexibility for restoring the model later. which is why it is the recommended method for saving models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Savin models using either a .pt or .pth file extension is a common PyTorch convention\n",
    "torch.save(model.state_dict(), \"saved_model.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T11:40:59.474859Z",
     "end_time": "2023-04-07T11:40:59.494665Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "model = TheModelClass()\n",
    "model.load_state_dict(torch.load(\"saved_model.pth\"))\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T11:38:48.916490Z",
     "end_time": "2023-04-07T11:38:48.957452Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You must call `model.eval()` to set dropout and batch normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results!\n",
    "\n",
    "> Notice that the load_state_dict() function takes a dictionary object, NOT a path to a saved object. This means that you must deserialize the saved state_dict before you pass it to the load_state_dict() function. For example, you CANNOT load using model.load_state_dict(PATH)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> If you only plan to keep the best performing model (according to the acquired validation loss), don’t forget that best_model_state = model.state_dict() returns a reference to the state and not its copy! You must serialize best_model_state or use best_model_state = deepcopy(model.state_dict()) otherwise your best best_model_state will keep getting updated by the subsequent training iterations. As a result, the final model state will be the state of the overfitted model.\n",
    "\n",
    "### Save/Load Entire Model\n",
    "Not really flexible because it is bounded to the exact class and directory structure used when the model is saved, so I'm not going to try that. Here is the link in case: https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-entire-model\n",
    "\n",
    "### Export/Load Model in TorchScript Format\n",
    "TorchScript is the recommended model format for scaled inference and deployment. (so not really useful for me rn): https://pytorch.org/tutorials/beginner/saving_loading_models.html#export-load-model-in-torchscript-format"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving & Loading a General Checkpoint for Inference and/or Resuming Training\n",
    "When saving a general checkpoint, to be used for either inference or resuming training, you must save more than just the model’s state_dict. It is important to also save the optimizer’s state_dict, as this contains buffers and parameters that are updated as the model trains. Other items that you may want to save are the epoch you left off on, the latest recorded training loss, external torch.nn.Embedding layers, etc. As a result, such a checkpoint is often 2~3 times larger than the model alone.\n",
    "\n",
    "To save multiple components, organize them in a dictionary and use torch.save() to serialize the dictionary. A common PyTorch convention is to save these checkpoints using the .tar file extension."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Save\n",
    "epoch = 150\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss,\n",
    "    # ...\n",
    "}, \"saved_checkpoint.tar\")\n",
    "# Convention to save multiple components is to use the .tar file extension."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T11:55:18.646168Z",
     "end_time": "2023-04-07T11:55:18.689524Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "TheModelClass(\n  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=400, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=10, bias=True)\n)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load\n",
    "model = TheModelClass()\n",
    "\n",
    "checkpoint = torch.load(\"saved_checkpoint.tar\")\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "epoch = checkpoint[\"epoch\"]\n",
    "loss = checkpoint[\"loss\"]\n",
    "\n",
    "model.eval()\n",
    "# - or -\n",
    "model.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T11:55:55.410451Z",
     "end_time": "2023-04-07T11:55:55.426603Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving Multiple Models in One File\n",
    "When saving a model comprised of multiple torch.nn.Modules, such as a GAN, a sequence-to-sequence model, or an ensemble of models, you follow the same approach as when you are saving a general checkpoint. In other words, save a dictionary of each model’s state_dict and corresponding optimizer. As mentioned before, you can save any other items that may aid you in resuming training by simply appending them to the dictionary."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "modelA = TheModelClass()\n",
    "optimizerA = optim.SGD(modelA.parameters(), lr=0.001)\n",
    "modelB = TheModelClass()\n",
    "optimizerB = optim.SGD(modelA.parameters(), lr=0.001)\n",
    "\n",
    "torch.save({\n",
    "    'modelA_state_dict': modelA.state_dict(),\n",
    "    'modelB_state_dict': modelB.state_dict(),\n",
    "    'optimizeA_state_dict': optimizerA.state_dict(),\n",
    "    'optimizerB_state_dict': optimizerB.state_dict(),\n",
    "    # ...\n",
    "}, \"saved_multiple_models.tar\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T11:59:02.262827Z",
     "end_time": "2023-04-07T11:59:02.305327Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"saved_multiple_models.tar\")\n",
    "modelA.load_state_dict(checkpoint[\"modelA_state_dict\"])\n",
    "modelB.load_state_dict(checkpoint[\"modelB_state_dict\"])\n",
    "optimizerA.load_state_dict(checkpoint[\"optimizerA_state_dict\"])\n",
    "optimizerB.load_state_dict(checkpoint[\"optimizerB_state_dict\"])\n",
    "\n",
    "modelA.eval()\n",
    "modelB.eval()\n",
    "# - or -\n",
    "modelA.train()\n",
    "modelB.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "'07-04-2023_12:34:45'"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "current_datetime = datetime.datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
    "current_datetime"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T12:34:45.143424Z",
     "end_time": "2023-04-07T12:34:45.150171Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
