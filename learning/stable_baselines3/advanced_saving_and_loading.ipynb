{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Advanced Saving and Loading\n",
    "In this example, we show how to use a policy independently from a model (and how to save it, load it) and save/load a replay buffer.\n",
    "\n",
    "From: [https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#id3](https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#id3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "By default, the replay buffer is not saved when calling `model.save()`, in order to save space on the disk (a replay buffer can be up to several GB when using images). However, SB3 provides a `save_replay_buffer()` and `load_replay_buffer()` method to save it separately."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-08T15:04:13.720504Z",
     "end_time": "2023-04-08T15:04:13.761859Z"
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.sac.policies import MlpPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -1.4e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 139      |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 800      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 24       |\n",
      "|    critic_loss     | 0.0879   |\n",
      "|    ent_coef        | 0.507    |\n",
      "|    ent_coef_loss   | -0.928   |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 699      |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.47e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 135       |\n",
      "|    time_elapsed    | 11        |\n",
      "|    total_timesteps | 1600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 52.2      |\n",
      "|    critic_loss     | 0.0652    |\n",
      "|    ent_coef        | 0.262     |\n",
      "|    ent_coef_loss   | -1.2      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 1499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.26e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 133       |\n",
      "|    time_elapsed    | 18        |\n",
      "|    total_timesteps | 2400      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 66.8      |\n",
      "|    critic_loss     | 0.155     |\n",
      "|    ent_coef        | 0.201     |\n",
      "|    ent_coef_loss   | -0.157    |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 2299      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -999     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 132      |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 3200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 65.6     |\n",
      "|    critic_loss     | 0.258    |\n",
      "|    ent_coef        | 0.2      |\n",
      "|    ent_coef_loss   | -0.142   |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3099     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -837     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 131      |\n",
      "|    time_elapsed    | 30       |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 69.8     |\n",
      "|    critic_loss     | 0.316    |\n",
      "|    ent_coef        | 0.172    |\n",
      "|    ent_coef_loss   | 0.247    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -718     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 131      |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 4800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 74.6     |\n",
      "|    critic_loss     | 0.866    |\n",
      "|    ent_coef        | 0.135    |\n",
      "|    ent_coef_loss   | 0.36     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 4699     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -773     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 129      |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 5600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 75.3     |\n",
      "|    critic_loss     | 2.02     |\n",
      "|    ent_coef        | 0.13     |\n",
      "|    ent_coef_loss   | -0.42    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5499     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "<stable_baselines3.sac.sac.SAC at 0x7f8f106c8ee0>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Creating environment from the given name 'Pendulum-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.57e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 149       |\n",
      "|    time_elapsed    | 5         |\n",
      "|    total_timesteps | 800       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 29.4      |\n",
      "|    critic_loss     | 0.0431    |\n",
      "|    ent_coef        | 0.5       |\n",
      "|    ent_coef_loss   | -1        |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 699       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.53e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 135       |\n",
      "|    time_elapsed    | 11        |\n",
      "|    total_timesteps | 1600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 54.8      |\n",
      "|    critic_loss     | 0.0609    |\n",
      "|    ent_coef        | 0.264     |\n",
      "|    ent_coef_loss   | -0.98     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 1499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.33e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 132       |\n",
      "|    time_elapsed    | 18        |\n",
      "|    total_timesteps | 2400      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 69.5      |\n",
      "|    critic_loss     | 0.185     |\n",
      "|    ent_coef        | 0.211     |\n",
      "|    ent_coef_loss   | -0.196    |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 2299      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.14e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 132       |\n",
      "|    time_elapsed    | 24        |\n",
      "|    total_timesteps | 3200      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 76        |\n",
      "|    critic_loss     | 0.634     |\n",
      "|    ent_coef        | 0.213     |\n",
      "|    ent_coef_loss   | 0.0667    |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 3099      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -947     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 130      |\n",
      "|    time_elapsed    | 30       |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 80.9     |\n",
      "|    critic_loss     | 0.552    |\n",
      "|    ent_coef        | 0.208    |\n",
      "|    ent_coef_loss   | -0.38    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -816     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 131      |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 4800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 69.9     |\n",
      "|    critic_loss     | 1.13     |\n",
      "|    ent_coef        | 0.167    |\n",
      "|    ent_coef_loss   | -0.0124  |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 4699     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -730     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 131      |\n",
      "|    time_elapsed    | 42       |\n",
      "|    total_timesteps | 5600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 71.7     |\n",
      "|    critic_loss     | 1.33     |\n",
      "|    ent_coef        | 0.145    |\n",
      "|    ent_coef_loss   | -0.206   |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5499     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "<stable_baselines3.sac.sac.SAC at 0x7f8f08118250>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model and the training environment\n",
    "model = SAC(\"MlpPolicy\", \"Pendulum-v1\", verbose=1, learning_rate=1e-3)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=6000)\n",
    "\n",
    "model.save(\"sac_pendulum\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T15:05:02.314486Z",
     "end_time": "2023-04-08T15:05:47.962631Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loaded_model has 0 transitions in its buffer\n"
     ]
    }
   ],
   "source": [
    "# The saved model does not contain the replay buffer\n",
    "loaded_model = SAC.load(\"sac_pendulum\")\n",
    "print(f\"The loaded_model has {loaded_model.replay_buffer.size()} transitions in its buffer\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T15:06:14.261255Z",
     "end_time": "2023-04-08T15:06:14.319674Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Now, save the replay buffer too\n",
    "model.save_replay_buffer(\"sac_replay_buffer\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T15:06:29.072499Z",
     "end_time": "2023-04-08T15:06:29.119307Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loaded_model has 6000 transitions in its buffer\n"
     ]
    }
   ],
   "source": [
    "# Load it into the loaded_model\n",
    "loaded_model.load_replay_buffer(\"sac_replay_buffer\")\n",
    "\n",
    "# Now the loaded replay is not empty anymore\n",
    "print(f\"The loaded_model has {loaded_model.replay_buffer.size()} transitions in its buffer\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T15:07:52.499571Z",
     "end_time": "2023-04-08T15:07:52.581741Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Save the policy independently from the model\n",
    "# Note: if you don't save the complete model with `model.save()`\n",
    "# you cannot continue training afterward\n",
    "policy = model.policy\n",
    "policy.save(\"sac_policy_pendulum\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T15:08:48.844635Z",
     "end_time": "2023-04-08T15:08:48.885761Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward=-123.37 +/- 78.91112482520943\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the environment\n",
    "env = model.get_env()\n",
    "\n",
    "# Evaluate the policy\n",
    "mean_reward, std_reward = evaluate_policy(policy, env, n_eval_episodes=10, deterministic=True)\n",
    "\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T15:10:04.258012Z",
     "end_time": "2023-04-08T15:10:05.551441Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward=-147.48 +/- 76.16165036682195\n"
     ]
    }
   ],
   "source": [
    "# Load the policy independently from the model\n",
    "saved_policy = MlpPolicy.load(\"sac_policy_pendulum\")\n",
    "\n",
    "# Evaluate the loaded policy\n",
    "mean_reward, std_reward = evaluate_policy(saved_policy, env, n_eval_episodes=10, deterministic=True)\n",
    "\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T15:11:10.811476Z",
     "end_time": "2023-04-08T15:11:12.353745Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
