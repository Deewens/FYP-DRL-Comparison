{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-03T18:51:25.652659Z",
     "end_time": "2023-04-03T18:51:26.455749Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "\n",
    "def random_games():\n",
    "    # Each of this episode is its own game.\n",
    "    for episode in range(10):\n",
    "        env.reset()\n",
    "        # This is each frame, up to 500... but we wont make it that far with random.\n",
    "        for t in range(500):\n",
    "            # Display the environment\n",
    "            env.render()\n",
    "\n",
    "            # Just create a sample action in any environment\n",
    "            # In CartPole, the action can be 0 or 1 (left or right)\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            # Execute the environment with an action and returns the observation of the environment, if the env is over and other infos\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            print(t, next_state, reward, done, info, action)\n",
    "            if done:\n",
    "                break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "random_games()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def OurModel(input_shape, action_space):\n",
    "    x_input = Input(input_shape)\n",
    "\n",
    "    # 'Dense' is the basic form of a neural network layer\n",
    "    # Input Layer of state size(4) and Hidden Layer with 512 nodes\n",
    "    x = Dense(512, input_shape=input_shape, activation=\"relu\", kernel_initializer='he_uniform')(x_input)\n",
    "\n",
    "    # Hidden layer with 256 nodes\n",
    "    x = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(x)\n",
    "\n",
    "    # Hidden layer with 64 nodes\n",
    "    x = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(x)\n",
    "\n",
    "    # Output Layer with # of actions: 2 nodes (left, right)\n",
    "    x = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(x)\n",
    "\n",
    "    model = Model(inputs = x_input, outputs = x, name='cartpole_dqn_model')\n",
    "    model.compile(loss=\"mse\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T18:51:26.430656Z",
     "end_time": "2023-04-03T18:51:26.475611Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "\n",
    "        # By default cartpole-v1 has max episode steps = 500\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "\n",
    "        self.MAX_EPISODES = 1000\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        self.GAMMA = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.EPSILON_MIN = 0.001\n",
    "        self.EPSILON_DECAY = 0.999\n",
    "        self.BATCH_SIZE = 64\n",
    "\n",
    "        self.TRAIN_START = 1000\n",
    "\n",
    "        # Create model\n",
    "        self.model = OurModel(input_shape=(self.state_size, ), action_space=self.action_size)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "        # Decay epsilon after training started\n",
    "        if len(self.memory) > self.TRAIN_START:\n",
    "            if self.epsilon > self.EPSILON_MIN:\n",
    "                self.epsilon *= self.EPSILON_DECAY\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state, verbose=0))\n",
    "\n",
    "    def replay(self):\n",
    "        # Do not train the network is training did not started\n",
    "        if len(self.memory) < self.TRAIN_START:\n",
    "            return\n",
    "\n",
    "        # Randomly sample minibatch from the memory\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), self.BATCH_SIZE))\n",
    "\n",
    "        state = np.zeros((self.BATCH_SIZE, self.state_size))\n",
    "        next_state = np.zeros((self.BATCH_SIZE, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        # do this before prediction\n",
    "        # for speedup, this could be done on the tensor level\n",
    "        # but easier to understand using a loop\n",
    "        for i in range(self.BATCH_SIZE):\n",
    "            state[i] = minibatch[i][0]\n",
    "            action.append(minibatch[i][1])\n",
    "            reward.append(minibatch[i][2])\n",
    "            next_state[i] = minibatch[i][3]\n",
    "            done.append(minibatch[i][4])\n",
    "\n",
    "        # Batch prediction to save speed\n",
    "        target = self.model.predict(state, verbose=0)\n",
    "        target_next = self.model.predict(next_state, verbose=0)\n",
    "\n",
    "        for i in range(self.BATCH_SIZE):\n",
    "            # Correction on the Q value for the action used\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # Standard - DQN\n",
    "                # DQN chooses the max Q value among next actions\n",
    "                # selection and evaluation of action is on the target Q Network\n",
    "                # Q_max = max_a' Q_target(s', a')\n",
    "                target[i][action[i]] = reward[i] + self.GAMMA * (np.amax(target_next[i]))\n",
    "\n",
    "        # Train the neural network\n",
    "        self.model.fit(state, target, batch_size=self.BATCH_SIZE, verbose=0)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model = tf.keras.models.load_model(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)\n",
    "\n",
    "    def run(self):\n",
    "        for episode in range(self.MAX_EPISODES):\n",
    "            state, _ = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                #self.env.render()\n",
    "                action = self.act(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                next_state = np.reshape(next_state, [1, self.state_size])\n",
    "                if not done or i == self.env.spec.max_episode_steps -1:\n",
    "                    reward = reward\n",
    "                else:\n",
    "                    reward = -100\n",
    "\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "\n",
    "                state = next_state\n",
    "                i += 1\n",
    "\n",
    "                if done:\n",
    "                    print(f\"Episode: {episode}/{self.MAX_EPISODES}, score: {i}, epsilon: {self.epsilon:.2}\")\n",
    "                    if i == 500:\n",
    "                        print(\"Saving trained model as cartpole-dqn.h5\")\n",
    "                        self.save(\"cartpole-dqn.h5\")\n",
    "                        return\n",
    "\n",
    "                self.replay()\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "        self.load(\"cartpole-dqn.h5\")\n",
    "        for e in range(self.MAX_EPISODES):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = np.argmax(self.model.predict(state))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                state = np.reshape(next_state, [1, self.state_size])\n",
    "                i += 1\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.MAX_EPISODES, i))\n",
    "                    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T18:52:02.234142Z",
     "end_time": "2023-04-03T18:52:02.268586Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cartpole_dqn_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               2560      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 150,466\n",
      "Trainable params: 150,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Episode: 0/1000, score: 26, epsilon: 1.0\n",
      "Episode: 1/1000, score: 32, epsilon: 1.0\n",
      "Episode: 2/1000, score: 22, epsilon: 1.0\n",
      "Episode: 3/1000, score: 14, epsilon: 1.0\n",
      "Episode: 4/1000, score: 36, epsilon: 1.0\n",
      "Episode: 5/1000, score: 36, epsilon: 1.0\n",
      "Episode: 6/1000, score: 36, epsilon: 1.0\n",
      "Episode: 7/1000, score: 24, epsilon: 1.0\n",
      "Episode: 8/1000, score: 15, epsilon: 1.0\n",
      "Episode: 9/1000, score: 19, epsilon: 1.0\n",
      "Episode: 10/1000, score: 21, epsilon: 1.0\n",
      "Episode: 11/1000, score: 17, epsilon: 1.0\n",
      "Episode: 12/1000, score: 32, epsilon: 1.0\n",
      "Episode: 13/1000, score: 26, epsilon: 1.0\n",
      "Episode: 14/1000, score: 35, epsilon: 1.0\n",
      "Episode: 15/1000, score: 27, epsilon: 1.0\n",
      "Episode: 16/1000, score: 12, epsilon: 1.0\n",
      "Episode: 17/1000, score: 15, epsilon: 1.0\n",
      "Episode: 18/1000, score: 26, epsilon: 1.0\n",
      "Episode: 19/1000, score: 26, epsilon: 1.0\n",
      "Episode: 20/1000, score: 22, epsilon: 1.0\n",
      "Episode: 21/1000, score: 20, epsilon: 1.0\n",
      "Episode: 22/1000, score: 17, epsilon: 1.0\n",
      "Episode: 23/1000, score: 18, epsilon: 1.0\n",
      "Episode: 24/1000, score: 14, epsilon: 1.0\n",
      "Episode: 25/1000, score: 24, epsilon: 1.0\n",
      "Episode: 26/1000, score: 33, epsilon: 1.0\n",
      "Episode: 27/1000, score: 17, epsilon: 1.0\n",
      "Episode: 28/1000, score: 28, epsilon: 1.0\n",
      "Episode: 29/1000, score: 25, epsilon: 1.0\n",
      "Episode: 30/1000, score: 13, epsilon: 1.0\n",
      "Episode: 31/1000, score: 42, epsilon: 1.0\n",
      "Episode: 32/1000, score: 18, epsilon: 1.0\n",
      "Episode: 33/1000, score: 16, epsilon: 1.0\n",
      "Episode: 34/1000, score: 22, epsilon: 1.0\n",
      "Episode: 35/1000, score: 21, epsilon: 1.0\n",
      "Episode: 36/1000, score: 26, epsilon: 1.0\n",
      "Episode: 37/1000, score: 44, epsilon: 1.0\n",
      "Episode: 38/1000, score: 25, epsilon: 1.0\n",
      "Episode: 39/1000, score: 18, epsilon: 1.0\n",
      "Episode: 40/1000, score: 17, epsilon: 1.0\n",
      "Episode: 41/1000, score: 15, epsilon: 1.0\n",
      "Episode: 42/1000, score: 14, epsilon: 0.99\n",
      "Episode: 43/1000, score: 17, epsilon: 0.98\n",
      "Episode: 44/1000, score: 18, epsilon: 0.96\n",
      "Episode: 45/1000, score: 34, epsilon: 0.93\n",
      "Episode: 46/1000, score: 36, epsilon: 0.89\n",
      "Episode: 47/1000, score: 48, epsilon: 0.85\n",
      "Episode: 48/1000, score: 10, epsilon: 0.84\n",
      "Episode: 49/1000, score: 11, epsilon: 0.84\n",
      "Episode: 50/1000, score: 56, epsilon: 0.79\n",
      "Episode: 51/1000, score: 59, epsilon: 0.74\n",
      "Episode: 52/1000, score: 55, epsilon: 0.7\n",
      "Episode: 53/1000, score: 33, epsilon: 0.68\n",
      "Episode: 54/1000, score: 61, epsilon: 0.64\n",
      "Episode: 55/1000, score: 35, epsilon: 0.62\n",
      "Episode: 56/1000, score: 181, epsilon: 0.52\n",
      "Episode: 57/1000, score: 500, epsilon: 0.31\n",
      "SAving trained model as cartpole-dqn.h5\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent()\n",
    "agent.run()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T18:52:02.683996Z",
     "end_time": "2023-04-03T18:54:17.619880Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
