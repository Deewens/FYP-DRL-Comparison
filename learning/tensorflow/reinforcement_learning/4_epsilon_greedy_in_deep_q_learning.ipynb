{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Epsilon Greedy in Deep Q Learning\n",
    "From this tutorial: [https://pylessons.com/Epsilon-Greedy-DQN](https://pylessons.com/Epsilon-Greedy-DQN)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-04T11:45:47.637921Z",
     "end_time": "2023-04-04T11:45:47.647377Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda, Add\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def OurModel(input_shape, action_space, dueling=False):\n",
    "    x_input = Input(input_shape)\n",
    "\n",
    "    # 'Dense' is the basic form of a neural network layer\n",
    "    # Input Layer of state size(4) and Hidden Layer with 512 nodes\n",
    "    x = Dense(512, input_shape=input_shape, activation=\"relu\", kernel_initializer='he_uniform')(x_input)\n",
    "\n",
    "    # Hidden layer with 256 nodes\n",
    "    x = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(x)\n",
    "\n",
    "    # Hidden layer with 64 nodes\n",
    "    x = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(x)\n",
    "\n",
    "    if dueling:\n",
    "        state_value = Dense(1, kernel_initializer=\"he_uniform\")(x)\n",
    "        state_value = Lambda(lambda s: tf.expand_dims(s[:,0], -1), output_shape=(action_space,))(state_value)\n",
    "\n",
    "        action_advantage = Dense(action_space, kernel_initializer=\"he_uniform\")(x)\n",
    "        action_advantage = Lambda(lambda a: a[:, :] - tf.reduce_mean(a[:, :], keepdims=True), output_shape=(action_space,))(action_advantage)\n",
    "\n",
    "        x = Add()([state_value, action_advantage])\n",
    "    else:\n",
    "        # Output Layer with # of actions: 2 nodes (left, right)\n",
    "        x = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(x)\n",
    "\n",
    "    model = Model(inputs = x_input, outputs = x)\n",
    "    model.compile(loss=\"mse\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T11:37:08.008531Z",
     "end_time": "2023-04-04T11:37:08.035653Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.env.spec.max_episode_steps = 4000\n",
    "\n",
    "        # By default cartpole-v1 has max episode steps = 500\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "\n",
    "        self.MAX_EPISODES = 1000\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        self.GAMMA = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.EPSILON_MIN = 0.01\n",
    "        self.EPSILON_DECAY = 0.0005\n",
    "        self.BATCH_SIZE = 32\n",
    "\n",
    "        self.TRAIN_START = 1000\n",
    "\n",
    "        self.TAU = 0.1 # Target network soft update hyperparameter\n",
    "\n",
    "        self.ddqn = True\n",
    "        self.soft_update = False # Use Soft update to update the target network?\n",
    "        self.dueling = True # Use dueling network\n",
    "        self.epsilon_greedy = True # Use epsilon greedy strategy\n",
    "\n",
    "        self.save_path = 'models'\n",
    "        self.scores, self.episodes, self.averages = [], [], []\n",
    "\n",
    "        self.model_name = \"CartPole-v1\"\n",
    "\n",
    "        if self.ddqn:\n",
    "            if self.dueling:\n",
    "                print(\"---------Dueling Double DQN---------\")\n",
    "                self.model_name = os.path.join(self.save_path, self.model_name + \"_Dueling_DDQN_e_greedy.h5\")\n",
    "            else:\n",
    "                print(\"---------Double DQN---------\")\n",
    "                self.model_name = os.path.join(self.save_path, self.model_name + \"_DDQN_e_greedy.h5\")\n",
    "        else:\n",
    "            print(\"---------Standard DQN---------\")\n",
    "            self.model_name = os.path.join(self.save_path, self.model_name + \"_DQN.h5\")\n",
    "\n",
    "        # Create model\n",
    "        self.model = OurModel(input_shape=(self.state_size, ), action_space=self.action_size, dueling=self.dueling)\n",
    "        self.target_model = OurModel(input_shape=(self.state_size, ), action_space=self.action_size, dueling=self.dueling)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state, decay_step):\n",
    "        if self.epsilon_greedy:\n",
    "            # Epsilon greedy strategy\n",
    "            # We use an improved version of epsilon greedy strategy for Q-Learning\n",
    "            explore_probability = self.EPSILON_MIN + (self.epsilon - self.EPSILON_MIN) * np.exp(-self.EPSILON_DECAY * decay_step)\n",
    "        else:\n",
    "            # Old epsilon strategy\n",
    "            if self.epsilon > self.EPSILON_MIN:\n",
    "                self.epsilon *= (1 - self.EPSILON_DECAY)\n",
    "            explore_probability = self.epsilon\n",
    "\n",
    "        if explore_probability > self.epsilon:\n",
    "            return random.randrange(self.action_size), explore_probability\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state, verbose=0)), explore_probability\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        # Randomly sample minibatch from the memory\n",
    "        minibatch = random.sample(self.memory, self.BATCH_SIZE)\n",
    "\n",
    "        state = np.zeros((self.BATCH_SIZE, self.state_size))\n",
    "        next_state = np.zeros((self.BATCH_SIZE, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        # do this before prediction\n",
    "        # for speedup, this could be done on the tensor level\n",
    "        # but easier to understand using a loop\n",
    "        for i in range(self.BATCH_SIZE):\n",
    "            state[i] = minibatch[i][0]\n",
    "            action.append(minibatch[i][1])\n",
    "            reward.append(minibatch[i][2])\n",
    "            next_state[i] = minibatch[i][3]\n",
    "            done.append(minibatch[i][4])\n",
    "\n",
    "        # Batch prediction to save speed\n",
    "        target = self.model.predict(state, verbose=0)\n",
    "        target_next = self.model.predict(next_state, verbose=0)\n",
    "        target_val = self.target_model.predict(next_state, verbose=0)\n",
    "\n",
    "        for i in range(self.BATCH_SIZE):\n",
    "            # Correction on the Q value for the action used\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                if self.ddqn:\n",
    "                    # Double DQN (with slight variation by predicting next state using both main and target network)\n",
    "                    a = np.argmax(target_next[i])\n",
    "                    target[i][action[i]] = reward[i] + self.GAMMA * (target_val[i][a])\n",
    "                else:\n",
    "                    # Standard DQN\n",
    "                    # DQN chooses the max Q value among next actions\n",
    "                    # selection and evaluation of action is on the target Q Network\n",
    "                    # Q_max = max_a' Q_target(s', a')\n",
    "                    target[i][action[i]] = reward[i] + self.GAMMA * (np.amax(target_next[i]))\n",
    "\n",
    "        # Train the neural network\n",
    "        self.model.fit(state, target, batch_size=self.BATCH_SIZE, verbose=0)\n",
    "\n",
    "    def update_target_model(self):\n",
    "        if not self.soft_update and self.ddqn:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "        if self.soft_update and self.ddqn:\n",
    "            q_model_theta = self.model.get_weights()\n",
    "            target_model_theta = self.target_model.get_weights()\n",
    "            counter = 0\n",
    "            for q_weight, target_weight in zip(q_model_theta, target_model_theta):\n",
    "                target_weight = target_weight * (1-self.TAU) + q_weight * self.TAU\n",
    "                target_model_theta[counter] = target_weight\n",
    "                counter += 1\n",
    "            self.target_model.set_weights(target_model_theta)\n",
    "\n",
    "    plt.figure(figsize=(18, 9))\n",
    "    def plot_model(self, score, episode):\n",
    "        self.scores.append(score)\n",
    "        self.episodes.append(episode)\n",
    "        self.averages.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
    "\n",
    "        plt.plot(self.episodes, self.averages, 'r')\n",
    "        plt.plot(self.episodes, self.scores, 'b')\n",
    "        plt.ylabel('Score', fontsize=18)\n",
    "        plt.ylabel('Steps', fontsize=18)\n",
    "\n",
    "        dqn = \"DQN_\"\n",
    "        soft_update = \"\"\n",
    "        dueling = \"\"\n",
    "        greedy = \"\"\n",
    "\n",
    "        if self.ddqn: dqn = 'DDQN_'\n",
    "        if self.soft_update: soft_update = '_soft'\n",
    "        if self.dueling: dueling = '_Dueling'\n",
    "        if self.epsilon_greedy: greedy = '_Greedy'\n",
    "\n",
    "        try:\n",
    "            plt.savefig(dqn + \"CartPole-v1\" + soft_update + dueling + greedy + \".png\")\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "        return str(self.averages[-1])[:5]\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model = tf.keras.models.load_model(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)\n",
    "\n",
    "    def run(self):\n",
    "        decay_step = 0\n",
    "        for episode in range(self.MAX_EPISODES):\n",
    "            state, _ = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                #self.env.render()\n",
    "                action, explore_probability = self.act(state, decay_step)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                next_state = np.reshape(next_state, [1, self.state_size])\n",
    "                if not done or i == self.env.spec.max_episode_steps -1:\n",
    "                    reward = reward\n",
    "                else:\n",
    "                    reward = -100\n",
    "\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "\n",
    "                state = next_state\n",
    "                i += 1\n",
    "\n",
    "                if done:\n",
    "                    # Every episode, update target model\n",
    "                    self.update_target_model()\n",
    "\n",
    "                    average = self.plot_model(i, episode)\n",
    "\n",
    "                    print(f\"Episode: {episode}/{self.MAX_EPISODES}, score: {i}, epsilon: {explore_probability:.2}, average: {average}\")\n",
    "                    if i == self.env.spec.max_episode_steps:\n",
    "                        print(\"Saving trained model...\")\n",
    "                        self.save(self.model_name)\n",
    "                        break\n",
    "\n",
    "                self.replay()\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "        self.load(self.model_name)\n",
    "        for e in range(self.MAX_EPISODES):\n",
    "            state, _ = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = np.argmax(self.model.predict(state))\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                state = np.reshape(next_state, [1, self.state_size])\n",
    "                i += 1\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.MAX_EPISODES, i))\n",
    "                    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T11:45:25.528847Z",
     "end_time": "2023-04-04T11:45:25.933735Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent = DQNAgent()\n",
    "agent.run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
