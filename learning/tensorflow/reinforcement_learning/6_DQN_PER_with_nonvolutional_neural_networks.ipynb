{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DQN PER with Convolutional Neural Networks\n",
    "From this tutorial: [https://pylessons.com/CartPole-PER-CNN](https://pylessons.com/CartPole-PER-CNN)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-04T18:11:56.093825Z",
     "end_time": "2023-04-04T18:11:58.248576Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 18:11:56.563738: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-04 18:11:57.145492: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import PixelObservationWrapper, TransformObservation, FrameStack, GrayScaleObservation, ResizeObservation\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda, Add, Conv2D, Flatten\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def OurModel(input_shape, action_space, dueling):\n",
    "    x_input = Input(input_shape)\n",
    "    x = x_input\n",
    "\n",
    "    x = Conv2D(64, 5, strides=(3, 3),padding=\"valid\", input_shape=input_shape, activation=\"relu\", data_format=\"channels_first\")(x)\n",
    "    x = Conv2D(64, 4, strides=(2, 2),padding=\"valid\", activation=\"relu\", data_format=\"channels_first\")(x)\n",
    "    x = Conv2D(64, 3, strides=(1, 1),padding=\"valid\", activation=\"relu\", data_format=\"channels_first\")(x)\n",
    "    x = Flatten()(x)\n",
    "    # 'Dense' is the basic form of a neural network layer\n",
    "    # Input Layer of state size(4) and Hidden Layer with 512 nodes\n",
    "    x = Dense(512, input_shape=input_shape, activation=\"relu\", kernel_initializer='he_uniform')(x)\n",
    "\n",
    "    # Hidden layer with 256 nodes\n",
    "    x = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(x)\n",
    "\n",
    "    # Hidden layer with 64 nodes\n",
    "    x = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(x)\n",
    "\n",
    "    if dueling:\n",
    "        state_value = Dense(1, kernel_initializer='he_uniform')(x)\n",
    "        state_value = Lambda(lambda s: tf.expand_dims(s[:, 0], -1), output_shape=(action_space,))(state_value)\n",
    "\n",
    "        action_advantage = Dense(action_space, kernel_initializer='he_uniform')(x)\n",
    "        action_advantage = Lambda(lambda a: a[:, :] - tf.reduce_mean(a[:, :], keepdims=True), output_shape=(action_space,))(action_advantage)\n",
    "\n",
    "        x = Add()([state_value, action_advantage])\n",
    "    else:\n",
    "        # Output Layer with # of actions: 2 nodes (left, right)\n",
    "        x = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(x)\n",
    "\n",
    "    model = Model(inputs = x_input, outputs = x, name='CartPole_PER_D3QN_CNN_model')\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T18:12:00.146706Z",
     "end_time": "2023-04-04T18:12:00.150713Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class SumTree(object):\n",
    "    data_pointer = 0\n",
    "\n",
    "    # Here we initialize the tree with all nodes = 0, and initialize the data with all values = 0\n",
    "    def __init__(self, capacity):\n",
    "        # Number of leaf nodes (final nodes) that contains experiences\n",
    "        self.capacity = capacity\n",
    "\n",
    "        # Generate the tree with all nodes values = 0\n",
    "        # To understand this calculation (2 * capacity - 1) look at the schema below\n",
    "        # Remember we are in a binary node (each node has max 2 children) so 2x size of leaf (capacity) - 1 (root node)\n",
    "        # Parent nodes = capacity - 1\n",
    "        # Leaf nodes = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "\n",
    "        # Contains the experiences (so the size of data is capacity)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "\n",
    "    # Here we define function that will add our priority score in the sumtree leaf and add the experience in data:\n",
    "    def add(self, priority, data):\n",
    "        # Look at what index we want to put the experience\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "\n",
    "        # Update data frame\n",
    "        self.data[self.data_pointer] = data\n",
    "\n",
    "        # Update the leaf\n",
    "        self.update(tree_index, priority)\n",
    "\n",
    "        # Add 1 to data_pointer\n",
    "        self.data_pointer += 1\n",
    "\n",
    "        if self.data_pointer >= self.capacity:  # If we're above the capacity, we go back to first index (we overwrite)\n",
    "            self.data_pointer = 0\n",
    "\n",
    "    # Update the leaf priority score and propagate the change through tree\n",
    "    def update(self, tree_index, priority):\n",
    "        # Change = new priority score - former priority score\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "\n",
    "        # then propagate the change through tree\n",
    "        # this method is faster than the recursive loop in the reference code\n",
    "        while tree_index != 0:\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "\n",
    "    # Here build a function to get a leaf from our tree. So we'll build a function to get the leaf_index, priority value of that leaf and experience associated with that leaf index:\n",
    "    def get_leaf(self, v):\n",
    "        parent_index = 0\n",
    "\n",
    "        # the while loop is faster than the method in the reference code\n",
    "        while True:\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "\n",
    "            # If we reach bottom, end the search\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            else:  # downward search, always search for a higher priority node\n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "\n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "\n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0]  # Returns the root node\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T18:12:01.115822Z",
     "end_time": "2023-04-04T18:12:01.123026Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Now we finished constructing our SumTree object, next we'll build a memory object.\n",
    "class Memory(object):  # stored as ( state, action, reward, next_state ) in SumTree\n",
    "    PER_e = 0.01  # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken\n",
    "    PER_a = 0.6  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "    PER_b = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "\n",
    "    PER_b_increment_per_sampling = 0.001\n",
    "\n",
    "    absolute_error_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        # Making the tree\n",
    "        self.tree = SumTree(capacity)\n",
    "\n",
    "    # Next, we define a function to store a new experience in our tree.\n",
    "    # Each new experience will have a score of max_prority (it will be then improved when we use this exp to train our DDQN).\n",
    "    def store(self, experience):\n",
    "        # Find the max priority\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "\n",
    "        # If the max priority = 0 we can't put priority = 0 since this experience will never have a chance to be selected\n",
    "        # So we use a minimum priority\n",
    "        if max_priority == 0:\n",
    "            max_priority = self.absolute_error_upper\n",
    "\n",
    "        self.tree.add(max_priority, experience)  # set the max priority for new priority\n",
    "\n",
    "    # Now we create sample function, which will be used to pick batch from our tree memory, which will be used to train our model.\n",
    "    # - First, we sample a minibatch of n size, the range [0, priority_total] into priority ranges.\n",
    "    # - Then a value is uniformly sampled from each range.\n",
    "    # - Then we search in the sumtree, for the experience where priority score correspond to sample values are retrieved from.\n",
    "    def sample(self, n):\n",
    "        # Create a minibatch array that will contains the minibatch\n",
    "        minibatch = []\n",
    "\n",
    "        b_idx = np.empty((n,), dtype=np.int32)\n",
    "\n",
    "        # Calculate the priority segment\n",
    "        # Here, as explained in the paper, we divide the Range[0, ptotal] into n ranges\n",
    "        priority_segment = self.tree.total_priority / n  # priority segment\n",
    "\n",
    "        for i in range(n):\n",
    "            # A value is uniformly sample from each range\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "\n",
    "            # Experience that correspond to each value is retrieved\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "\n",
    "            b_idx[i] = index\n",
    "\n",
    "            minibatch.append([data[0], data[1], data[2], data[3], data[4]])\n",
    "\n",
    "        return b_idx, minibatch\n",
    "\n",
    "    # Update the priorities on the tree\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.PER_e  # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n",
    "        ps = np.power(clipped_errors, self.PER_a)\n",
    "\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T18:12:01.779459Z",
     "end_time": "2023-04-04T18:12:01.782802Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def make_env(env_name=\"CartPole-v1\", seed=42, obs_resize_shape=(84, 84)):\n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "    env = PixelObservationWrapper(env)\n",
    "\n",
    "    # PixelObservationWrapper replace Box space to Dict space env, we don't want because it won't work with following wrappers, so we directly returns the Pixels value\n",
    "    env = TransformObservation(env, lambda obs: obs[\"pixels\"])\n",
    "    # Need to change the observation space to keep it true with observation after the TransformObservation wrapper\n",
    "    env.observation_space = env.observation_space['pixels']\n",
    "\n",
    "    env = GrayScaleObservation(env)\n",
    "    env = ResizeObservation(env, obs_resize_shape)\n",
    "    env = FrameStack(env, 4)\n",
    "    env.observation_space.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "\n",
    "    return env"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T18:12:02.417269Z",
     "end_time": "2023-04-04T18:12:02.423814Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1800x900 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.env = make_env('CartPole-v1', obs_resize_shape=(160, 240))\n",
    "\n",
    "        self.ROWS = self.env.observation_space.shape[1]\n",
    "        self.COLS = self.env.observation_space.shape[2]\n",
    "        self.FRAME_STEP = 4\n",
    "\n",
    "        # By default cartpole-v1 has max episode steps = 500\n",
    "        self.env.spec.max_episode_steps = 4000\n",
    "\n",
    "        self.state_size = self.env.observation_space.shape\n",
    "        self.action_size = self.env.action_space.n\n",
    "\n",
    "        self.MAX_EPISODES = 1000\n",
    "\n",
    "        self.memory_per = Memory(10_000)\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        self.GAMMA = 0.95\n",
    "\n",
    "        self.epsilon = 1.0\n",
    "        self.EPSILON_MIN = 0.01\n",
    "        self.EPSILON_DECAY = 0.0005\n",
    "\n",
    "        self.BATCH_SIZE = 32\n",
    "\n",
    "        self.TRAIN_START = 1000\n",
    "\n",
    "        self.TAU = 0.1 # Target network soft update hyperparameter\n",
    "\n",
    "        self.ddqn = True\n",
    "        self.soft_update = False # Use Soft update to update the target network?\n",
    "        self.dueling = False # Use dueling network\n",
    "        self.epsilon_greedy = True # Use epsilon greedy strategy\n",
    "        self.use_PER = True\n",
    "\n",
    "        self.save_path = 'models'\n",
    "        self.scores, self.episodes, self.averages = [], [], []\n",
    "\n",
    "        self.model_name = \"CartPole-v1\"\n",
    "\n",
    "        if self.ddqn:\n",
    "            if self.dueling:\n",
    "                print(\"---------Dueling Double DQN---------\")\n",
    "                self.model_name = os.path.join(self.save_path, self.model_name + \"_Dueling_DDQN_e_greedy.h5\")\n",
    "            else:\n",
    "                print(\"---------Double DQN---------\")\n",
    "                self.model_name = os.path.join(self.save_path, self.model_name + \"_DDQN_e_greedy.h5\")\n",
    "        else:\n",
    "            print(\"---------Standard DQN---------\")\n",
    "            self.model_name = os.path.join(self.save_path, self.model_name + \"_DQN.h5\")\n",
    "\n",
    "        # Create model\n",
    "        self.model = OurModel(input_shape=self.state_size, action_space=self.action_size, dueling=self.dueling)\n",
    "        self.target_model = OurModel(input_shape=self.state_size, action_space=self.action_size, dueling=self.dueling)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        experience = state, action, reward, next_state, done\n",
    "        if self.use_PER:\n",
    "            self.memory_per.store(experience)\n",
    "        else:\n",
    "            self.memory.append((experience))\n",
    "\n",
    "    def act(self, state, decay_step):\n",
    "        if self.epsilon_greedy:\n",
    "            # Epsilon greedy strategy\n",
    "            # We use an improved version of epsilon greedy strategy for Q-Learning\n",
    "            explore_probability = self.EPSILON_MIN + (self.epsilon - self.EPSILON_MIN) * np.exp(-self.EPSILON_DECAY * decay_step)\n",
    "        else:\n",
    "            # Old epsilon strategy\n",
    "            if self.epsilon > self.EPSILON_MIN:\n",
    "                self.epsilon *= (1 - self.EPSILON_DECAY)\n",
    "            explore_probability = self.epsilon\n",
    "\n",
    "        if explore_probability > self.epsilon:\n",
    "            return random.randrange(self.action_size), explore_probability\n",
    "        else:\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, axis=0)\n",
    "            return np.argmax(self.model.predict(state_tensor, verbose=0)), explore_probability\n",
    "\n",
    "    def replay(self):\n",
    "        if self.use_PER:\n",
    "            tree_idx, minibatch = self.memory_per.sample(self.BATCH_SIZE)\n",
    "        else:\n",
    "            minibatch = random.sample(self.memory, min(len(self.memory), self.BATCH_SIZE))\n",
    "\n",
    "        state = np.zeros((self.BATCH_SIZE,) + self.state_size)\n",
    "        next_state = np.zeros((self.BATCH_SIZE,) + self.state_size)\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        # do this before prediction\n",
    "        # for speedup, this could be done on the tensor level\n",
    "        # but easier to understand using a loop\n",
    "        for i in range(self.BATCH_SIZE):\n",
    "            state[i] = minibatch[i][0]\n",
    "            action.append(minibatch[i][1])\n",
    "            reward.append(minibatch[i][2])\n",
    "            next_state[i] = minibatch[i][3]\n",
    "            done.append(minibatch[i][4])\n",
    "\n",
    "        # Batch prediction to save speed\n",
    "        target = self.model.predict(state, verbose=0)\n",
    "        target_old = np.array(target)\n",
    "        target_next = self.model.predict(next_state, verbose=0)\n",
    "        target_val = self.target_model.predict(next_state, verbose=0)\n",
    "\n",
    "        for i in range(self.BATCH_SIZE):\n",
    "            # Correction on the Q value for the action used\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                if self.ddqn:\n",
    "                    # Double DQN\n",
    "                    a = np.argmax(target_next[i])\n",
    "                    target[i][action[i]] = reward[i] + self.GAMMA * (target_val[i][a])\n",
    "                else:\n",
    "                    # Standard DQN\n",
    "                    # DQN chooses the max Q value among next actions\n",
    "                    # selection and evaluation of action is on the target Q Network\n",
    "                    # Q_max = max_a' Q_target(s', a')\n",
    "                    target[i][action[i]] = reward[i] + self.GAMMA * (np.amax(target_next[i]))\n",
    "\n",
    "        if self.use_PER:\n",
    "            indices = np.arange(self.BATCH_SIZE, dtype=np.int32)\n",
    "            absolute_errors = np.abs(target_old[indices, np.array(action)] - target[indices, np.array(action)])\n",
    "            self.memory_per.batch_update(tree_idx, absolute_errors)\n",
    "\n",
    "        # Train the neural network\n",
    "        self.model.fit(state, target, batch_size=self.BATCH_SIZE, verbose=0)\n",
    "\n",
    "    def update_target_model(self):\n",
    "        if not self.soft_update and self.ddqn:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "        if self.soft_update and self.ddqn:\n",
    "            q_model_theta = self.model.get_weights()\n",
    "            target_model_theta = self.target_model.get_weights()\n",
    "            counter = 0\n",
    "            for q_weight, target_weight in zip(q_model_theta, target_model_theta):\n",
    "                target_weight = target_weight * (1-self.TAU) + q_weight * self.TAU\n",
    "                target_model_theta[counter] = target_weight\n",
    "                counter += 1\n",
    "            self.target_model.set_weights(target_model_theta)\n",
    "\n",
    "    plt.figure(figsize=(18, 9))\n",
    "    def plot_model(self, score, episode):\n",
    "        self.scores.append(score)\n",
    "        self.episodes.append(episode)\n",
    "        self.averages.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
    "\n",
    "        plt.plot(self.episodes, self.averages, 'r')\n",
    "        plt.plot(self.episodes, self.scores, 'b')\n",
    "        plt.ylabel('Score', fontsize=18)\n",
    "        plt.xlabel('Steps', fontsize=18)\n",
    "\n",
    "        dqn = \"DQN_\"\n",
    "        soft_update = \"\"\n",
    "        dueling = \"\"\n",
    "        greedy = \"\"\n",
    "        PER = \"\"\n",
    "\n",
    "        if self.ddqn: dqn = 'DDQN_'\n",
    "        if self.soft_update: soft_update = '_soft'\n",
    "        if self.dueling: dueling = '_Dueling'\n",
    "        if self.epsilon_greedy: greedy = '_Greedy'\n",
    "        if self.use_PER: PER = \"_PER\"\n",
    "\n",
    "        try:\n",
    "            plt.savefig(dqn + \"CartPole-v1_CNN\" + soft_update + dueling + greedy + PER + \".png\")\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "        return str(self.averages[-1])[:5]\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model = tf.keras.models.load_model(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)\n",
    "\n",
    "    def run(self):\n",
    "        decay_step = 0\n",
    "        for episode in range(self.MAX_EPISODES):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "\n",
    "                decay_step += 1\n",
    "\n",
    "                action, explore_probability = self.act(state, decay_step)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                if not done or i == self.env.spec.max_episode_steps -1:\n",
    "                    reward = reward\n",
    "                else:\n",
    "                    reward = -100\n",
    "\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "\n",
    "                state = next_state\n",
    "                i += 1\n",
    "\n",
    "                if done:\n",
    "                    # Update target model every four episodes\n",
    "                    if episode % self.FRAME_STEP == 0:\n",
    "                        self.update_target_model()\n",
    "\n",
    "                    average = self.plot_model(i, episode)\n",
    "\n",
    "                    print(f\"Episode: {episode}/{self.MAX_EPISODES}, score: {i}, epsilon: {explore_probability:.2}, average: {average}\")\n",
    "                    if i == self.env.spec.max_episode_steps:\n",
    "                        print(\"Saving trained model...\")\n",
    "                        self.save(self.model_name)\n",
    "                        break\n",
    "\n",
    "                self.replay()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T18:12:03.188430Z",
     "end_time": "2023-04-04T18:12:03.228883Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Double DQN---------\n",
      "Model: \"CartPole_PER_D3QN_CNN_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 4, 160, 240)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 64, 52, 79)        6464      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 64, 25, 38)        65600     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 64, 23, 36)        36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 52992)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               27132416  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,389,314\n",
      "Trainable params: 27,389,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 18:12:04.058070: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-04 18:12:04.083823: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-04 18:12:04.084061: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-04 18:12:04.085273: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-04 18:12:04.085457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-04 18:12:04.085620: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-04 18:12:04.830401: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-04 18:12:04.830636: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-04 18:12:04.830798: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-04 18:12:04.830922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4260 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "/home/deewens/miniconda3/envs/tf/lib/python3.9/site-packages/keras/optimizers/legacy/rmsprop.py:143: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CartPole_PER_D3QN_CNN_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 4, 160, 240)]     0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 64, 52, 79)        6464      \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 64, 25, 38)        65600     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 64, 23, 36)        36928     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 52992)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               27132416  \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,389,314\n",
      "Trainable params: 27,389,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 18:12:05.933481: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0/1000, score: 14, epsilon: 0.99, average: 14.0\n",
      "Episode: 1/1000, score: 11, epsilon: 0.99, average: 12.5\n",
      "Episode: 2/1000, score: 17, epsilon: 0.98, average: 14.0\n",
      "Episode: 3/1000, score: 11, epsilon: 0.97, average: 13.25\n",
      "Episode: 4/1000, score: 12, epsilon: 0.97, average: 13.0\n",
      "Episode: 5/1000, score: 17, epsilon: 0.96, average: 13.66\n",
      "Episode: 6/1000, score: 26, epsilon: 0.95, average: 15.42\n",
      "Episode: 7/1000, score: 15, epsilon: 0.94, average: 15.37\n",
      "Episode: 8/1000, score: 10, epsilon: 0.94, average: 14.77\n",
      "Episode: 9/1000, score: 11, epsilon: 0.93, average: 14.4\n",
      "Episode: 10/1000, score: 51, epsilon: 0.91, average: 17.72\n",
      "Episode: 11/1000, score: 38, epsilon: 0.89, average: 19.41\n",
      "Episode: 12/1000, score: 32, epsilon: 0.88, average: 20.38\n",
      "Episode: 13/1000, score: 12, epsilon: 0.87, average: 19.78\n",
      "Episode: 14/1000, score: 26, epsilon: 0.86, average: 20.2\n",
      "Episode: 15/1000, score: 28, epsilon: 0.85, average: 20.68\n",
      "Episode: 16/1000, score: 32, epsilon: 0.84, average: 21.35\n",
      "Episode: 17/1000, score: 30, epsilon: 0.82, average: 21.83\n",
      "Episode: 18/1000, score: 26, epsilon: 0.81, average: 22.05\n",
      "Episode: 19/1000, score: 10, epsilon: 0.81, average: 21.45\n",
      "Episode: 20/1000, score: 29, epsilon: 0.8, average: 21.80\n",
      "Episode: 21/1000, score: 32, epsilon: 0.78, average: 22.27\n",
      "Episode: 22/1000, score: 11, epsilon: 0.78, average: 21.78\n",
      "Episode: 23/1000, score: 26, epsilon: 0.77, average: 21.95\n",
      "Episode: 24/1000, score: 34, epsilon: 0.76, average: 22.44\n",
      "Episode: 25/1000, score: 14, epsilon: 0.75, average: 22.11\n",
      "Episode: 26/1000, score: 16, epsilon: 0.75, average: 21.88\n",
      "Episode: 27/1000, score: 34, epsilon: 0.73, average: 22.32\n",
      "Episode: 28/1000, score: 13, epsilon: 0.73, average: 22.0\n",
      "Episode: 29/1000, score: 25, epsilon: 0.72, average: 22.1\n",
      "Episode: 30/1000, score: 28, epsilon: 0.71, average: 22.29\n",
      "Episode: 31/1000, score: 14, epsilon: 0.71, average: 22.03\n",
      "Episode: 32/1000, score: 28, epsilon: 0.7, average: 22.21\n",
      "Episode: 33/1000, score: 33, epsilon: 0.68, average: 22.52\n",
      "Episode: 34/1000, score: 9, epsilon: 0.68, average: 22.14\n",
      "Episode: 35/1000, score: 17, epsilon: 0.68, average: 22.0\n",
      "Episode: 36/1000, score: 17, epsilon: 0.67, average: 21.86\n",
      "Episode: 37/1000, score: 14, epsilon: 0.67, average: 21.65\n",
      "Episode: 38/1000, score: 15, epsilon: 0.66, average: 21.48\n",
      "Episode: 39/1000, score: 37, epsilon: 0.65, average: 21.87\n",
      "Episode: 40/1000, score: 35, epsilon: 0.64, average: 22.19\n",
      "Episode: 41/1000, score: 27, epsilon: 0.63, average: 22.30\n",
      "Episode: 42/1000, score: 48, epsilon: 0.61, average: 22.90\n",
      "Episode: 43/1000, score: 16, epsilon: 0.61, average: 22.75\n",
      "Episode: 44/1000, score: 14, epsilon: 0.61, average: 22.55\n",
      "Episode: 45/1000, score: 30, epsilon: 0.6, average: 22.71\n",
      "Episode: 46/1000, score: 42, epsilon: 0.58, average: 23.12\n",
      "Episode: 47/1000, score: 32, epsilon: 0.58, average: 23.31\n",
      "Episode: 48/1000, score: 10, epsilon: 0.57, average: 23.04\n",
      "Episode: 49/1000, score: 55, epsilon: 0.56, average: 23.68\n",
      "Episode: 50/1000, score: 11, epsilon: 0.55, average: 23.62\n",
      "Episode: 51/1000, score: 36, epsilon: 0.54, average: 24.12\n",
      "Episode: 52/1000, score: 19, epsilon: 0.54, average: 24.16\n",
      "Episode: 53/1000, score: 17, epsilon: 0.54, average: 24.28\n",
      "Episode: 54/1000, score: 10, epsilon: 0.53, average: 24.24\n",
      "Episode: 55/1000, score: 35, epsilon: 0.52, average: 24.6\n",
      "Episode: 56/1000, score: 14, epsilon: 0.52, average: 24.36\n",
      "Episode: 57/1000, score: 31, epsilon: 0.51, average: 24.68\n",
      "Episode: 58/1000, score: 10, epsilon: 0.51, average: 24.68\n",
      "Episode: 59/1000, score: 22, epsilon: 0.5, average: 24.9\n",
      "Episode: 60/1000, score: 16, epsilon: 0.5, average: 24.2\n",
      "Episode: 61/1000, score: 32, epsilon: 0.49, average: 24.08\n",
      "Episode: 62/1000, score: 11, epsilon: 0.49, average: 23.66\n",
      "Episode: 63/1000, score: 51, epsilon: 0.48, average: 24.44\n",
      "Episode: 64/1000, score: 36, epsilon: 0.47, average: 24.64\n",
      "Episode: 65/1000, score: 42, epsilon: 0.46, average: 24.92\n",
      "Episode: 66/1000, score: 37, epsilon: 0.45, average: 25.02\n",
      "Episode: 67/1000, score: 15, epsilon: 0.45, average: 24.72\n",
      "Episode: 68/1000, score: 51, epsilon: 0.44, average: 25.22\n",
      "Episode: 69/1000, score: 63, epsilon: 0.42, average: 26.28\n",
      "Episode: 70/1000, score: 80, epsilon: 0.41, average: 27.3\n",
      "Episode: 71/1000, score: 70, epsilon: 0.39, average: 28.06\n",
      "Episode: 72/1000, score: 28, epsilon: 0.39, average: 28.4\n",
      "Episode: 73/1000, score: 48, epsilon: 0.38, average: 28.84\n",
      "Episode: 74/1000, score: 27, epsilon: 0.37, average: 28.7\n",
      "Episode: 75/1000, score: 58, epsilon: 0.36, average: 29.58\n",
      "Episode: 76/1000, score: 37, epsilon: 0.36, average: 30.0\n",
      "Episode: 77/1000, score: 62, epsilon: 0.35, average: 30.56\n",
      "Episode: 78/1000, score: 46, epsilon: 0.34, average: 31.22\n",
      "Episode: 79/1000, score: 44, epsilon: 0.33, average: 31.6\n",
      "Episode: 80/1000, score: 58, epsilon: 0.32, average: 32.2\n",
      "Episode: 81/1000, score: 56, epsilon: 0.31, average: 33.04\n",
      "Episode: 82/1000, score: 42, epsilon: 0.31, average: 33.32\n",
      "Episode: 83/1000, score: 24, epsilon: 0.3, average: 33.14\n",
      "Episode: 84/1000, score: 32, epsilon: 0.3, average: 33.6\n",
      "Episode: 85/1000, score: 70, epsilon: 0.29, average: 34.66\n",
      "Episode: 86/1000, score: 65, epsilon: 0.28, average: 35.62\n",
      "Episode: 87/1000, score: 109, epsilon: 0.27, average: 37.52\n",
      "Episode: 88/1000, score: 35, epsilon: 0.26, average: 37.92\n",
      "Episode: 89/1000, score: 131, epsilon: 0.25, average: 39.8\n",
      "Episode: 90/1000, score: 49, epsilon: 0.24, average: 40.08\n",
      "Episode: 91/1000, score: 46, epsilon: 0.24, average: 40.46\n",
      "Episode: 92/1000, score: 36, epsilon: 0.23, average: 40.22\n",
      "Episode: 93/1000, score: 36, epsilon: 0.23, average: 40.62\n",
      "Episode: 94/1000, score: 44, epsilon: 0.22, average: 41.22\n",
      "Episode: 95/1000, score: 46, epsilon: 0.22, average: 41.54\n",
      "Episode: 96/1000, score: 23, epsilon: 0.22, average: 41.16\n",
      "Episode: 97/1000, score: 40, epsilon: 0.21, average: 41.32\n",
      "Episode: 98/1000, score: 111, epsilon: 0.2, average: 43.34\n",
      "Episode: 99/1000, score: 80, epsilon: 0.19, average: 43.84\n",
      "Episode: 100/1000, score: 41, epsilon: 0.19, average: 44.44\n",
      "Episode: 101/1000, score: 11, epsilon: 0.19, average: 43.94\n",
      "Episode: 102/1000, score: 33, epsilon: 0.19, average: 44.22\n",
      "Episode: 103/1000, score: 46, epsilon: 0.18, average: 44.8\n",
      "Episode: 104/1000, score: 12, epsilon: 0.18, average: 44.84\n",
      "Episode: 105/1000, score: 74, epsilon: 0.17, average: 45.62\n",
      "Episode: 106/1000, score: 115, epsilon: 0.17, average: 47.64\n",
      "Episode: 107/1000, score: 50, epsilon: 0.16, average: 48.02\n",
      "Episode: 108/1000, score: 78, epsilon: 0.16, average: 49.38\n",
      "Episode: 109/1000, score: 69, epsilon: 0.15, average: 50.32\n",
      "Episode: 110/1000, score: 10, epsilon: 0.15, average: 50.2\n",
      "Episode: 111/1000, score: 54, epsilon: 0.15, average: 50.64\n",
      "Episode: 112/1000, score: 85, epsilon: 0.14, average: 52.12\n",
      "Episode: 113/1000, score: 24, epsilon: 0.14, average: 51.58\n",
      "Episode: 114/1000, score: 49, epsilon: 0.14, average: 51.84\n",
      "Episode: 115/1000, score: 21, epsilon: 0.13, average: 51.42\n",
      "Episode: 116/1000, score: 88, epsilon: 0.13, average: 52.44\n",
      "Episode: 117/1000, score: 27, epsilon: 0.13, average: 52.68\n",
      "Episode: 118/1000, score: 70, epsilon: 0.12, average: 53.06\n",
      "Episode: 119/1000, score: 51, epsilon: 0.12, average: 52.82\n",
      "Episode: 120/1000, score: 84, epsilon: 0.12, average: 52.9\n",
      "Episode: 121/1000, score: 93, epsilon: 0.11, average: 53.36\n",
      "Episode: 122/1000, score: 100, epsilon: 0.11, average: 54.8\n",
      "Episode: 123/1000, score: 36, epsilon: 0.1, average: 54.56\n",
      "Episode: 124/1000, score: 65, epsilon: 0.1, average: 55.32\n",
      "Episode: 125/1000, score: 67, epsilon: 0.099, average: 55.5\n",
      "Episode: 126/1000, score: 53, epsilon: 0.096, average: 55.82\n",
      "Episode: 127/1000, score: 34, epsilon: 0.095, average: 55.26\n",
      "Episode: 128/1000, score: 135, epsilon: 0.089, average: 57.04\n",
      "Episode: 129/1000, score: 75, epsilon: 0.086, average: 57.66\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent()\n",
    "agent.run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
